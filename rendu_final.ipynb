{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-23T19:03:09.088485300Z",
     "start_time": "2025-12-23T19:03:07.519631900Z"
    }
   },
   "source": [
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from utils import reward_function_updated\n",
    "\n",
    "# --- 2. TRAITEMENT (Inchangé) ---\n",
    "def calculate_indicators(df):\n",
    "    delta = df['close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    df['feature_rsi'] = 100 - (100 / (1 + rs))\n",
    "    df['feature_rsi'] = df['feature_rsi'] / 100.0\n",
    "\n",
    "    exp1 = df['close'].ewm(span=12, adjust=False).mean()\n",
    "    exp2 = df['close'].ewm(span=26, adjust=False).mean()\n",
    "    slow_macd = exp1 - exp2\n",
    "    fast_macd = slow_macd.ewm(span=9, adjust=False).mean()\n",
    "    df['feature_macd_histogram'] = slow_macd - fast_macd\n",
    "    df['feature_macd_fast'] = fast_macd\n",
    "    df['feature_macd_slow'] = slow_macd\n",
    "\n",
    "    high_low = df['high'] - df['low']\n",
    "    high_close = np.abs(df['high'] - df['close'].shift())\n",
    "    low_close = np.abs(df['low'] - df['close'].shift())\n",
    "    true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    df['feature_atr'] = true_range.rolling(14).mean() / df['close']\n",
    "\n",
    "    df['feature_return'] = df['close'].pct_change()\n",
    "\n",
    "\n",
    "    return df.dropna()\n",
    "\n",
    "def preprocess(df):\n",
    "    df = df.sort_index().dropna().drop_duplicates()\n",
    "    return calculate_indicators(df)\n",
    "\n",
    "def run_env(position_range, learning_rate, ent_coef, gamma, gae_lambda, batch_size):\n",
    "    config = {\n",
    "        \"policy_type\": \"MlpLstmPolicy\",\n",
    "        \"total_timesteps\": 300_000,    # On allonge encore, l'Alpha est dur à trouver\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"ent_coef\": ent_coef,\n",
    "        \"batch_size\": batch_size, # Batch plus gros pour lisser le bruit des returns\n",
    "        \"n_steps\": 2048,\n",
    "        \"position_range\": position_range,\n",
    "        \"gae_lambda\": gae_lambda,\n",
    "        \"gamma\": gamma,\n",
    "        \"project_name\": \"RL-Trading-Project\",\n",
    "        \"run_name\": f\"{learning_rate}_{ent_coef}_{position_range}_{gamma}_{gae_lambda}_{batch_size}\",\n",
    "        }\n",
    "\n",
    "    run = wandb.init(\n",
    "        project=config[\"project_name\"],\n",
    "        name=config[\"run_name\"],\n",
    "        config=config,\n",
    "        sync_tensorboard=True,\n",
    "        monitor_gym=True,\n",
    "        save_code=True,\n",
    "    )\n",
    "\n",
    "    env = gym.make(\n",
    "        \"MultiDatasetTradingEnv\",\n",
    "        dataset_dir=\"./data/*.pkl\",\n",
    "        preprocess=preprocess,\n",
    "        portfolio_initial_value=1000,\n",
    "        trading_fees=0.1/100,\n",
    "        borrow_interest_rate=0.02/100/24,\n",
    "        reward_function=reward_function_updated,\n",
    "        position_range=config[\"position_range\"]\n",
    "    )\n",
    "\n",
    "    # Création de la callback Wandb\n",
    "    wandb_callback = WandbCallback(\n",
    "        gradient_save_freq=100,\n",
    "        model_save_path=f\"models/{run.id}\",\n",
    "        verbose=2,\n",
    "    )\n",
    "\n",
    "    model = RecurrentPPO(\n",
    "        config[\"policy_type\"],\n",
    "        env,\n",
    "        verbose=1,\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        ent_coef=config[\"ent_coef\"],\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        n_steps=config[\"n_steps\"],\n",
    "        tensorboard_log=f\"runs/{run.id}\",\n",
    "        gamma=config[\"gamma\"],\n",
    "        gae_lambda=config[\"gae_lambda\"]\n",
    "    )\n",
    "\n",
    "    print(f\"--- Démarrage Alpha Hunter ---\")\n",
    "    print(f\"Objectif : Battre le Buy & Hold (Reward = Return - Market)\")\n",
    "    print(f\"Exploration forcée (Ent_coef={config['ent_coef']})\")\n",
    "\n",
    "    model.learn(\n",
    "        total_timesteps=config[\"total_timesteps\"],\n",
    "        callback=wandb_callback\n",
    "    )\n",
    "\n",
    "    print(\"Fin de l'apprentissage\")\n",
    "\n",
    "    model.save(\"recurrent_ppo_alpha_hunter\")\n",
    "\n",
    "    obs, info = env.reset()\n",
    "    done, truncated = False, False\n",
    "\n",
    "    while not (done or truncated):\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "    metrics = env.unwrapped.get_metrics()\n",
    "    print(\"Métriques finales :\", metrics)\n",
    "\n",
    "    market_return = float(metrics.get(\"Market Return\", \"0%\").replace('%',''))\n",
    "    portfolio_return = float(metrics.get(\"Portfolio Return\", \"0%\").replace('%', ''))\n",
    "    metrics_float = {'Market Return (%): ': market_return, 'Portfolio Return (%): ': portfolio_return}\n",
    "    wandb.log(metrics_float)\n",
    "\n",
    "    wandb.finish()"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-12-23T19:12:10.335050100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Liste des différents hyperparamètres à tester\n",
    "learning_rates = [1e-4, 1e-3] # Taux d'apprentissage (1e-4 et 1e-3)\n",
    "ent_coefs = [0.01, 0.05] # Coefficients d'entropie pour encourager l'exploration (0.01 et 0.05)\n",
    "position_ranges = [(-1,1), (-0.5, 0.5), (0,0.8)] # Différentes plages de positions [-1,1], [-0.5,0.5], [0,0.8]\n",
    "gammas = [0.9, 0.95, 0.99] # Facteurs d'actualisation (0.9, 0.95, 0.99)\n",
    "gae_lambdas = [0.01, 0.05, 0.1] # Paramètres GAE (0.01, 0.05, 0.1)\n",
    "batch_sizes = [128, 256] # Tailles de batch (128 et 256)\n",
    "\n",
    "# Boucle sur toutes les combinaisons d'hyperparamètres\n",
    "for lr in learning_rates:\n",
    "    for ent in ent_coefs:\n",
    "        for pos_range in position_ranges:\n",
    "            for gamma in gammas:\n",
    "                for gae_lambda in gae_lambdas:\n",
    "                    for batch_size in batch_sizes:\n",
    "                        config = {\n",
    "                            \"policy_type\": \"MlpLstmPolicy\",\n",
    "                            \"total_timesteps\": 200_000,    # On allonge encore, l'Alpha est dur à trouver\n",
    "                            \"learning_rate\": lr,\n",
    "                            \"ent_coef\": ent,\n",
    "                            \"batch_size\": batch_size, # Batch plus gros pour lisser le bruit des returns\n",
    "                            \"n_steps\": 2048,\n",
    "                            \"position_range\": pos_range,\n",
    "                            \"gae_lambda\": gae_lambda,\n",
    "                            \"gamma\": gamma,\n",
    "                            \"project_name\": \"RL-Trading-Project\",\n",
    "                            \"run_name\": f\"{lr}_{ent}_{pos_range}_{gamma}_{gae_lambda}_{batch_size}\",\n",
    "                        }\n",
    "                        run = wandb.init(\n",
    "                            project=config[\"project_name\"],\n",
    "                            name=config[\"run_name\"],\n",
    "                            config=config,\n",
    "                            sync_tensorboard=True,\n",
    "                            monitor_gym=True,\n",
    "                            save_code=True,\n",
    "                        )\n",
    "\n",
    "                        env = gym.make(\n",
    "                            \"MultiDatasetTradingEnv\",\n",
    "                            dataset_dir=\"./data/*.pkl\",\n",
    "                            preprocess=preprocess,\n",
    "                            portfolio_initial_value=1000,\n",
    "                            trading_fees=0.1/100,\n",
    "                            borrow_interest_rate=0.02/100/24,\n",
    "                            reward_function=reward_function_updated,\n",
    "                            position_range=config[\"position_range\"]\n",
    "                        )\n",
    "\n",
    "                        # Création de la callback Wandb\n",
    "                        wandb_callback = WandbCallback(\n",
    "                            gradient_save_freq=100,\n",
    "                            model_save_path=f\"models/{run.id}\",\n",
    "                            verbose=2,\n",
    "                        )\n",
    "\n",
    "                        model = RecurrentPPO(\n",
    "                            config[\"policy_type\"],\n",
    "                            env,\n",
    "                            verbose=1,\n",
    "                            learning_rate=config[\"learning_rate\"],\n",
    "                            ent_coef=config[\"ent_coef\"],\n",
    "                            batch_size=config[\"batch_size\"],\n",
    "                            n_steps=config[\"n_steps\"],\n",
    "                            tensorboard_log=f\"runs/{run.id}\",\n",
    "                            gamma=config[\"gamma\"],\n",
    "                            gae_lambda=config[\"gae_lambda\"]\n",
    "                        )\n",
    "\n",
    "                        print(f\"--- Démarrage Alpha Hunter ---\")\n",
    "                        print(f\"Objectif : Battre le Buy & Hold (Reward = Return - Market)\")\n",
    "                        print(f\"Exploration forcée (Ent_coef={config['ent_coef']})\")\n",
    "\n",
    "                        model.learn(\n",
    "                            total_timesteps=config[\"total_timesteps\"],\n",
    "                            callback=wandb_callback\n",
    "                        )\n",
    "\n",
    "                        print(\"Fin de l'apprentissage\")\n",
    "\n",
    "                        model.save(\"recurrent_ppo_alpha_hunter\")\n",
    "\n",
    "                        obs, info = env.reset()\n",
    "                        done, truncated = False, False\n",
    "\n",
    "                        while not (done or truncated):\n",
    "                            action, _states = model.predict(obs, deterministic=True)\n",
    "                            obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "                        metrics = env.unwrapped.get_metrics()\n",
    "                        print(\"Métriques finales :\", metrics)\n",
    "\n",
    "                        market_return = float(metrics.get(\"Market Return\", \"0%\").replace('%',''))\n",
    "                        portfolio_return = float(metrics.get(\"Portfolio Return\", \"0%\").replace('%', ''))\n",
    "                        metrics_float = {'Market Return (%): ': market_return, 'Portfolio Return (%): ': portfolio_return}\n",
    "                        wandb.log(metrics_float)\n",
    "\n",
    "                        wandb.finish()\n",
    "\n",
    "\n"
   ],
   "id": "d0d6ed58ea3c0694",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "dc79ed6dfd2a80d75bc2da492cf96ef8"
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\tomgr\\Documents\\Cours\\RL\\RL_Project_Money\\wandb\\run-20251223_201211-i9bwm7ab</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thomas-derville-cpe-lyon/RL-Trading-Project/runs/i9bwm7ab' target=\"_blank\">0.0001_0.01_(-1, 1)_0.9_0.01_128</a></strong> to <a href='https://wandb.ai/thomas-derville-cpe-lyon/RL-Trading-Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/thomas-derville-cpe-lyon/RL-Trading-Project' target=\"_blank\">https://wandb.ai/thomas-derville-cpe-lyon/RL-Trading-Project</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/thomas-derville-cpe-lyon/RL-Trading-Project/runs/i9bwm7ab' target=\"_blank\">https://wandb.ai/thomas-derville-cpe-lyon/RL-Trading-Project/runs/i9bwm7ab</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "--- Démarrage Alpha Hunter ---\n",
      "Objectif : Battre le Buy & Hold (Reward = Return - Market)\n",
      "Exploration forcée (Ent_coef=0.01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING When using several event log directories, please call `wandb.tensorboard.patch(root_logdir=\"...\")` before `wandb.init`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to runs/i9bwm7ab\\RecurrentPPO_1\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1032 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "Market Return : 39.25%   |   Portfolio Return : -94.83%   |   \n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 3.48e+03     |\n",
      "|    ep_rew_mean          | -245         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 599          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012395335 |\n",
      "|    clip_fraction        | 0.000635     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.023        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.0409       |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00102     |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 0.181        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.48e+03    |\n",
      "|    ep_rew_mean          | -245        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 502         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007329774 |\n",
      "|    clip_fraction        | 0.072       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.0929      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0912      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00756    |\n",
      "|    std                  | 0.999       |\n",
      "|    value_loss           | 0.202       |\n",
      "-----------------------------------------\n",
      "Market Return : 26.96%   |   Portfolio Return : -97.93%   |   \n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.99e+03    |\n",
      "|    ep_rew_mean          | -279        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 504         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009933648 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.08        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0197      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0152     |\n",
      "|    std                  | 0.995       |\n",
      "|    value_loss           | 0.0568      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.99e+03    |\n",
      "|    ep_rew_mean          | -279        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 465         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008621756 |\n",
      "|    clip_fraction        | 0.0985      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.0609      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.011      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    std                  | 0.99        |\n",
      "|    value_loss           | 0.0588      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.99e+03    |\n",
      "|    ep_rew_mean          | -279        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 477         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016699404 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.0519      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0265     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0208     |\n",
      "|    std                  | 0.981       |\n",
      "|    value_loss           | 0.0327      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.99e+03    |\n",
      "|    ep_rew_mean          | -279        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 486         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017088545 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.0646      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0476      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0233     |\n",
      "|    std                  | 0.974       |\n",
      "|    value_loss           | 0.0392      |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
