{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL -- Projet \"Trading automatique\"\n",
    "\n",
    "Ce notebook contient du code de base et quelques explications pour vous aider sur ce sujet.\n",
    "\n",
    "Vous êtes libres de réaliser ce projet avec des scripts Python ou des Jupyter Notebooks, à votre convenance.\n",
    "\n",
    "Vous devez télécharger les paquets Python suivants :\n",
    "\n",
    "```sh\n",
    "pip install gymnasium\n",
    "pip install pandas\n",
    "pip install gym-trading-env-continuous\n",
    "```\n",
    "\n",
    "Vous utiliserez l'environnement `gym-trading-env-continuous`, qui est un *fork* de [Gym Trading Env](https://gym-trading-env.readthedocs.io/en/latest/index.html). La différence majeure est expliquée dans ce document ; la documentation originelle reste utilisable."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "import gym_trading_env"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilisation des données de simulation\n",
    "\n",
    "Les données sont dans un format binaire (Pickle) que vous pouvez lire avec Pandas. Vous devez vous assurer que les données sont triées par date.\n",
    "\n",
    "Des étapes de prétraitement peuvent aider votre apprentissage, par exemple, supprimer les doublons, etc."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def preprocess(df):\n",
    "    df = df.sort_index()\n",
    "    df = df.dropna()\n",
    "    df = df.drop_duplicates()\n",
    "    return df\n",
    "\n",
    "df = preprocess(pd.read_pickle('./data/binance-ETHUSD-1h.pkl'))\n",
    "df.head(5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajout de *features*\n",
    "\n",
    "Vous pouvez également rajouter de nouvelles données au DataFrame pour créer de nouvelles *features* que l'agent pourra utiliser.\n",
    "Voir pour cela la [doc](https://gym-trading-env.readthedocs.io/en/latest/features.html).\n",
    "\n",
    "Chaque nouvelle *feature* doit commencer par `feature_` pour être détectée."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def preprocess(df):\n",
    "    df = df.sort_index()\n",
    "    df = df.dropna()\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    df['feature_close'] = (df['close'] - df['close'].mean()) / df['close'].std()\n",
    "\n",
    "    return df\n",
    "\n",
    "df = preprocess(pd.read_pickle('./data/binance-ETHUSD-1h.pkl'))\n",
    "df.head(5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par défaut, l'agent ne reçoit comme *features* que sa dernière *position* (voir le paragraphe suivant), ce qui ne sera certainement pas suffisant ! À vous d'ajouter les *features* qui seront pertinentes pour que l'agent apprenne la politique optimale..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctionnement des actions\n",
    "\n",
    "Une action est une **position**, c'est-à-dire un ratio entre la proportion d'*assets* (exemple : ETH) et la proportion de *fiat* (exemple : USD) dans le portefeuille.\n",
    "Ainsi, la position `0.5` consiste à avoir exactement 50% d'ETH et 50% d'USD (en vendant l'un ou l'autre pour arriver à ce ratio). `0.1` consiste à avoir 10% d'ETH et 90% d'USD.\n",
    "\n",
    "Il existe des positions un peu plus complexes :\n",
    "\n",
    "- `< 0` : une position inférieure à 0 va vendre encore plus d'ETH que le portefeuille n'en contient, pour obtenir des USD. Cela nécessite un emprunt, qui sera remboursé avec un intérêt.\n",
    "- `> 1` : une position supérieure à 1 va dépenser encore plus d'USD que le portefeuille n'en contient, pour acheter des ETH. Cela nécessite également un emprunt.\n",
    "\n",
    "Ces positions (qui sont appelées *short* et *margin* en finance) peuvent faire gagner beaucoup à votre agent, mais démultiplient les risques également. Si votre agent fait une bonne affaire, vous pouvez vendre à un prix élevé, racheter quand le prix est plus faible, et rembourser l'emprunt en empochant la différence. En revanche, si votre agent fait une mauvaise affaire, et doit vider son portefeuille pour rembourser l'emprunt, vous perdez automatiquement (`terminated=True`).\n",
    "\n",
    "### Actions continues\n",
    "\n",
    "Par rapport à l'environnement `gym-trading-env` d'origine, la version que je vous fournis permet de spécifier directement une position comme action, c'est-à-dire un nombre flottant. Votre agent a donc un contrôle précis sur la position désirée. Cela rajoute de la flexibilité mais rend l'apprentissage beaucoup plus difficile.\n",
    "\n",
    "Exemple :"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    portfolio_initial_value=1_000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    ")\n",
    "\n",
    "obs, _ = env.reset()\n",
    "# On veut une position de 88% ETH / 12% USD\n",
    "obs, reward, terminated, truncated, info = env.step(0.88)\n",
    "print(obs)\n",
    "print(info)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par défaut, l'espace des actions est limité à $[-1, 2]$ pour que votre agent ne puisse emprunter que jusqu'à 100%. Vous pouvez empêcher votre agent de prendre de telles positions, ou limiter le risque, en contrôlant les bornes autorisées des actions.\n",
    "\n",
    "Par exemple, en clippant l'action dans l'intervalle $[0,1]$, vous empêchez l'agent de faire des emprunts.\n",
    "\n",
    "À l'inverse, vous pouvez augmenter l'intervalle pour permettre des emprunts plus risqués, mais qui peuvent rapporter plus. À vous de choisir !\n",
    "\n",
    "Vous pouvez changer les bornes via le paramètre `position_range` du constructeur :"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    position_range=(0, 1),  # ICI : (borne min, borne max)\n",
    "    portfolio_initial_value=1_000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez aussi modifier l'action en sortie de votre algorithme d'apprentissage, de la manière que vous souhaitez (clipping, interpolation, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions discrètes\n",
    "\n",
    "Pour simplifier l'apprentissage, vous pouvez utiliser le *wrapper* `gym_trading_env.wrapper.DiscreteActionsWrapper` que je vous fournis, et qui permet de revenir au fonctionnement d'origine de l'environnement `gym-trading-env`. Vous devrez alors spécifier l'ensemble des positions possibles, puis votre agent choisira une position parmi cette liste à chaque pas de temps.\n",
    "Par exemple, si la liste des positions est `[0, 0.5, 1]` et que l'action choisie est `1`, cela veut dire qu'on veut la position qui correspond au 2e élément de la liste, soit `0.5` (50%/50%).\n",
    "\n",
    "Vous pouvez rajouter autant d'actions que vous voulez, par exemple `[0, 0.25, 0.5, 1]` ou encore tous les 0.1 entre 0 et 1, etc. Plus il y a d'actions possibles, plus votre agent aura de choix (flexibilité), donc plus son comportement pourra être complexe, mais cela rajoute de la difficulté durant l'entraînement.\n",
    "\n",
    "N'oubliez pas que vous pouvez autoriser les positions avec emprunt en ajoutant des nombres inférieurs à 0 ou supérieurs à 1 à la liste autorisée.\n",
    "\n",
    "Exemple :"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from gym_trading_env.wrapper import DiscreteActionsWrapper\n",
    "\n",
    "# Vous pouvez aussi appeler le wrapper `env` pour faire plus simple\n",
    "# Ici, je fais explicitement la distinction entre `wrapper` et `env`\n",
    "wrapper = DiscreteActionsWrapper(env, positions=[-1, 0, 0.25, 0.5, 0.75, 1, 2])\n",
    "obs, _ = wrapper.reset()\n",
    "# On veut une position de 25% ETH / 75% USD ; cela correspond à la position\n",
    "# d'index 2 dans la liste ci-dessus\n",
    "obs, reward, terminated, truncated, info = wrapper.step(2)\n",
    "print(obs)\n",
    "print(info)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notez que, quand les actions continues sont utilisées, la variable `position_index` du dictionnaire `info` n'est pas disponible (c'est logique)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changement de la fonction de récompense\n",
    "\n",
    "Vous pouvez changer la fonction de récompense pour améliorer l'apprentissage de l'agent.\n",
    "Dans tous les cas, vous serez évalué(e)s sur la valuation du portefeuille à la fin de l'épisode (voir [ci-dessous](#évaluation)), mais cette simple mesure n'est peut-être pas la meilleure fonction de récompense.\n",
    "D'autres fonctions peuvent encourager l'agent à mieux apprendre, en explorant diverses possibilités, etc."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def reward_function(history):\n",
    "    return history['portfolio_valuation', -1]\n",
    "\n",
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    portfolio_initial_value=1_000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    "    # On spécifie la fonction de récompense\n",
    "    reward_function=reward_function,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Déroulément d'un épisode\n",
    "\n",
    "Un épisode se déroule jusqu'à ce que :\n",
    "\n",
    "- l'agent atteigne la fin des données d'entraînement (nous n'avons plus de nouvelle donnée) => `truncated=True`\n",
    "\n",
    "- la valeur du portefeuille atteint 0 (l'agent a perdu tout l'argent) => `terminated=True`\n",
    "\n",
    "Vous devrez probablement entraîner l'agent sur plusieurs épisodes avant que son comportement ne converge.\n",
    "\n",
    "Pour éviter de sur-apprendre (*overfit*), vous devrez utiliser plusieurs jeux de données via [MultiDatasetTradingEnv](https://gym-trading-env.readthedocs.io/en/latest/multi_datasets.html).\n",
    "\n",
    "Dans ce cas, chaque épisode utilisera un jeu de données différent (en bouclant si vous demandez plus d'épisodes qu'il n'y a de jeux de données). Vous pouvez accéder au nom du jeu de données de l'épisode en cours via `env.name`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "nb_episodes = 2\n",
    "for episode in range(1, nb_episodes + 1):\n",
    "    obs, _ = env.reset()\n",
    "    print(f'Episode n˚{episode} -- Jeu de donnée {env.name}')\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "    if terminated:\n",
    "        print('Argent perdu')\n",
    "    elif truncated:\n",
    "        print('Épisode terminé')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Évaluation\n",
    "\n",
    "Afin de disposer d'un critère simple pour comparer les différentes solutions, nous utiliserons la valeur du portefeuille (`portfolio_valuation`).\n",
    "C'est assez simple : on veut que l'agent ait gagné le plus d'argent à la fin de la simulation.\n",
    "\n",
    "Vous pouvez ajouter ce critère à la liste des métriques affichées à la fin de chaque épisode, pour que ce soit plus visible :"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def metric_portfolio_valuation(history):\n",
    "    return round(history['portfolio_valuation', -1], 2)\n",
    "\n",
    "env.add_metric('Portfolio Valuation', metric_portfolio_valuation)\n",
    "\n",
    "done = False\n",
    "obs, _ = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puisque l'environnement peut se dérouler sur plusieurs épisodes (1 par jeu de données), vous devrez calculer la **moyenne des `portfolio_valuation`** sur l'ensemble des jeux de données possibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ Pour que ce soit honnête, vous **devez initialiser l'environnement avec les contraintes** imposées dans le sujet :\n",
    "\n",
    "- une valeur initiale du portefeuille de `1000` ;\n",
    "- des frais de 0.1% par transaction ;\n",
    "- un taux d'intérêt de 0.02% par jour soit 0.02/100/24 par heure.\n",
    "\n",
    "Sinon, il est beaucoup plus simple d'augmenter la valeur finale...\n",
    "\n",
    "```py\n",
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    # LIGNES SUIVANTES :\n",
    "    # Valeur initiale du portefeuille\n",
    "    portfolio_initial_value=1_000,\n",
    "    # Frais de transactions\n",
    "    trading_fees=0.1/100,\n",
    "    # Intérêts sur les prêts\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    ")\n",
    "```\n",
    "\n",
    "Vous pouvez également accéder à la métrique de `portfolio_valuation` à la fin d'une simulation, si vous voulez par exemple l'ajouter à votre *run* WandB :"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "portfolio_valuation = env.historical_info['portfolio_valuation', -1]\n",
    "# Si on avait WandB :\n",
    "# run.summary['portfolio_valuation'] = portfolio_valuation\n",
    "# On simule ça par un simple print...\n",
    "print(portfolio_valuation)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou bien, pour récupérer les métriques calculées par l'environnement (cela peut être utile pour les ajouter à WandB) :"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "metrics = env.get_metrics()\n",
    "print(metrics)\n",
    "portfolio_valuation = metrics['Portfolio Valuation']\n",
    "print(portfolio_valuation)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conseils\n",
    "\n",
    "À part les quelques contraintes présentées dans ce fichier (et rappelées sur la page du projet), vous êtes assez libres !\n",
    "\n",
    "Votre algorithme de RL peut être arbitrairement simple ou complexe. Je liste ici quelques conseils ou pistes, que vous pouvez explorer :\n",
    "\n",
    "- *Features* : Par défaut, votre agent n'utilise que le prix de l'*asset* (`close`) comme *feature* pour la prise de décision. Vous pouvez ajouter les *features* que vous voulez. En particulier, des métriques spécifiques à la finance peuvent être intéressantes, par exemple pour déterminer le risque que le prix change brutalement (à la hausse ou à la baisse)...\n",
    "\n",
    "- Algorithme : Vous pouvez utiliser des algorithmes existants, ou en inventer un nouveau. N'hésitez pas à ré-utiliser tout ce que vous avez appris en *Machine Learning* et *Deep Learning*. Typiquement, les données financières sont des données temporelles : certains réseaux de neurones sont plus appropriés que d'autres pour ce genre de tâche...\n",
    "\n",
    "- Configuration de l'environnement : L'environnement est très extensible ! Vous pouvez par exemple ajouter des *features* dynamiques (pas seulement calculées lors du prétraitement). La [documentation](https://gym-trading-env.readthedocs.io/en/latest/index.html) est très claire et très complète.\n",
    "\n",
    "Vous pouvez vous inspirer de travaux existants trouvés sur l'Internet à condition de **citer votre source**. Utiliser le travail de quelqu'un d'autre sans le citer sera considéré comme du plagiat."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Version aléatoire complète avec RSI et MACD, et visualisation"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "from gym_trading_env.wrapper import DiscreteActionsWrapper\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Définition des indicateurs techniques"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_rsi(series, window=14):\n",
    "    delta = series.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def calculate_macd(series, slow=26, fast=12, signal=9):\n",
    "    exp1 = series.ewm(span=fast, adjust=False).mean()\n",
    "    exp2 = series.ewm(span=slow, adjust=False).mean()\n",
    "    macd = exp1 - exp2\n",
    "    return macd"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Prétraitement des données"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def preprocess(df):\n",
    "    # Tri et nettoyage\n",
    "    df = df.sort_index().dropna().drop_duplicates()\n",
    "\n",
    "    # Ajout de features (doivent commencer par \"feature_\")\n",
    "    # 1. RSI normalisé entre 0 et 1\n",
    "    df['feature_RSI'] = calculate_rsi(df['close']) / 100\n",
    "\n",
    "    # 2. MACD\n",
    "    df['feature_MACD'] = calculate_macd(df['close'])\n",
    "\n",
    "    # 3. Rendements logarithmiques (plus stable pour le RL que le prix brut)\n",
    "    df['feature_log_return'] = np.log(df['close'] / df['close'].shift(1))\n",
    "\n",
    "    # 4. Position du prix par rapport à la moyenne mobile\n",
    "    df['feature_sma_dist'] = (df['close'] - df['close'].rolling(20).mean()) / df['close'].rolling(20).std()\n",
    "\n",
    "    return df.dropna()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Configuration de l'environnement"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def reward_function(history):\n",
    "    # Récompense basée sur la variation logarithmique de la valeur du portefeuille\n",
    "    # Cela encourage une croissance stable plutôt que des paris risqués\n",
    "    if len(history[\"portfolio_valuation\"]) < 2:\n",
    "        return 0\n",
    "    return np.log(history['portfolio_valuation', -1] / history['portfolio_valuation', -2])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Création de l'environnement avec les contraintes du projet\n",
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"./data/*.pkl\", # Assure-toi que le dossier data contient tes fichiers .pkl\n",
    "    preprocess=preprocess,\n",
    "    portfolio_initial_value=1000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    "    reward_function=reward_function,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Exécution d'une simulation (Exemple avec un agent aléatoire)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"Démarrage de la simulation sur le dataset : {env.unwrapped.name}\")\n",
    "\n",
    "obs, info = env.reset()\n",
    "done = False\n",
    "truncated = False\n",
    "\n",
    "while not (done or truncated):\n",
    "    # Ici, tu remplaceras par : action, _states = model.predict(obs) si tu utilises Stable Baselines\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, truncated, info = env.step(action)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Visualisation"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from gym_trading_env.renderer import Renderer\n",
    "\n",
    "print(\"Sauvegarde des logs pour la visualisation...\")\n",
    "env.unwrapped.save_for_render(dir=\"render_logs\")\n",
    "\n",
    "print(\"Simulation terminée.\")\n",
    "print(f\"Valeur finale du portefeuille : {info['portfolio_valuation']:.2f}$\")\n",
    "\n",
    "renderer = Renderer(render_logs_dir=\"render_logs\")\n",
    "renderer.run()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Version avec Stable Baselines3 et PPO"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Création de l'environnement d'entraînement"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"./data/*.pkl\",\n",
    "    preprocess=preprocess, # Ta fonction avec RSI, MACD, etc.\n",
    "    portfolio_initial_value=1000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    "    reward_function=reward_function,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Création de l'agent PPO avec MLP"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=1,\n",
    "    learning_rate=0.0003, # Hyperparamètre à ajuster\n",
    "    gamma=0.99,           # Facteur de réduction\n",
    "    tensorboard_log=\"./ppo_tensorboard/\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Apprentissage"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Entraînement en cours...\")\n",
    "model.learn(total_timesteps=100000) # Augmente ce chiffre pour de meilleures performances\n",
    "model.save(\"mon_agent_trading\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " ## 4. Évaluation et Visualisation sur un épisode"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Évaluation de l'agent entraîné...\")\n",
    "obs, info = env.reset()\n",
    "done, truncated = False, False\n",
    "\n",
    "while not (done or truncated):\n",
    "    # L'agent utilise maintenant son expérience pour choisir l'action\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, truncated, info = env.step(action)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Visualisation"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "env.unwrapped.save_for_render(dir=\"render_logs\")\n",
    "renderer = Renderer(render_logs_dir=\"render_logs\")\n",
    "renderer.run()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df= pd.read_pickle('./render_logs/yfinance-GOLDUSD-1h.pkl_2025-12-17_14-21-49.pkl')\n",
    "df.head()\n",
    "df.columns\n",
    "df.position.unique()\n",
    "df[\"position\"] = df.position.astype(float)\n",
    "df.to_pickle('./render_logs/yfinance-GOLDUSD-1h.pkl_2025-12-17_14-21-49.pkl')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# RecurrentPPO + Gestion du Risque"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "from gym_trading_env.wrapper import DiscreteActionsWrapper\n",
    "from stable_baselines3 import PPO\n",
    "from gym_trading_env.renderer import Renderer\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Preprocess (On garde tes indicateurs) ---\n",
    "def preprocess(df):\n",
    "    df = df.sort_index().dropna().drop_duplicates()\n",
    "    df['feature_close'] = df['close'].pct_change()\n",
    "    df['feature_rsi'] = calculate_rsi(df['close']) / 100\n",
    "    df['feature_macd'] = calculate_macd(df['close'])\n",
    "    return df.dropna()\n",
    "\n",
    "# --- 2. Nouvelle fonction de récompense stricte ---\n",
    "def reward_function(history):\n",
    "    # 1. Calcul du rendement réel du portefeuille (frais inclus par l'env)\n",
    "    current_val = history['portfolio_valuation', -1]\n",
    "    prev_val = history['portfolio_valuation', -2]\n",
    "\n",
    "    # Rendement logarithmique\n",
    "    reward = np.log(current_val / prev_val)\n",
    "\n",
    "    # 2. PÉNALITÉ DE CHANGEMENT (Anti-Churning)\n",
    "    # Si l'agent change de position, il paie des frais.\n",
    "    # On ajoute une punition supplémentaire pour l'inciter à \"Hold\".\n",
    "    current_pos = history['position', -1]\n",
    "    prev_pos = history['position', -2]\n",
    "\n",
    "    if current_pos != prev_pos:\n",
    "        # On lui enlève artificiellement un peu plus de reward\n",
    "        # pour qu'il ne trade que si c'est vraiment nécessaire\n",
    "        reward -= 0.0005\n",
    "\n",
    "    return reward\n",
    "\n",
    "# --- 3. Création de l'environnement de base ---\n",
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"./data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    portfolio_initial_value=1000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    "    reward_function=reward_function,\n",
    ")\n",
    "\n",
    "# --- 4. LE WRAPPER DISCRET (La solution miracle) ---\n",
    "env = DiscreteActionsWrapper(env, positions=[-0.25, 0, 1, 0.25,  0.5, 0.75])\n",
    "\n",
    "# --- 5. Agent PPO ---\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=1,\n",
    "    learning_rate=0.0003,\n",
    "    ent_coef=0.01, # Encourage l'exploration pour ne pas rester bloqué sur 0\n",
    "    tensorboard_log=\"./ppo_discrete_tensorboard/\"\n",
    ")\n",
    "\n",
    "print(\"Entraînement en mode 'Sécurité' (Actions Discrètes)...\")\n",
    "model.learn(total_timesteps=100_000)\n",
    "model.save(\"ppo_discrete_safe\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 6. Visualisation ---\n",
    "print(\"Lancement de la simulation...\")\n",
    "obs, info = env.reset()\n",
    "done, truncated = False, False\n",
    "\n",
    "while not (done or truncated):\n",
    "    action, _ = model.predict(obs)\n",
    "\n",
    "    # --- CORRECTION ---\n",
    "    # On force la conversion du tableau numpy vers un entier Python standard\n",
    "    action = int(action)\n",
    "    # ------------------\n",
    "\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "# Sauvegarde\n",
    "env.unwrapped.save_for_render(dir=\"render_logs\")\n",
    "renderer = Renderer(render_logs_dir=\"render_logs\")\n",
    "renderer.run()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Solution Hybride & Anti-Short Bias"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "from gym_trading_env.wrapper import DiscreteActionsWrapper\n",
    "from stable_baselines3 import PPO\n",
    "from gym_trading_env.renderer import Renderer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "# --- 1. CONFIGURATION ET INDICATEURS ---\n",
    "\n",
    "# On définit les hyperparamètres ici pour que WandB puisse les enregistrer\n",
    "config = {\n",
    "    \"policy_type\": \"MlpPolicy\",\n",
    "    \"total_timesteps\": 200_000,\n",
    "    \"learning_rate\": 0.0003,\n",
    "    \"ent_coef\": 0.02, # Coefficient d'exploration\n",
    "    \"batch_size\": 128,\n",
    "    \"positions\": [-0.5, 0, 0.25, 0.5, 0.75, 1.0, 1.25, 1.5], # Hybride\n",
    "    \"project_name\": \"RL-Trading-Project\"\n",
    "}\n",
    "\n",
    "def calculate_rsi(series, window=14):\n",
    "    delta = series.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def calculate_macd(series, slow=26, fast=12, signal=9):\n",
    "    exp1 = series.ewm(span=fast, adjust=False).mean()\n",
    "    exp2 = series.ewm(span=slow, adjust=False).mean()\n",
    "    macd = exp1 - exp2\n",
    "    return macd\n",
    "\n",
    "def preprocess(df):\n",
    "    df = df.sort_index().dropna().drop_duplicates()\n",
    "    df['feature_close'] = df['close'].pct_change()\n",
    "    df['feature_rsi'] = calculate_rsi(df['close']) / 100\n",
    "    df['feature_macd'] = calculate_macd(df['close'])\n",
    "    return df.dropna()\n",
    "\n",
    "def reward_function(history):\n",
    "    current_val = history['portfolio_valuation', -1]\n",
    "    prev_val = history['portfolio_valuation', -2]\n",
    "    reward = np.log(current_val / prev_val)\n",
    "\n",
    "    # Malus pour les positions Short (pour éviter le biais négatif)\n",
    "    if history['position', -1] < 0:\n",
    "        reward -= 0.0002\n",
    "\n",
    "    return reward\n",
    "\n",
    "# --- 2. INITIALISATION DE WANDB ---\n",
    "run = wandb.init(\n",
    "    project=config[\"project_name\"],\n",
    "    config=config,\n",
    "    sync_tensorboard=True, # Synchronise automatiquement les logs SB3\n",
    "    monitor_gym=True,      # Essaie d'enregistrer les vidéos (si disponible)\n",
    "    save_code=True,        # Sauvegarde ce script dans WandB\n",
    ")\n",
    "\n",
    "# --- 3. CRÉATION DE L'ENVIRONNEMENT ---\n",
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"./data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    portfolio_initial_value=1000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    "    reward_function=reward_function,\n",
    ")\n",
    "\n",
    "# Wrapper Hybride (Int -> Float spécifique)\n",
    "env = DiscreteActionsWrapper(env, positions=config[\"positions\"])\n",
    "\n",
    "# --- 4. ENTRAÎNEMENT AVEC CALLBACK WANDB ---\n",
    "model = PPO(\n",
    "    config[\"policy_type\"],\n",
    "    env,\n",
    "    verbose=1,\n",
    "    learning_rate=config[\"learning_rate\"],\n",
    "    ent_coef=config[\"ent_coef\"],\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    tensorboard_log=f\"runs/{run.id}\" # Dossier unique pour Tensorboard\n",
    ")\n",
    "\n",
    "print(f\"Lancement de l'entraînement WandB : {run.name}\")\n",
    "model.learn(\n",
    "    total_timesteps=config[\"total_timesteps\"],\n",
    "    callback=WandbCallback(\n",
    "        gradient_save_freq=100,\n",
    "        model_save_path=f\"models/{run.id}\",\n",
    "        verbose=2,\n",
    "    )\n",
    ")\n",
    "model.save(\"ppo_trading_wandb_final\")\n",
    "\n",
    "# --- 5. ÉVALUATION ET LOGGING FINAL ---\n",
    "print(\"Évaluation finale...\")\n",
    "obs, info = env.reset()\n",
    "done, truncated = False, False\n",
    "\n",
    "while not (done or truncated):\n",
    "    action, _ = model.predict(obs)\n",
    "    action = int(action) # Conversion array -> int pour le wrapper\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "# Récupération des métriques finales de l'environnement\n",
    "final_metrics = env.unwrapped.get_metrics()\n",
    "print(\"Métriques finales :\", final_metrics)\n",
    "\n",
    "# Envoi des métriques clés à WandB (pour le tableau de bord)\n",
    "wandb.log({\n",
    "    \"final_portfolio_valuation\": info['portfolio_valuation'],\n",
    "    \"market_return\": final_metrics.get(\"Market Return\", 0),\n",
    "    \"portfolio_return\": final_metrics.get(\"Portfolio Return\", 0)\n",
    "})\n",
    "\n",
    "# --- 6. VISUALISATION ---\n",
    "env.unwrapped.save_for_render(dir=\"render_logs\")\n",
    "\n",
    "# On ferme le run WandB proprement\n",
    "wandb.finish()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Lancement du renderer local\n",
    "renderer = Renderer(render_logs_dir=\"render_logs\")\n",
    "renderer.run()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Agent : RecurentPPO"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "from gym_trading_env.wrapper import DiscreteActionsWrapper\n",
    "from gym_trading_env.renderer import Renderer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "from sb3_contrib import RecurrentPPO\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "config = {\n",
    "    \"policy_type\": \"MlpLstmPolicy\",\n",
    "    \"total_timesteps\": 500_000,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"batch_size\": 128,\n",
    "    \"n_steps\": 2048,\n",
    "    # \"window_size\": 20,  <-- SUPPRIMÉ car géré par le LSTM interne\n",
    "    \"positions\": [0, 0.5, 1.0],\n",
    "    \"project_name\": \"RL-Trading-Project\",\n",
    "    \"run_name\": \"RecurrentPPO_Fix\"\n",
    "}\n",
    "\n",
    "# --- 2. FONCTIONS DE TRAITEMENT ---\n",
    "def calculate_indicators(df):\n",
    "    # RSI\n",
    "    delta = df['close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    df['feature_rsi'] = 100 - (100 / (1 + rs))\n",
    "    df['feature_rsi'] = df['feature_rsi'] / 100.0\n",
    "\n",
    "    # MACD\n",
    "    exp1 = df['close'].ewm(span=12, adjust=False).mean()\n",
    "    exp2 = df['close'].ewm(span=26, adjust=False).mean()\n",
    "    df['feature_macd'] = (exp1 - exp2) / df['close']\n",
    "\n",
    "    # ATR (Volatilité)\n",
    "    high_low = df['high'] - df['low']\n",
    "    high_close = np.abs(df['high'] - df['close'].shift())\n",
    "    low_close = np.abs(df['low'] - df['close'].shift())\n",
    "    true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    df['feature_atr'] = true_range.rolling(14).mean() / df['close']\n",
    "\n",
    "    # Returns\n",
    "    df['feature_return'] = df['close'].pct_change()\n",
    "\n",
    "    return df.dropna()\n",
    "\n",
    "def preprocess(df):\n",
    "    df = df.sort_index().dropna().drop_duplicates()\n",
    "    return calculate_indicators(df)\n",
    "\n",
    "def reward_function(history):\n",
    "    current_val = history['portfolio_valuation', -1]\n",
    "    prev_val = history['portfolio_valuation', -2]\n",
    "    ret = np.log(current_val / prev_val)\n",
    "    # Pénalité de volatilité\n",
    "    risk_penalty = 0.1 * (ret ** 2)\n",
    "    return ret - risk_penalty\n",
    "\n",
    "# --- 3. INITIALISATION WANDB ---\n",
    "run = wandb.init(\n",
    "    project=config[\"project_name\"],\n",
    "    name=config[\"run_name\"],\n",
    "    config=config,\n",
    "    sync_tensorboard=True,\n",
    "    monitor_gym=True,\n",
    "    save_code=True,\n",
    ")\n",
    "\n",
    "# --- 4. CRÉATION DE L'ENVIRONNEMENT ---\n",
    "# CORRECTION ICI : Suppression de window_size\n",
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"./data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    portfolio_initial_value=1000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    "    reward_function=reward_function,\n",
    ")\n",
    "\n",
    "env = DiscreteActionsWrapper(env, positions=config[\"positions\"])\n",
    "\n",
    "# --- 5. CRÉATION DU MODÈLE ET ENTRAÎNEMENT ---\n",
    "model = RecurrentPPO(\n",
    "    config[\"policy_type\"],\n",
    "    env,\n",
    "    verbose=1,\n",
    "    learning_rate=config[\"learning_rate\"],\n",
    "    ent_coef=config[\"ent_coef\"],\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    n_steps=config[\"n_steps\"],\n",
    "    tensorboard_log=f\"runs/{run.id}\"\n",
    ")\n",
    "\n",
    "print(f\"Lancement du run WandB : {run.name}\")\n",
    "model.learn(\n",
    "    total_timesteps=config[\"total_timesteps\"],\n",
    "    callback=WandbCallback(\n",
    "        gradient_save_freq=100,\n",
    "        model_save_path=f\"models/{run.id}\",\n",
    "        verbose=2,\n",
    "    )\n",
    ")\n",
    "\n",
    "model.save(\"recurrent_ppo_final\")\n",
    "\n",
    "# --- 6. ÉVALUATION ---\n",
    "print(\"Évaluation...\")\n",
    "obs, info = env.reset()\n",
    "done, truncated = False, False\n",
    "\n",
    "while not (done or truncated):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    action = int(action)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "final_metrics = env.unwrapped.get_metrics()\n",
    "wandb.log({\n",
    "    \"final_portfolio_valuation\": info['portfolio_valuation'],\n",
    "    \"market_return\": final_metrics.get(\"Market Return\", 0),\n",
    "    \"portfolio_return\": final_metrics.get(\"Portfolio Return\", 0)\n",
    "})\n",
    "\n",
    "wandb.finish()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualisation locale\n",
    "env.unwrapped.save_for_render(dir=\"render_logs\")\n",
    "renderer = Renderer(render_logs_dir=\"render_logs\")\n",
    "renderer.run()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Modèle : RecurrentPPO + Short Bias +modification reward"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "from gym_trading_env.wrapper import DiscreteActionsWrapper\n",
    "from gym_trading_env.renderer import Renderer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "from sb3_contrib import RecurrentPPO\n",
    "\n",
    "# --- 1. CONFIGURATION \"PHASE 2\" ---\n",
    "config = {\n",
    "    \"policy_type\": \"MlpLstmPolicy\",\n",
    "    \"total_timesteps\": 1_000_000,    # DOUBLÉ : Le LSTM a besoin de temps\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"batch_size\": 128,\n",
    "    \"n_steps\": 2048,\n",
    "\n",
    "    # CHANGEMENT MAJEUR : On active le SHORT (-1)\n",
    "    # Positions : [-1 = Short, 0 = Cash, 1 = Long]\n",
    "    # Toujours pas de levier (1.5) pour l'instant, on veut d'abord qu'il maîtrise le sens.\n",
    "    \"positions\": [-1, 0, 1],\n",
    "\n",
    "    \"project_name\": \"RL-Trading-Project\",\n",
    "    \"run_name\": \"RecurrentPPO_Phase2_ShortEnabled\"\n",
    "}\n",
    "\n",
    "# --- 2. INDICATEURS (Inchangés car robustes) ---\n",
    "def calculate_indicators(df):\n",
    "    # RSI\n",
    "    delta = df['close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    df['feature_rsi'] = 100 - (100 / (1 + rs))\n",
    "    df['feature_rsi'] = df['feature_rsi'] / 100.0\n",
    "\n",
    "    # MACD\n",
    "    exp1 = df['close'].ewm(span=12, adjust=False).mean()\n",
    "    exp2 = df['close'].ewm(span=26, adjust=False).mean()\n",
    "    df['feature_macd'] = (exp1 - exp2) / df['close']\n",
    "\n",
    "    # ATR (Volatilité)\n",
    "    high_low = df['high'] - df['low']\n",
    "    high_close = np.abs(df['high'] - df['close'].shift())\n",
    "    low_close = np.abs(df['low'] - df['close'].shift())\n",
    "    true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    df['feature_atr'] = true_range.rolling(14).mean() / df['close']\n",
    "\n",
    "    # Returns\n",
    "    df['feature_return'] = df['close'].pct_change()\n",
    "\n",
    "    return df.dropna()\n",
    "\n",
    "def preprocess(df):\n",
    "    df = df.sort_index().dropna().drop_duplicates()\n",
    "    return calculate_indicators(df)\n",
    "\n",
    "# --- 3. RÉCOMPENSE AJUSTÉE ---\n",
    "def reward_function(history):\n",
    "    current_val = history['portfolio_valuation', -1]\n",
    "    prev_val = history['portfolio_valuation', -2]\n",
    "    ret = np.log(current_val / prev_val)\n",
    "\n",
    "    # AJUSTEMENT : Pénalité réduite (0.05 au lieu de 0.1)\n",
    "    # On laisse l'agent prendre un peu plus de risques pour chercher du profit.\n",
    "    risk_penalty = 0.05 * (ret ** 2)\n",
    "\n",
    "    return ret - risk_penalty\n",
    "\n",
    "# --- 4. INIT WANDB ---\n",
    "run = wandb.init(\n",
    "    project=config[\"project_name\"],\n",
    "    name=config[\"run_name\"],\n",
    "    config=config,\n",
    "    sync_tensorboard=True,\n",
    "    monitor_gym=True,\n",
    "    save_code=True,\n",
    ")\n",
    "\n",
    "# --- 5. ENVIRONNEMENT ---\n",
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"./data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    portfolio_initial_value=1000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    "    reward_function=reward_function,\n",
    "    # Rappel : Pas de window_size ici, le LSTM gère sa mémoire\n",
    ")\n",
    "\n",
    "env = DiscreteActionsWrapper(env, positions=config[\"positions\"])\n",
    "\n",
    "# --- 6. MODÈLE & ENTRAÎNEMENT ---\n",
    "model = RecurrentPPO(\n",
    "    config[\"policy_type\"],\n",
    "    env,\n",
    "    verbose=1,\n",
    "    learning_rate=config[\"learning_rate\"],\n",
    "    ent_coef=config[\"ent_coef\"],\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    n_steps=config[\"n_steps\"],\n",
    "    tensorboard_log=f\"runs/{run.id}\"\n",
    ")\n",
    "\n",
    "print(f\"Lancement du run WandB : {run.name} (Short activé)\")\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=config[\"total_timesteps\"],\n",
    "    callback=WandbCallback(\n",
    "        gradient_save_freq=100,\n",
    "        model_save_path=f\"models/{run.id}\",\n",
    "        verbose=2,\n",
    "    )\n",
    ")\n",
    "\n",
    "model.save(\"recurrent_ppo_short_enabled\")\n",
    "\n",
    "# --- 7. ÉVALUATION FINALE ---\n",
    "print(\"Évaluation finale...\")\n",
    "obs, info = env.reset()\n",
    "done, truncated = False, False\n",
    "\n",
    "while not (done or truncated):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    action = int(action)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "final_metrics = env.unwrapped.get_metrics()\n",
    "wandb.log({\n",
    "    \"final_portfolio_valuation\": info['portfolio_valuation'],\n",
    "    \"market_return\": final_metrics.get(\"Market Return\", 0),\n",
    "    \"portfolio_return\": final_metrics.get(\"Portfolio Return\", 0)\n",
    "})\n",
    "\n",
    "wandb.finish()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Rendu visuel\n",
    "env.unwrapped.save_for_render(dir=\"render_logs\")\n",
    "renderer = Renderer(render_logs_dir=\"render_logs\")\n",
    "renderer.run()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Mode \"Alpha Hunter\"\n",
    "*Récompenser uniquement la sur-performance (Alpha) :* Si le marché fait +1% et l'agent fait +1%, récompense = 0. Il ne gagne des points que s'il fait mieux.\n",
    "Booster l'exploration (Entropie) : multiplier par 5 le coefficient d'entropie (ent_coef). Cela l'interdit de se \"figer\" dans une stratégie simple trop vite. Il sera obligé de tester des choses (y compris le Short).\n",
    "Normalisation Dynamique : S'assurer qu'il voit bien les variations."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "from gym_trading_env.wrapper import DiscreteActionsWrapper\n",
    "from gym_trading_env.renderer import Renderer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "from sb3_contrib import RecurrentPPO\n",
    "\n",
    "# --- 1. CONFIGURATION \"ALPHA HUNTER\" ---\n",
    "config = {\n",
    "    \"policy_type\": \"MlpLstmPolicy\",\n",
    "    \"total_timesteps\": 10_000,    # On allonge encore, l'Alpha est dur à trouver\n",
    "    \"learning_rate\": 3e-4,\n",
    "\n",
    "    # CHANGEMENT CRUCIAL : Entropie x5\n",
    "    # Cela force l'agent à essayer des actions \"bizarres\" (comme shorter en bull run)\n",
    "    # au lieu de s'endormir sur une position Long.\n",
    "    \"ent_coef\": 0.05,\n",
    "\n",
    "    \"batch_size\": 256, # Batch plus gros pour lisser le bruit des returns\n",
    "    \"n_steps\": 2048,\n",
    "    \"positions\": [-1, 0, 1],\n",
    "    \"project_name\": \"RL-Trading-Project\",\n",
    "    \"run_name\": \"RecurrentPPO_AlphaHunter\"\n",
    "}\n",
    "\n",
    "# --- 2. TRAITEMENT (Inchangé) ---\n",
    "def calculate_indicators(df):\n",
    "    delta = df['close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    df['feature_rsi'] = 100 - (100 / (1 + rs))\n",
    "    df['feature_rsi'] = df['feature_rsi'] / 100.0\n",
    "\n",
    "    exp1 = df['close'].ewm(span=12, adjust=False).mean()\n",
    "    exp2 = df['close'].ewm(span=26, adjust=False).mean()\n",
    "    df['feature_macd'] = (exp1 - exp2) / df['close']\n",
    "\n",
    "    high_low = df['high'] - df['low']\n",
    "    high_close = np.abs(df['high'] - df['close'].shift())\n",
    "    low_close = np.abs(df['low'] - df['close'].shift())\n",
    "    true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    df['feature_atr'] = true_range.rolling(14).mean() / df['close']\n",
    "\n",
    "    df['feature_return'] = df['close'].pct_change()\n",
    "\n",
    "    return df.dropna()\n",
    "\n",
    "def preprocess(df):\n",
    "    df = df.sort_index().dropna().drop_duplicates()\n",
    "    return calculate_indicators(df)\n",
    "\n",
    "# --- 3. RÉCOMPENSE DIFFERENCIELLE (ALPHA) ---\n",
    "def reward_function(history):\n",
    "    # Performance de l'agent\n",
    "    current_val = history['portfolio_valuation', -1]\n",
    "    prev_val = history['portfolio_valuation', -2]\n",
    "    portfolio_ret = np.log(current_val / prev_val)\n",
    "\n",
    "    # Performance du marché (Data \"close\" est souvent la colonne 0 ou accessible via history)\n",
    "    # Gym-trading-env stocke les données brutes dans history['data_close', t]\n",
    "    current_price = history['data_close', -1]\n",
    "    prev_price = history['data_close', -2]\n",
    "    market_ret = np.log(current_price / prev_price)\n",
    "\n",
    "    # RECOMPENSE = ALPHA (Surperformance)\n",
    "    # Si l'agent fait pareil que le marché, Reward = 0.\n",
    "    # S'il fait mieux (ex: cash quand ça baisse, ou short), Reward > 0.\n",
    "    reward = portfolio_ret - market_ret\n",
    "\n",
    "    # Petit bonus pour l'action (éviter la léthargie)\n",
    "    # reward += 0.00001\n",
    "\n",
    "    return reward\n",
    "\n",
    "# --- 4. RUN WANDB ---\n",
    "run = wandb.init(\n",
    "    project=config[\"project_name\"],\n",
    "    name=config[\"run_name\"],\n",
    "    config=config,\n",
    "    sync_tensorboard=True,\n",
    "    monitor_gym=True,\n",
    "    save_code=True,\n",
    ")\n",
    "\n",
    "# --- 5. ENVIRONNEMENT ---\n",
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"./data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    portfolio_initial_value=1000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    "    reward_function=reward_function,\n",
    ")\n",
    "\n",
    "env = DiscreteActionsWrapper(env, positions=config[\"positions\"])\n",
    "\n",
    "# Création de la callback Wandb\n",
    "wandb_callback = WandbCallback(\n",
    "    gradient_save_freq=100,\n",
    "    model_save_path=f\"models/{run.id}\",\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "# --- 6. MODÈLE ---\n",
    "model = RecurrentPPO(\n",
    "    config[\"policy_type\"],\n",
    "    env,\n",
    "    verbose=1,\n",
    "    learning_rate=config[\"learning_rate\"],\n",
    "    ent_coef=config[\"ent_coef\"], # C'est ici que ça se joue\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    n_steps=config[\"n_steps\"],\n",
    "    tensorboard_log=f\"runs/{run.id}\"\n",
    ")\n",
    "\n",
    "print(f\"--- Démarrage Alpha Hunter ---\")\n",
    "print(f\"Objectif : Battre le Buy & Hold (Reward = Return - Market)\")\n",
    "print(f\"Exploration forcée (Ent_coef={config['ent_coef']})\")\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=config[\"total_timesteps\"],\n",
    "    callback=wandb_callback\n",
    ")\n",
    "\n",
    "print(\"Fin de l'apprentissage\")\n",
    "\n",
    "model.save(\"recurrent_ppo_alpha_hunter\")\n",
    "\n",
    "# --- 7. EVALUATION ---\n",
    "obs, info = env.reset()\n",
    "done, truncated = False, False\n",
    "\n",
    "while not (done or truncated):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    action = int(action)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "metrics = env.unwrapped.get_metrics()\n",
    "print(\"Métriques finales :\", metrics)\n",
    "\n",
    "market_return = float(metrics.get(\"Market Return\", \"0%\").replace('%',''))\n",
    "portfolio_return = float(metrics.get(\"Portfolio Return\", \"0%\").replace('%', ''))\n",
    "metrics_float = {'Market Return (%): ': market_return, 'Portfolio Return (%): ': portfolio_return}\n",
    "wandb.log(metrics_float)\n",
    "\n",
    "wandb.finish()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "env.unwrapped.save_for_render(dir=\"render_logs\")\n",
    "renderer = Renderer(render_logs_dir=\"render_logs\")\n",
    "renderer.run()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Alpha Hunter + Amplification des récompenses"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "from gym_trading_env.wrapper import DiscreteActionsWrapper\n",
    "from gym_trading_env.renderer import Renderer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "from sb3_contrib import RecurrentPPO\n",
    "\n",
    "# --- 1. CONFIGURATION \"PHASE 3\" ---\n",
    "config = {\n",
    "    \"policy_type\": \"MlpLstmPolicy\",\n",
    "    \"total_timesteps\": 1_000_000,\n",
    "    \"learning_rate\": 1e-4, # On ralentit un peu le learning rate car on booste les rewards\n",
    "    \"ent_coef\": 0.02,      # On remet une valeur standard car le scaling va suffire\n",
    "    \"batch_size\": 256,\n",
    "    \"n_steps\": 2048,\n",
    "    \"positions\": [-1, 0, 1],\n",
    "    \"project_name\": \"RL-Trading-Project\",\n",
    "    \"run_name\": \"RecurrentPPO_ScaledReward\"\n",
    "}\n",
    "\n",
    "# --- 2. TRAITEMENT AVEC LAGS (Mémoire explicite) ---\n",
    "def calculate_indicators(df):\n",
    "    # On garde les classiques\n",
    "    df['feature_return'] = df['close'].pct_change()\n",
    "\n",
    "    # RSI\n",
    "    delta = df['close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    df['feature_rsi'] = (100 - (100 / (1 + rs))) / 100.0\n",
    "\n",
    "    # MACD\n",
    "    exp1 = df['close'].ewm(span=12, adjust=False).mean()\n",
    "    exp2 = df['close'].ewm(span=26, adjust=False).mean()\n",
    "    df['feature_macd'] = (exp1 - exp2) / df['close']\n",
    "\n",
    "    # ATR (Volatilité)\n",
    "    high_low = df['high'] - df['low']\n",
    "    high_close = np.abs(df['high'] - df['close'].shift())\n",
    "    low_close = np.abs(df['low'] - df['close'].shift())\n",
    "    true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    df['feature_atr'] = true_range.rolling(14).mean() / df['close']\n",
    "\n",
    "    # AJOUT : Log Returns passés (Donner le contexte immédiat explicitement)\n",
    "    # Même si le LSTM a de la mémoire, lui donner les 5 dernières variations aide énormément\n",
    "    for i in range(1, 6):\n",
    "        df[f'feature_ret_lag_{i}'] = df['feature_return'].shift(i)\n",
    "\n",
    "    return df.dropna()\n",
    "\n",
    "def preprocess(df):\n",
    "    df = df.sort_index().dropna().drop_duplicates()\n",
    "    return calculate_indicators(df)\n",
    "\n",
    "# --- 3. RÉCOMPENSE AMPLIFIÉE (SCALED) ---\n",
    "def reward_function(history):\n",
    "    # 1. Calcul du rendement Alpha\n",
    "    current_val = history['portfolio_valuation', -1]\n",
    "    prev_val = history['portfolio_valuation', -2]\n",
    "    portfolio_ret = np.log(current_val / prev_val)\n",
    "\n",
    "    current_price = history['data_close', -1]\n",
    "    prev_price = history['data_close', -2]\n",
    "    market_ret = np.log(current_price / prev_price)\n",
    "\n",
    "    alpha = portfolio_ret - market_ret\n",
    "\n",
    "    # 2. SCALING : C'est ici que la magie opère\n",
    "    # On multiplie par 1000.\n",
    "    # Un Alpha de 0.05% (0.0005) devient 0.5 points.\n",
    "    # Un Alpha de 1% (0.01) devient 10 points.\n",
    "    reward = alpha * 1000\n",
    "\n",
    "    return reward\n",
    "\n",
    "# --- 4. RUN WANDB ---\n",
    "run = wandb.init(\n",
    "    project=config[\"project_name\"],\n",
    "    name=config[\"run_name\"],\n",
    "    config=config,\n",
    "    sync_tensorboard=True,\n",
    "    monitor_gym=True,\n",
    "    save_code=True,\n",
    ")\n",
    "\n",
    "# --- 5. ENVIRONNEMENT ---\n",
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"./data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    portfolio_initial_value=1000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    "    reward_function=reward_function,\n",
    ")\n",
    "\n",
    "env = DiscreteActionsWrapper(env, positions=config[\"positions\"])\n",
    "\n",
    "# --- 6. MODÈLE ---\n",
    "model = RecurrentPPO(\n",
    "    config[\"policy_type\"],\n",
    "    env,\n",
    "    verbose=1,\n",
    "    learning_rate=config[\"learning_rate\"],\n",
    "    ent_coef=config[\"ent_coef\"],\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    n_steps=config[\"n_steps\"],\n",
    "    tensorboard_log=f\"runs/{run.id}\"\n",
    ")\n",
    "\n",
    "print(f\"--- Démarrage Phase 3 (Scaled Rewards) ---\")\n",
    "print(f\"Les récompenses sont multipliées par 1000 pour aider le réseau.\")\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=config[\"total_timesteps\"],\n",
    "    callback=WandbCallback(\n",
    "        gradient_save_freq=100,\n",
    "        model_save_path=f\"models/{run.id}\",\n",
    "        verbose=2,\n",
    "    )\n",
    ")\n",
    "\n",
    "model.save(\"recurrent_ppo_scaled\")\n",
    "\n",
    "# --- 7. EVALUATION ---\n",
    "obs, info = env.reset()\n",
    "done, truncated = False, False\n",
    "\n",
    "while not (done or truncated):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    action = int(action)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "metrics = env.unwrapped.get_metrics()\n",
    "print(\"Métriques finales :\", metrics)\n",
    "\n",
    "wandb.finish()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "env.unwrapped.save_for_render(dir=\"render_logs\")\n",
    "renderer = Renderer(render_logs_dir=\"render_logs\")\n",
    "renderer.run()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
