{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL -- Projet \"Trading automatique\"\n",
    "\n",
    "Ce notebook contient du code de base et quelques explications pour vous aider sur ce sujet.\n",
    "\n",
    "Vous êtes libres de réaliser ce projet avec des scripts Python ou des Jupyter Notebooks, à votre convenance.\n",
    "\n",
    "Vous devez télécharger les paquets Python suivants :\n",
    "\n",
    "```sh\n",
    "pip install gymnasium\n",
    "pip install pandas\n",
    "pip install gym-trading-env-continuous\n",
    "```\n",
    "\n",
    "Vous utiliserez l'environnement `gym-trading-env-continuous`, qui est un *fork* de [Gym Trading Env](https://gym-trading-env.readthedocs.io/en/latest/index.html). La différence majeure est expliquée dans ce document ; la documentation originelle reste utilisable."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T09:33:05.457425Z",
     "start_time": "2025-12-11T09:33:04.900982Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "import gym_trading_env"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilisation des données de simulation\n",
    "\n",
    "Les données sont dans un format binaire (Pickle) que vous pouvez lire avec Pandas. Vous devez vous assurer que les données sont triées par date.\n",
    "\n",
    "Des étapes de prétraitement peuvent aider votre apprentissage, par exemple, supprimer les doublons, etc."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T09:33:05.499527Z",
     "start_time": "2025-12-11T09:33:05.465013Z"
    }
   },
   "source": [
    "def preprocess(df):\n",
    "    df = df.sort_index()\n",
    "    df = df.dropna()\n",
    "    df = df.drop_duplicates()\n",
    "    return df\n",
    "\n",
    "df = preprocess(pd.read_pickle('./data/binance-ETHUSD-1h.pkl'))\n",
    "df.head(5)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                       open    high     low   close       volume  \\\n",
       "date_open                                                          \n",
       "2020-08-18 07:00:00  430.00  435.00  410.00  430.30   487.154463   \n",
       "2020-08-18 08:00:00  430.27  431.79  430.27  430.80   454.176153   \n",
       "2020-08-18 09:00:00  430.86  431.13  428.71  429.35  1183.710884   \n",
       "2020-08-18 10:00:00  429.75  432.69  428.59  431.90  1686.183227   \n",
       "2020-08-18 11:00:00  432.09  432.89  426.99  427.45  1980.692724   \n",
       "\n",
       "                             date_close  \n",
       "date_open                                \n",
       "2020-08-18 07:00:00 2020-08-18 08:00:00  \n",
       "2020-08-18 08:00:00 2020-08-18 09:00:00  \n",
       "2020-08-18 09:00:00 2020-08-18 10:00:00  \n",
       "2020-08-18 10:00:00 2020-08-18 11:00:00  \n",
       "2020-08-18 11:00:00 2020-08-18 12:00:00  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>date_close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_open</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-08-18 07:00:00</th>\n",
       "      <td>430.00</td>\n",
       "      <td>435.00</td>\n",
       "      <td>410.00</td>\n",
       "      <td>430.30</td>\n",
       "      <td>487.154463</td>\n",
       "      <td>2020-08-18 08:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-18 08:00:00</th>\n",
       "      <td>430.27</td>\n",
       "      <td>431.79</td>\n",
       "      <td>430.27</td>\n",
       "      <td>430.80</td>\n",
       "      <td>454.176153</td>\n",
       "      <td>2020-08-18 09:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-18 09:00:00</th>\n",
       "      <td>430.86</td>\n",
       "      <td>431.13</td>\n",
       "      <td>428.71</td>\n",
       "      <td>429.35</td>\n",
       "      <td>1183.710884</td>\n",
       "      <td>2020-08-18 10:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-18 10:00:00</th>\n",
       "      <td>429.75</td>\n",
       "      <td>432.69</td>\n",
       "      <td>428.59</td>\n",
       "      <td>431.90</td>\n",
       "      <td>1686.183227</td>\n",
       "      <td>2020-08-18 11:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-18 11:00:00</th>\n",
       "      <td>432.09</td>\n",
       "      <td>432.89</td>\n",
       "      <td>426.99</td>\n",
       "      <td>427.45</td>\n",
       "      <td>1980.692724</td>\n",
       "      <td>2020-08-18 12:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajout de *features*\n",
    "\n",
    "Vous pouvez également rajouter de nouvelles données au DataFrame pour créer de nouvelles *features* que l'agent pourra utiliser.\n",
    "Voir pour cela la [doc](https://gym-trading-env.readthedocs.io/en/latest/features.html).\n",
    "\n",
    "Chaque nouvelle *feature* doit commencer par `feature_` pour être détectée."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T09:33:05.543264Z",
     "start_time": "2025-12-11T09:33:05.508123Z"
    }
   },
   "source": [
    "def preprocess(df):\n",
    "    df = df.sort_index()\n",
    "    df = df.dropna()\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    df['feature_close'] = (df['close'] - df['close'].mean()) / df['close'].std()\n",
    "\n",
    "    return df\n",
    "\n",
    "df = preprocess(pd.read_pickle('./data/binance-ETHUSD-1h.pkl'))\n",
    "df.head(5)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                       open    high     low   close       volume  \\\n",
       "date_open                                                          \n",
       "2020-08-18 07:00:00  430.00  435.00  410.00  430.30   487.154463   \n",
       "2020-08-18 08:00:00  430.27  431.79  430.27  430.80   454.176153   \n",
       "2020-08-18 09:00:00  430.86  431.13  428.71  429.35  1183.710884   \n",
       "2020-08-18 10:00:00  429.75  432.69  428.59  431.90  1686.183227   \n",
       "2020-08-18 11:00:00  432.09  432.89  426.99  427.45  1980.692724   \n",
       "\n",
       "                             date_close  feature_close  \n",
       "date_open                                               \n",
       "2020-08-18 07:00:00 2020-08-18 08:00:00      -1.891634  \n",
       "2020-08-18 08:00:00 2020-08-18 09:00:00      -1.891128  \n",
       "2020-08-18 09:00:00 2020-08-18 10:00:00      -1.892594  \n",
       "2020-08-18 10:00:00 2020-08-18 11:00:00      -1.890016  \n",
       "2020-08-18 11:00:00 2020-08-18 12:00:00      -1.894514  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>date_close</th>\n",
       "      <th>feature_close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_open</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-08-18 07:00:00</th>\n",
       "      <td>430.00</td>\n",
       "      <td>435.00</td>\n",
       "      <td>410.00</td>\n",
       "      <td>430.30</td>\n",
       "      <td>487.154463</td>\n",
       "      <td>2020-08-18 08:00:00</td>\n",
       "      <td>-1.891634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-18 08:00:00</th>\n",
       "      <td>430.27</td>\n",
       "      <td>431.79</td>\n",
       "      <td>430.27</td>\n",
       "      <td>430.80</td>\n",
       "      <td>454.176153</td>\n",
       "      <td>2020-08-18 09:00:00</td>\n",
       "      <td>-1.891128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-18 09:00:00</th>\n",
       "      <td>430.86</td>\n",
       "      <td>431.13</td>\n",
       "      <td>428.71</td>\n",
       "      <td>429.35</td>\n",
       "      <td>1183.710884</td>\n",
       "      <td>2020-08-18 10:00:00</td>\n",
       "      <td>-1.892594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-18 10:00:00</th>\n",
       "      <td>429.75</td>\n",
       "      <td>432.69</td>\n",
       "      <td>428.59</td>\n",
       "      <td>431.90</td>\n",
       "      <td>1686.183227</td>\n",
       "      <td>2020-08-18 11:00:00</td>\n",
       "      <td>-1.890016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-18 11:00:00</th>\n",
       "      <td>432.09</td>\n",
       "      <td>432.89</td>\n",
       "      <td>426.99</td>\n",
       "      <td>427.45</td>\n",
       "      <td>1980.692724</td>\n",
       "      <td>2020-08-18 12:00:00</td>\n",
       "      <td>-1.894514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par défaut, l'agent ne reçoit comme *features* que sa dernière *position* (voir le paragraphe suivant), ce qui ne sera certainement pas suffisant ! À vous d'ajouter les *features* qui seront pertinentes pour que l'agent apprenne la politique optimale..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctionnement des actions\n",
    "\n",
    "Une action est une **position**, c'est-à-dire un ratio entre la proportion d'*assets* (exemple : ETH) et la proportion de *fiat* (exemple : USD) dans le portefeuille.\n",
    "Ainsi, la position `0.5` consiste à avoir exactement 50% d'ETH et 50% d'USD (en vendant l'un ou l'autre pour arriver à ce ratio). `0.1` consiste à avoir 10% d'ETH et 90% d'USD.\n",
    "\n",
    "Il existe des positions un peu plus complexes :\n",
    "\n",
    "- `< 0` : une position inférieure à 0 va vendre encore plus d'ETH que le portefeuille n'en contient, pour obtenir des USD. Cela nécessite un emprunt, qui sera remboursé avec un intérêt.\n",
    "- `> 1` : une position supérieure à 1 va dépenser encore plus d'USD que le portefeuille n'en contient, pour acheter des ETH. Cela nécessite également un emprunt.\n",
    "\n",
    "Ces positions (qui sont appelées *short* et *margin* en finance) peuvent faire gagner beaucoup à votre agent, mais démultiplient les risques également. Si votre agent fait une bonne affaire, vous pouvez vendre à un prix élevé, racheter quand le prix est plus faible, et rembourser l'emprunt en empochant la différence. En revanche, si votre agent fait une mauvaise affaire, et doit vider son portefeuille pour rembourser l'emprunt, vous perdez automatiquement (`terminated=True`).\n",
    "\n",
    "### Actions continues\n",
    "\n",
    "Par rapport à l'environnement `gym-trading-env` d'origine, la version que je vous fournis permet de spécifier directement une position comme action, c'est-à-dire un nombre flottant. Votre agent a donc un contrôle précis sur la position désirée. Cela rajoute de la flexibilité mais rend l'apprentissage beaucoup plus difficile.\n",
    "\n",
    "Exemple :"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T09:33:05.622272Z",
     "start_time": "2025-12-11T09:33:05.552640Z"
    }
   },
   "source": [
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    portfolio_initial_value=1_000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    ")\n",
    "\n",
    "obs, _ = env.reset()\n",
    "# On veut une position de 88% ETH / 12% USD\n",
    "obs, reward, terminated, truncated, info = env.step(0.88)\n",
    "print(obs)\n",
    "print(info)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.1176176   0.88        0.86199826]\n",
      "{'idx': 1, 'step': 1, 'date': np.datetime64('2021-01-29T13:00:00.000000000'), 'position_index': None, 'position': 0.88, 'real_position': np.float64(0.8619982420158219), 'data_volume': 47022463.7, 'data_open': 0.05181, 'data_date_close': Timestamp('2021-01-29 14:00:00'), 'data_high': 0.05699, 'data_close': 0.04413, 'data_low': 0.042, 'portfolio_valuation': np.float64(868.8994655314681), 'portfolio_distribution_asset': np.float64(16972.35014223006), 'portfolio_distribution_fiat': np.float64(119.90965375485541), 'portfolio_distribution_borrowed_asset': 0, 'portfolio_distribution_borrowed_fiat': 0, 'portfolio_distribution_interest_asset': 0.0, 'portfolio_distribution_interest_fiat': 0.0, 'reward': np.float64(-0.14052785024653625)}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par défaut, l'espace des actions est limité à $[-1, 2]$ pour que votre agent ne puisse emprunter que jusqu'à 100%. Vous pouvez empêcher votre agent de prendre de telles positions, ou limiter le risque, en contrôlant les bornes autorisées des actions.\n",
    "\n",
    "Par exemple, en clippant l'action dans l'intervalle $[0,1]$, vous empêchez l'agent de faire des emprunts.\n",
    "\n",
    "À l'inverse, vous pouvez augmenter l'intervalle pour permettre des emprunts plus risqués, mais qui peuvent rapporter plus. À vous de choisir !\n",
    "\n",
    "Vous pouvez changer les bornes via le paramètre `position_range` du constructeur :"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T09:33:05.654986Z",
     "start_time": "2025-12-11T09:33:05.630508Z"
    }
   },
   "source": [
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    position_range=(0, 1),  # ICI : (borne min, borne max)\n",
    "    portfolio_initial_value=1_000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez aussi modifier l'action en sortie de votre algorithme d'apprentissage, de la manière que vous souhaitez (clipping, interpolation, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions discrètes\n",
    "\n",
    "Pour simplifier l'apprentissage, vous pouvez utiliser le *wrapper* `gym_trading_env.wrapper.DiscreteActionsWrapper` que je vous fournis, et qui permet de revenir au fonctionnement d'origine de l'environnement `gym-trading-env`. Vous devrez alors spécifier l'ensemble des positions possibles, puis votre agent choisira une position parmi cette liste à chaque pas de temps.\n",
    "Par exemple, si la liste des positions est `[0, 0.5, 1]` et que l'action choisie est `1`, cela veut dire qu'on veut la position qui correspond au 2e élément de la liste, soit `0.5` (50%/50%).\n",
    "\n",
    "Vous pouvez rajouter autant d'actions que vous voulez, par exemple `[0, 0.25, 0.5, 1]` ou encore tous les 0.1 entre 0 et 1, etc. Plus il y a d'actions possibles, plus votre agent aura de choix (flexibilité), donc plus son comportement pourra être complexe, mais cela rajoute de la difficulté durant l'entraînement.\n",
    "\n",
    "N'oubliez pas que vous pouvez autoriser les positions avec emprunt en ajoutant des nombres inférieurs à 0 ou supérieurs à 1 à la liste autorisée.\n",
    "\n",
    "Exemple :"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T09:33:05.681956Z",
     "start_time": "2025-12-11T09:33:05.662301Z"
    }
   },
   "source": [
    "from gym_trading_env.wrapper import DiscreteActionsWrapper\n",
    "\n",
    "# Vous pouvez aussi appeler le wrapper `env` pour faire plus simple\n",
    "# Ici, je fais explicitement la distinction entre `wrapper` et `env`\n",
    "wrapper = DiscreteActionsWrapper(env, positions=[-1, 0, 0.25, 0.5, 0.75, 1, 2])\n",
    "obs, _ = wrapper.reset()\n",
    "# On veut une position de 25% ETH / 75% USD ; cela correspond à la position\n",
    "# d'index 2 dans la liste ci-dessus\n",
    "obs, reward, terminated, truncated, info = wrapper.step(2)\n",
    "print(obs)\n",
    "print(info)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.16626613  0.25        0.25014377]\n",
      "{'idx': 1, 'step': 1, 'date': np.datetime64('2023-11-21T14:00:00.000000000'), 'position_index': 2, 'position': 0.25, 'real_position': np.float64(0.2501437786412375), 'data_volume': 0, 'data_open': 1.0949305295944214, 'data_date_close': Timestamp('2023-11-21 15:00:00'), 'data_high': 1.0961307287216187, 'data_close': 1.0955302715301514, 'data_low': 1.0948106050491333, 'portfolio_valuation': np.float64(1000.0093399728391), 'portfolio_distribution_asset': np.float64(228.33336647827292), 'portfolio_distribution_fiat': np.float64(749.8632249955033), 'portfolio_distribution_borrowed_asset': 0, 'portfolio_distribution_borrowed_fiat': 0, 'portfolio_distribution_interest_asset': 0.0, 'portfolio_distribution_interest_fiat': 0.0, 'reward': np.float64(9.339929221864947e-06)}\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notez que, quand les actions continues sont utilisées, la variable `position_index` du dictionnaire `info` n'est pas disponible (c'est logique)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changement de la fonction de récompense\n",
    "\n",
    "Vous pouvez changer la fonction de récompense pour améliorer l'apprentissage de l'agent.\n",
    "Dans tous les cas, vous serez évalué(e)s sur la valuation du portefeuille à la fin de l'épisode (voir [ci-dessous](#évaluation)), mais cette simple mesure n'est peut-être pas la meilleure fonction de récompense.\n",
    "D'autres fonctions peuvent encourager l'agent à mieux apprendre, en explorant diverses possibilités, etc."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T09:33:05.745429Z",
     "start_time": "2025-12-11T09:33:05.688496Z"
    }
   },
   "source": [
    "def reward_function(history):\n",
    "    return history['portfolio_valuation', -1]\n",
    "\n",
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    portfolio_initial_value=1_000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    "    # On spécifie la fonction de récompense\n",
    "    reward_function=reward_function,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Déroulément d'un épisode\n",
    "\n",
    "Un épisode se déroule jusqu'à ce que :\n",
    "\n",
    "- l'agent atteigne la fin des données d'entraînement (nous n'avons plus de nouvelle donnée) => `truncated=True`\n",
    "\n",
    "- la valeur du portefeuille atteint 0 (l'agent a perdu tout l'argent) => `terminated=True`\n",
    "\n",
    "Vous devrez probablement entraîner l'agent sur plusieurs épisodes avant que son comportement ne converge.\n",
    "\n",
    "Pour éviter de sur-apprendre (*overfit*), vous devrez utiliser plusieurs jeux de données via [MultiDatasetTradingEnv](https://gym-trading-env.readthedocs.io/en/latest/multi_datasets.html).\n",
    "\n",
    "Dans ce cas, chaque épisode utilisera un jeu de données différent (en bouclant si vous demandez plus d'épisodes qu'il n'y a de jeux de données). Vous pouvez accéder au nom du jeu de données de l'épisode en cours via `env.name`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T09:33:06.549346Z",
     "start_time": "2025-12-11T09:33:05.752216Z"
    }
   },
   "source": [
    "nb_episodes = 2\n",
    "for episode in range(1, nb_episodes + 1):\n",
    "    obs, _ = env.reset()\n",
    "    print(f'Episode n˚{episode} -- Jeu de donnée {env.name}')\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "    if terminated:\n",
    "        print('Argent perdu')\n",
    "    elif truncated:\n",
    "        print('Épisode terminé')\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode n˚1 -- Jeu de donnée yfinance-EURUSD-1h.pkl\n",
      "Market Return :  5.27%   |   Portfolio Return : -100.00%   |   \n",
      "Épisode terminé\n",
      "Episode n˚2 -- Jeu de donnée yfinance-AAPL-1h.pkl\n",
      "Market Return : 39.94%   |   Portfolio Return : -96.92%   |   \n",
      "Épisode terminé\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Évaluation\n",
    "\n",
    "Afin de disposer d'un critère simple pour comparer les différentes solutions, nous utiliserons la valeur du portefeuille (`portfolio_valuation`).\n",
    "C'est assez simple : on veut que l'agent ait gagné le plus d'argent à la fin de la simulation.\n",
    "\n",
    "Vous pouvez ajouter ce critère à la liste des métriques affichées à la fin de chaque épisode, pour que ce soit plus visible :"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T09:33:06.773682Z",
     "start_time": "2025-12-11T09:33:06.555285Z"
    }
   },
   "source": [
    "def metric_portfolio_valuation(history):\n",
    "    return round(history['portfolio_valuation', -1], 2)\n",
    "\n",
    "env.add_metric('Portfolio Valuation', metric_portfolio_valuation)\n",
    "\n",
    "done = False\n",
    "obs, _ = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 10.54%   |   Portfolio Return : -98.67%   |   Portfolio Valuation : 13.33   |   \n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puisque l'environnement peut se dérouler sur plusieurs épisodes (1 par jeu de données), vous devrez calculer la **moyenne des `portfolio_valuation`** sur l'ensemble des jeux de données possibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ Pour que ce soit honnête, vous **devez initialiser l'environnement avec les contraintes** imposées dans le sujet :\n",
    "\n",
    "- une valeur initiale du portefeuille de `1000` ;\n",
    "- des frais de 0.1% par transaction ;\n",
    "- un taux d'intérêt de 0.02% par jour soit 0.02/100/24 par heure.\n",
    "\n",
    "Sinon, il est beaucoup plus simple d'augmenter la valeur finale...\n",
    "\n",
    "```py\n",
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    # LIGNES SUIVANTES :\n",
    "    # Valeur initiale du portefeuille\n",
    "    portfolio_initial_value=1_000,\n",
    "    # Frais de transactions\n",
    "    trading_fees=0.1/100,\n",
    "    # Intérêts sur les prêts\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    ")\n",
    "```\n",
    "\n",
    "Vous pouvez également accéder à la métrique de `portfolio_valuation` à la fin d'une simulation, si vous voulez par exemple l'ajouter à votre *run* WandB :"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T09:33:06.784809Z",
     "start_time": "2025-12-11T09:33:06.780582Z"
    }
   },
   "source": [
    "portfolio_valuation = env.historical_info['portfolio_valuation', -1]\n",
    "# Si on avait WandB :\n",
    "# run.summary['portfolio_valuation'] = portfolio_valuation\n",
    "# On simule ça par un simple print...\n",
    "print(portfolio_valuation)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.32516244673074\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou bien, pour récupérer les métriques calculées par l'environnement (cela peut être utile pour les ajouter à WandB) :"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T09:33:06.804983Z",
     "start_time": "2025-12-11T09:33:06.801548Z"
    }
   },
   "source": [
    "metrics = env.get_metrics()\n",
    "print(metrics)\n",
    "portfolio_valuation = metrics['Portfolio Valuation']\n",
    "print(portfolio_valuation)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Market Return': '10.54%', 'Portfolio Return': '-98.67%', 'Portfolio Valuation': np.float64(13.33)}\n",
      "13.33\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conseils\n",
    "\n",
    "À part les quelques contraintes présentées dans ce fichier (et rappelées sur la page du projet), vous êtes assez libres !\n",
    "\n",
    "Votre algorithme de RL peut être arbitrairement simple ou complexe. Je liste ici quelques conseils ou pistes, que vous pouvez explorer :\n",
    "\n",
    "- *Features* : Par défaut, votre agent n'utilise que le prix de l'*asset* (`close`) comme *feature* pour la prise de décision. Vous pouvez ajouter les *features* que vous voulez. En particulier, des métriques spécifiques à la finance peuvent être intéressantes, par exemple pour déterminer le risque que le prix change brutalement (à la hausse ou à la baisse)...\n",
    "\n",
    "- Algorithme : Vous pouvez utiliser des algorithmes existants, ou en inventer un nouveau. N'hésitez pas à ré-utiliser tout ce que vous avez appris en *Machine Learning* et *Deep Learning*. Typiquement, les données financières sont des données temporelles : certains réseaux de neurones sont plus appropriés que d'autres pour ce genre de tâche...\n",
    "\n",
    "- Configuration de l'environnement : L'environnement est très extensible ! Vous pouvez par exemple ajouter des *features* dynamiques (pas seulement calculées lors du prétraitement). La [documentation](https://gym-trading-env.readthedocs.io/en/latest/index.html) est très claire et très complète.\n",
    "\n",
    "Vous pouvez vous inspirer de travaux existants trouvés sur l'Internet à condition de **citer votre source**. Utiliser le travail de quelqu'un d'autre sans le citer sera considéré comme du plagiat."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T09:33:06.828323Z",
     "start_time": "2025-12-11T09:33:06.822583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_v2(df):\n",
    "    df = df.sort_index().dropna().drop_duplicates()\n",
    "\n",
    "    # --- 1. Log Returns (Rendements Logarithmiques) ---\n",
    "    df[\"feature_log_returns\"] = np.log(df[\"close\"]).diff()\n",
    "\n",
    "    # --- 2. Indicateurs de Volatilité (ATR simplifié) ---\n",
    "    df['tr1'] = df['high'] - df['low']\n",
    "    df['tr2'] = np.abs(df['high'] - df['close'].shift(1))\n",
    "    df['tr3'] = np.abs(df['low'] - df['close'].shift(1))\n",
    "    df['tr'] = df[['tr1', 'tr2', 'tr3']].max(axis=1)\n",
    "    df['feature_atr'] = df['tr'].rolling(window=14).mean() / df[\"close\"]\n",
    "\n",
    "    # --- 3. Indicateurs de Tendance (MACD) ---\n",
    "    ema_fast = df['close'].ewm(span=12, adjust=False).mean()\n",
    "    ema_slow = df['close'].ewm(span=26, adjust=False).mean()\n",
    "    df['feature_macd'] = ema_fast - ema_slow\n",
    "    df['feature_macd_signal'] = df['feature_macd'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "    # --- 4. Indicateurs de Momentum (RSI) ---\n",
    "    delta = df['close'].diff()\n",
    "    gain = delta.where(delta > 0, 0)\n",
    "    loss = -delta.where(delta < 0, 0)\n",
    "    avg_gain = gain.rolling(window=14).mean()\n",
    "    avg_loss = loss.rolling(window=14).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    df['feature_rsi'] = 100 - (100 / (1 + rs)) / 100\n",
    "\n",
    "    # --- 5. Nettoyage et Normalisation ---\n",
    "    df = df.dropna()\n",
    "    cols_to_normalize = ['feature_log_returns', 'feature_macd', 'feature_macd_signal', 'feature_atr']\n",
    "    for col in cols_to_normalize:\n",
    "        if df[col].std() > 0:\n",
    "            df[col] = (df[col] - df[col].mean()) / df[col].std()\n",
    "        else:\n",
    "             df[col] = 0.0\n",
    "\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T09:33:06.844559Z",
     "start_time": "2025-12-11T09:33:06.840144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def reward_function_v2(history):\n",
    "    # Rendement log du portefeuille à l'étape t\n",
    "    prev_val = history['portfolio_valuation', -2]\n",
    "    curr_val = history['portfolio_valuation', -1]\n",
    "\n",
    "    # Gestion du cas initial\n",
    "    if prev_val == 0: return 0\n",
    "\n",
    "    # Calcul du rendement log\n",
    "    reward = np.log(curr_val / prev_val)\n",
    "\n",
    "    # BONUS : Pénalité de risque (Sharpe Ratio simplifié)\n",
    "    # Si vous voulez un agent prudent, vous pouvez soustraire une fraction de la volatilité récente\n",
    "    return reward"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Chargement et Configuration"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T09:39:42.115816Z",
     "start_time": "2025-12-11T09:33:06.853456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# --- NOUVELLES IMPORTATIONS ---\n",
    "import wandb\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "# --- HYPERPARAMÈTRES (pour le suivi WandB) ---\n",
    "config = {\n",
    "    \"policy_type\": \"MlpLstmPolicy\",\n",
    "    \"total_timesteps\": 100_000,\n",
    "    \"env_id\": \"MultiDatasetTradingEnv\",\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"n_steps\": 2048,\n",
    "    \"batch_size\": 128,\n",
    "    \"ent_coef\": 0.01,\n",
    "    # Ajoutez ici tous les paramètres de l'environnement (frais, capital, etc.)\n",
    "}\n",
    "\n",
    "# --- 1. INITIALISATION DE WANDB ---\n",
    "run = wandb.init(\n",
    "    project=\"RL-Trading-Project\", # Nom de votre projet\n",
    "    entity=\"votre_nom_utilisateur\", # Remplacez par votre nom d'utilisateur WandB\n",
    "    config=config,\n",
    "    sync_tensorboard=True, # Synchroniser TensorBoard (si vous l'utilisez encore)\n",
    "    monitor_gym=True,\n",
    "    save_code=True,\n",
    ")\n",
    "\n",
    "# --- 2. CRÉATION DE L'ENVIRONNEMENT ET DU MODÈLE (Comme avant) ---\n",
    "\n",
    "# ... (Vos fonctions preprocess/reward_function ici) ...\n",
    "\n",
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    reward_function=reward_function,\n",
    "    position_range=(-1, 1),\n",
    "    portfolio_initial_value=1_000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    ")\n",
    "\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "model = RecurrentPPO(\n",
    "    config[\"policy_type\"], # Utiliser le dictionnaire de config\n",
    "    env,\n",
    "    verbose=0, # Mettre à 0 pour éviter les logs console en faveur de WandB\n",
    "    learning_rate=config[\"learning_rate\"],\n",
    "    n_steps=config[\"n_steps\"],\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    ent_coef=config[\"ent_coef\"],\n",
    "    tensorboard_log=f\"runs/{run.id}\", # Pointer le log TensorBoard vers le dossier de WandB\n",
    ")\n",
    "\n",
    "# --- 3. DÉFINITION DU CALLBACK WANDB ---\n",
    "wandb_callback = WandbCallback(\n",
    "    model_save_path=f\"models/{run.id}\",\n",
    "    verbose=1,\n",
    "    model_save_freq=10000, # Sauvegarder le modèle tous les 10,000 pas\n",
    ")\n",
    "\n",
    "\n",
    "# --- 4. ENTRAÎNEMENT AVEC LE CALLBACK ---\n",
    "try:\n",
    "    print(\"Début de l'entraînement avec WandB...\")\n",
    "    model.learn(\n",
    "        total_timesteps=config[\"total_timesteps\"],\n",
    "        callback=wandb_callback, # Passage du Callback ici\n",
    "    )\n",
    "finally:\n",
    "    # --- 5. FIN DU RUN WANDB ---\n",
    "    run.finish()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Début de l'entraînement...\n",
      "Logging to ./tensorboard_logs/RecurrentPPO_1\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 691  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 362          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022877017 |\n",
      "|    clip_fraction        | 0.00981      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -0.845       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0212      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.000946    |\n",
      "|    std                  | 0.991        |\n",
      "|    value_loss           | 0.0016       |\n",
      "------------------------------------------\n",
      "Market Return : 26.96%   |   Portfolio Return : -96.64%   |   \n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 338          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011980999 |\n",
      "|    clip_fraction        | 0.00225      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | -0.322       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0177      |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00103     |\n",
      "|    std                  | 0.97         |\n",
      "|    value_loss           | 0.000582     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 296         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006036695 |\n",
      "|    clip_fraction        | 0.0404      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | -0.328      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00254    |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00307    |\n",
      "|    std                  | 0.95        |\n",
      "|    value_loss           | 0.000466    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 298         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013248351 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | -0.0913     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0315     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0139     |\n",
      "|    std                  | 0.937       |\n",
      "|    value_loss           | 0.00128     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 298         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008153271 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | -0.0747     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0577     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    std                  | 0.921       |\n",
      "|    value_loss           | 7.92e-05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 295          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 48           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075398786 |\n",
      "|    clip_fraction        | 0.112        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.33        |\n",
      "|    explained_variance   | 0.0218       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.026       |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00932     |\n",
      "|    std                  | 0.914        |\n",
      "|    value_loss           | 0.00024      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 294         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009578479 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.0925      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0239     |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00517    |\n",
      "|    std                  | 0.899       |\n",
      "|    value_loss           | 0.000304    |\n",
      "-----------------------------------------\n",
      "Market Return :  5.54%   |   Portfolio Return : -99.97%   |   \n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 293          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 62           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068712058 |\n",
      "|    clip_fraction        | 0.141        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.31        |\n",
      "|    explained_variance   | 0.091        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0247      |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.0138      |\n",
      "|    std                  | 0.885        |\n",
      "|    value_loss           | 9.85e-05     |\n",
      "------------------------------------------\n",
      "Market Return : 39.25%   |   Portfolio Return : -85.39%   |   \n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 284         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 72          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005686409 |\n",
      "|    clip_fraction        | 0.0917      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | -0.0472     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0151     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00466    |\n",
      "|    std                  | 0.877       |\n",
      "|    value_loss           | 0.000741    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 275          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 81           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034412877 |\n",
      "|    clip_fraction        | 0.0473       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.29        |\n",
      "|    explained_variance   | -0.0201      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0129      |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -4.88e-05    |\n",
      "|    std                  | 0.88         |\n",
      "|    value_loss           | 0.0026       |\n",
      "------------------------------------------\n",
      "Market Return : 43.52%   |   Portfolio Return : -82.63%   |   \n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 273         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 89          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007979508 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 0.00607     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0281     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00531    |\n",
      "|    std                  | 0.873       |\n",
      "|    value_loss           | 0.000169    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 268          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 99           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076977112 |\n",
      "|    clip_fraction        | 0.105        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.28        |\n",
      "|    explained_variance   | -0.0584      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.033       |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00949     |\n",
      "|    std                  | 0.865        |\n",
      "|    value_loss           | 0.000143     |\n",
      "------------------------------------------\n",
      "Market Return :  9.60%   |   Portfolio Return : -83.15%   |   \n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 269          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 106          |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056029717 |\n",
      "|    clip_fraction        | 0.0875       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.27        |\n",
      "|    explained_variance   | -0.00416     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000255    |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00337     |\n",
      "|    std                  | 0.861        |\n",
      "|    value_loss           | 0.000285     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 264         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 116         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005343965 |\n",
      "|    clip_fraction        | 0.0722      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 0.202       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0196     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00609    |\n",
      "|    std                  | 0.847       |\n",
      "|    value_loss           | 0.000119    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 265          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 123          |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025941143 |\n",
      "|    clip_fraction        | 0.0399       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | -0.0207      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0268      |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.000838    |\n",
      "|    std                  | 0.834        |\n",
      "|    value_loss           | 0.00146      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 267         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 130         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016193274 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 0.00651     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0312     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00397    |\n",
      "|    std                  | 0.825       |\n",
      "|    value_loss           | 0.00142     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 268          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 137          |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042000962 |\n",
      "|    clip_fraction        | 0.0568       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.23        |\n",
      "|    explained_variance   | -0.0133      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00762     |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.000798    |\n",
      "|    std                  | 0.825        |\n",
      "|    value_loss           | 0.00121      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 269         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 144         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008166237 |\n",
      "|    clip_fraction        | 0.0771      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 0.00678     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0192     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | 0.000711    |\n",
      "|    std                  | 0.822       |\n",
      "|    value_loss           | 0.00178     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 269         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 151         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010004946 |\n",
      "|    clip_fraction        | 0.0849      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 0.0113      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00494     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00102    |\n",
      "|    std                  | 0.821       |\n",
      "|    value_loss           | 0.000621    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 268          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 159          |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045144344 |\n",
      "|    clip_fraction        | 0.05         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | 0.0131       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0156      |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | 0.00128      |\n",
      "|    std                  | 0.812        |\n",
      "|    value_loss           | 0.000692     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 269          |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 167          |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021223126 |\n",
      "|    clip_fraction        | 0.102        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | 0.0127       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0141      |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | 0.00405      |\n",
      "|    std                  | 0.816        |\n",
      "|    value_loss           | 0.000635     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 269         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 174         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005785734 |\n",
      "|    clip_fraction        | 0.0535      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.00312     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.014      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00289    |\n",
      "|    std                  | 0.814       |\n",
      "|    value_loss           | 0.00212     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 270         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 181         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005017774 |\n",
      "|    clip_fraction        | 0.0375      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | -0.00273    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.017      |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | 3.75e-05    |\n",
      "|    std                  | 0.807       |\n",
      "|    value_loss           | 0.00083     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 270          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 189          |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027854308 |\n",
      "|    clip_fraction        | 0.0697       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | -0.00179     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0169      |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.000239    |\n",
      "|    std                  | 0.809        |\n",
      "|    value_loss           | 0.000694     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 270         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 196         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009268616 |\n",
      "|    clip_fraction        | 0.0766      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.00207     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0107     |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00225    |\n",
      "|    std                  | 0.8         |\n",
      "|    value_loss           | 0.000325    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 270          |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 204          |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022376426 |\n",
      "|    clip_fraction        | 0.0853       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.19        |\n",
      "|    explained_variance   | 0.00222      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00901     |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | 0.00268      |\n",
      "|    std                  | 0.799        |\n",
      "|    value_loss           | 0.000243     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 270          |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 211          |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036703006 |\n",
      "|    clip_fraction        | 0.0912       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.19        |\n",
      "|    explained_variance   | 0.00606      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0373      |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | 0.00142      |\n",
      "|    std                  | 0.794        |\n",
      "|    value_loss           | 0.00012      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 271          |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 218          |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043158084 |\n",
      "|    clip_fraction        | 0.0595       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.18        |\n",
      "|    explained_variance   | 0.00545      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0115      |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.00415     |\n",
      "|    std                  | 0.784        |\n",
      "|    value_loss           | 0.000179     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 271         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 226         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005070829 |\n",
      "|    clip_fraction        | 0.0655      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | -0.00702    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0233     |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | 0.00141     |\n",
      "|    std                  | 0.78        |\n",
      "|    value_loss           | 0.000272    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 271         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 233         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009934235 |\n",
      "|    clip_fraction        | 0.0624      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.00749     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00683    |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | 6.03e-05    |\n",
      "|    std                  | 0.775       |\n",
      "|    value_loss           | 0.000689    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 271         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 241         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023104861 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | -0.00162    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00888     |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | 0.00954     |\n",
      "|    std                  | 0.77        |\n",
      "|    value_loss           | 0.000852    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 271          |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 248          |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033168322 |\n",
      "|    clip_fraction        | 0.0572       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.15        |\n",
      "|    explained_variance   | 0.00575      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00105     |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | 0.0011       |\n",
      "|    std                  | 0.759        |\n",
      "|    value_loss           | 0.000235     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 271         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 256         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005636762 |\n",
      "|    clip_fraction        | 0.0667      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | 0.00397     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.02       |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | 0.000843    |\n",
      "|    std                  | 0.757       |\n",
      "|    value_loss           | 0.000494    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 271         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 263         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008468777 |\n",
      "|    clip_fraction        | 0.0552      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | 0.00584     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0172     |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.000566   |\n",
      "|    std                  | 0.75        |\n",
      "|    value_loss           | 0.000712    |\n",
      "-----------------------------------------\n",
      "Market Return : 914.64%   |   Portfolio Return : -100.00%   |   \n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 270          |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 272          |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048810076 |\n",
      "|    clip_fraction        | 0.118        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.13        |\n",
      "|    explained_variance   | 0.0236       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0165      |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | 0.00275      |\n",
      "|    std                  | 0.749        |\n",
      "|    value_loss           | 0.000484     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 267         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 282         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006205583 |\n",
      "|    clip_fraction        | 0.0878      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | -2.41e-05   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0164     |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.000218   |\n",
      "|    std                  | 0.749       |\n",
      "|    value_loss           | 0.00034     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 268          |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 290          |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048726415 |\n",
      "|    clip_fraction        | 0.081        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.13        |\n",
      "|    explained_variance   | 0.00215      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0136      |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.003       |\n",
      "|    std                  | 0.743        |\n",
      "|    value_loss           | 0.000258     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 268         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 297         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007191594 |\n",
      "|    clip_fraction        | 0.0743      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.0106      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0138     |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.000834   |\n",
      "|    std                  | 0.734       |\n",
      "|    value_loss           | 0.000837    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 268         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 305         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006087666 |\n",
      "|    clip_fraction        | 0.0681      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.0245      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0257     |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.00308    |\n",
      "|    std                  | 0.731       |\n",
      "|    value_loss           | 0.000806    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 268          |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 312          |\n",
      "|    total_timesteps      | 83968        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037266845 |\n",
      "|    clip_fraction        | 0.062        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 0.0133       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.018       |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.0014      |\n",
      "|    std                  | 0.722        |\n",
      "|    value_loss           | 0.000342     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 268         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 320         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010899551 |\n",
      "|    clip_fraction        | 0.072       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.0355      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0209      |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | 0.00221     |\n",
      "|    std                  | 0.723       |\n",
      "|    value_loss           | 0.000234    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 268         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 327         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006371931 |\n",
      "|    clip_fraction        | 0.0752      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.0306      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00997     |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.000169   |\n",
      "|    std                  | 0.716       |\n",
      "|    value_loss           | 0.00026     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 268          |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 335          |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0113879405 |\n",
      "|    clip_fraction        | 0.138        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 0.0418       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0112       |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | 0.0041       |\n",
      "|    std                  | 0.709        |\n",
      "|    value_loss           | 0.00141      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 266          |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 345          |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061522797 |\n",
      "|    clip_fraction        | 0.109        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | -0.00241     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0207      |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | 0.00507      |\n",
      "|    std                  | 0.712        |\n",
      "|    value_loss           | 0.000545     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 264         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 355         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006672782 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | -0.000924   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00713     |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | 0.00479     |\n",
      "|    std                  | 0.71        |\n",
      "|    value_loss           | 0.000226    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 264         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 364         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006529591 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.0133      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.012      |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -4.03e-05   |\n",
      "|    std                  | 0.709       |\n",
      "|    value_loss           | 0.000147    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 262         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 374         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009909191 |\n",
      "|    clip_fraction        | 0.0914      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | -0.00629    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00664    |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.00034    |\n",
      "|    std                  | 0.703       |\n",
      "|    value_loss           | 0.000286    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 261         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 383         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010100732 |\n",
      "|    clip_fraction        | 0.0887      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.0166      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0227     |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.00274    |\n",
      "|    std                  | 0.702       |\n",
      "|    value_loss           | 0.000102    |\n",
      "-----------------------------------------\n",
      "Entraînement terminé.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T09:40:15.593292Z",
     "start_time": "2025-12-11T09:40:15.176784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Récupération des fonctions (Doivent être identiques à l'entraînement) ---\n",
    "# Copiez-collez ici votre fonction preprocess() améliorée de l'étape précédente\n",
    "# et votre reward_function (bien que pour le test, la reward ne serve à rien,\n",
    "# l'env en a besoin pour s'initialiser).\n",
    "\n",
    "# --- 2. Chargement de l'environnement de Test ---\n",
    "# Idéalement, pointez vers un fichier .pkl que l'agent n'a JAMAIS vu (ex: 2024.pkl)\n",
    "# Si vous n'avez pas de données séparées, utilisez le même dossier mais gardez en tête\n",
    "# que le résultat sera biaisé (overfitting).\n",
    "env_test = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"data/*.pkl\",\n",
    "    preprocess=preprocess_v2,\n",
    "    reward_function=reward_function_v2,\n",
    "    position_range=(-1, 1),\n",
    "    portfolio_initial_value=1_000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    "    # CETTE LIGNE DOIT ÊTRE SUPPRIMÉE :\n",
    "    # window_size=1\n",
    ")\n",
    "\n",
    "# On wrap l'environnement comme pour l'entraînement\n",
    "env_test = DummyVecEnv([lambda: env_test])\n",
    "\n",
    "# --- 3. Chargement de l'Agent ---\n",
    "# On charge le modèle sauvegardé\n",
    "model = RecurrentPPO.load(\"mon_agent_trading\")\n",
    "\n",
    "print(\"Modèle chargé avec succès. Début du backtest...\")"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'mon_agent_trading.zip'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[15]\u001B[39m\u001B[32m, line 36\u001B[39m\n\u001B[32m     32\u001B[39m env_test = DummyVecEnv([\u001B[38;5;28;01mlambda\u001B[39;00m: env_test])\n\u001B[32m     34\u001B[39m \u001B[38;5;66;03m# --- 3. Chargement de l'Agent ---\u001B[39;00m\n\u001B[32m     35\u001B[39m \u001B[38;5;66;03m# On charge le modèle sauvegardé\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m36\u001B[39m model = \u001B[43mRecurrentPPO\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmon_agent_trading\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     38\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mModèle chargé avec succès. Début du backtest...\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Work\\CPE\\S9\\ReinforcementLearning\\Projet\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:681\u001B[39m, in \u001B[36mBaseAlgorithm.load\u001B[39m\u001B[34m(cls, path, env, device, custom_objects, print_system_info, force_reset, **kwargs)\u001B[39m\n\u001B[32m    678\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m== CURRENT SYSTEM INFO ==\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    679\u001B[39m     get_system_info()\n\u001B[32m--> \u001B[39m\u001B[32m681\u001B[39m data, params, pytorch_variables = \u001B[43mload_from_zip_file\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    682\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    683\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    684\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcustom_objects\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcustom_objects\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    685\u001B[39m \u001B[43m    \u001B[49m\u001B[43mprint_system_info\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprint_system_info\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    686\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    688\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[33m\"\u001B[39m\u001B[33mNo data found in the saved file\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    689\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m params \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[33m\"\u001B[39m\u001B[33mNo params found in the saved file\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Work\\CPE\\S9\\ReinforcementLearning\\Projet\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:403\u001B[39m, in \u001B[36mload_from_zip_file\u001B[39m\u001B[34m(load_path, load_data, custom_objects, device, verbose, print_system_info)\u001B[39m\n\u001B[32m    376\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mload_from_zip_file\u001B[39m(\n\u001B[32m    377\u001B[39m     load_path: Union[\u001B[38;5;28mstr\u001B[39m, pathlib.Path, io.BufferedIOBase],\n\u001B[32m    378\u001B[39m     load_data: \u001B[38;5;28mbool\u001B[39m = \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    382\u001B[39m     print_system_info: \u001B[38;5;28mbool\u001B[39m = \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m    383\u001B[39m ) -> \u001B[38;5;28mtuple\u001B[39m[Optional[\u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]], TensorDict, Optional[TensorDict]]:\n\u001B[32m    384\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    385\u001B[39m \u001B[33;03m    Load model data from a .zip archive\u001B[39;00m\n\u001B[32m    386\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    401\u001B[39m \u001B[33;03m        and dict of pytorch variables\u001B[39;00m\n\u001B[32m    402\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m403\u001B[39m     file = \u001B[43mopen_path\u001B[49m\u001B[43m(\u001B[49m\u001B[43mload_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mr\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m=\u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuffix\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mzip\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    405\u001B[39m     \u001B[38;5;66;03m# set device to cpu if cuda is not available\u001B[39;00m\n\u001B[32m    406\u001B[39m     device = get_device(device=device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\functools.py:934\u001B[39m, in \u001B[36msingledispatch.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kw)\u001B[39m\n\u001B[32m    931\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m args:\n\u001B[32m    932\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfuncname\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m requires at least \u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m    933\u001B[39m                     \u001B[33m'\u001B[39m\u001B[33m1 positional argument\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m934\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdispatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__class__\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Work\\CPE\\S9\\ReinforcementLearning\\Projet\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:240\u001B[39m, in \u001B[36mopen_path_str\u001B[39m\u001B[34m(path, mode, verbose, suffix)\u001B[39m\n\u001B[32m    225\u001B[39m \u001B[38;5;129m@open_path\u001B[39m.register(\u001B[38;5;28mstr\u001B[39m)\n\u001B[32m    226\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mopen_path_str\u001B[39m(path: \u001B[38;5;28mstr\u001B[39m, mode: \u001B[38;5;28mstr\u001B[39m, verbose: \u001B[38;5;28mint\u001B[39m = \u001B[32m0\u001B[39m, suffix: Optional[\u001B[38;5;28mstr\u001B[39m] = \u001B[38;5;28;01mNone\u001B[39;00m) -> io.BufferedIOBase:\n\u001B[32m    227\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    228\u001B[39m \u001B[33;03m    Open a path given by a string. If writing to the path, the function ensures\u001B[39;00m\n\u001B[32m    229\u001B[39m \u001B[33;03m    that the path exists.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    238\u001B[39m \u001B[33;03m    :return:\u001B[39;00m\n\u001B[32m    239\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m240\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mopen_path_pathlib\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpathlib\u001B[49m\u001B[43m.\u001B[49m\u001B[43mPath\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuffix\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Work\\CPE\\S9\\ReinforcementLearning\\Projet\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:291\u001B[39m, in \u001B[36mopen_path_pathlib\u001B[39m\u001B[34m(path, mode, verbose, suffix)\u001B[39m\n\u001B[32m    285\u001B[39m         path.parent.mkdir(exist_ok=\u001B[38;5;28;01mTrue\u001B[39;00m, parents=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m    287\u001B[39m \u001B[38;5;66;03m# if opening was successful uses the open_path() function\u001B[39;00m\n\u001B[32m    288\u001B[39m \u001B[38;5;66;03m# if opening failed with IsADirectory|FileNotFound, calls open_path_pathlib\u001B[39;00m\n\u001B[32m    289\u001B[39m \u001B[38;5;66;03m#   with corrections\u001B[39;00m\n\u001B[32m    290\u001B[39m \u001B[38;5;66;03m# if reading failed with FileNotFoundError, calls open_path_pathlib with suffix\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m291\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mopen_path_pathlib\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuffix\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Work\\CPE\\S9\\ReinforcementLearning\\Projet\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:272\u001B[39m, in \u001B[36mopen_path_pathlib\u001B[39m\u001B[34m(path, mode, verbose, suffix)\u001B[39m\n\u001B[32m    270\u001B[39m             path, suffix = newpath, \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    271\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m272\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[32m    273\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    274\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Work\\CPE\\S9\\ReinforcementLearning\\Projet\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:264\u001B[39m, in \u001B[36mopen_path_pathlib\u001B[39m\u001B[34m(path, mode, verbose, suffix)\u001B[39m\n\u001B[32m    262\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m mode == \u001B[33m\"\u001B[39m\u001B[33mr\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    263\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m264\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m open_path(\u001B[43mpath\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrb\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m, mode, verbose, suffix)\n\u001B[32m    265\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[32m    266\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m suffix \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m suffix != \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pathlib\\_local.py:537\u001B[39m, in \u001B[36mPath.open\u001B[39m\u001B[34m(self, mode, buffering, encoding, errors, newline)\u001B[39m\n\u001B[32m    535\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[32m    536\u001B[39m     encoding = io.text_encoding(encoding)\n\u001B[32m--> \u001B[39m\u001B[32m537\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffering\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnewline\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: 'mon_agent_trading.zip'"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Exécution du Backtest (La boucle de prédiction)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Reset de l'environnement\n",
    "obs = env_test.reset()\n",
    "\n",
    "# Pour les LSTM, nous devons gérer les états cachés (cell state / hidden state)\n",
    "# Initialement, ils sont vides (None) ou zéros\n",
    "_states = None\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    # L'agent prédit l'action à prendre\n",
    "    # deterministic=True est CRUCIAL pour le test : on supprime l'aléatoire de l'exploration\n",
    "    action, _states = model.predict(obs, state=_states, deterministic=True)\n",
    "\n",
    "    # On joue l'action dans l'environnement\n",
    "    obs, reward, done, info = env_test.step(action)\n",
    "\n",
    "    # Note : Dans SB3 avec VecEnv, 'done' est un tableau de booléens.\n",
    "    # L'environnement se reset automatiquement quand done=True.\n",
    "    # Pour un backtest unique, on veut sortir de la boucle quand l'épisode finit.\n",
    "    if done[0]:\n",
    "        break\n",
    "\n",
    "print(\"Backtest terminé.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Analyse des Résultats et Visualisation"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# On récupère l'environnement \"nu\" (sans le wrapper VecEnv de SB3)\n",
    "raw_env = env_test.envs[0].unwrapped\n",
    "\n",
    "# Récupération de l'historique complet sous forme de DataFrame\n",
    "df_results = raw_env.save_for_render(dir=\"render_logs\")\n",
    "# Cela crée aussi un rendu HTML interactif dans le dossier 'render_logs' !\n",
    "\n",
    "# --- Affichage des Métriques Clés ---\n",
    "metrics = raw_env.get_metrics()\n",
    "print(\"\\n--- RÉSULTATS DU BACKTEST ---\")\n",
    "print(f\"Rendement du Marché (Buy & Hold) : {metrics['Market Return']}\")\n",
    "print(f\"Rendement de l'Agent             : {metrics['Portfolio Return']}\")\n",
    "print(f\"Valeur Finale du Portefeuille    : {metrics['Portfolio Valuation']:.2f} $\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- Comparaison Graphique ---\n",
    "# On va tracer la valeur du portefeuille vs le prix de l'actif (normalisé)\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# 1. Courbe de l'Agent (Valeur du portefeuille)\n",
    "plt.plot(df_results.index, df_results['portfolio_valuation'], label='Agent AI', color='blue', linewidth=2)\n",
    "\n",
    "# 2. Courbe du \"Buy & Hold\" (Si on avait juste acheté au début)\n",
    "# On normalise le prix de l'actif pour qu'il commence à 1000$ (comme le portefeuille)\n",
    "initial_price = df_results['data_close'].iloc[0]\n",
    "initial_portfolio = df_results['portfolio_valuation'].iloc[0]\n",
    "factor = initial_portfolio / initial_price\n",
    "plt.plot(df_results.index, df_results['data_close'] * factor, label='Buy & Hold (Marché)', color='gray', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.title(\"Performance : Intelligence Artificielle vs Marché\")\n",
    "plt.xlabel(\"Temps\")\n",
    "plt.ylabel(\"Valeur du Portefeuille ($)\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# --- Visualisation des Positions ---\n",
    "# Pour voir QUAND l'agent achète ou vend\n",
    "plt.figure(figsize=(15, 4))\n",
    "plt.plot(df_results.index, df_results['position'], label='Position de l\\'Agent', color='orange')\n",
    "plt.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "plt.axhline(y=1, color='green', linestyle=':', alpha=0.5, label='100% Long')\n",
    "plt.axhline(y=-1, color='red', linestyle=':', alpha=0.5, label='100% Short')\n",
    "plt.title(\"Décisions de l'Agent au cours du temps\")\n",
    "plt.ylabel(\"Position (-1 = Short, 1 = Long)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
