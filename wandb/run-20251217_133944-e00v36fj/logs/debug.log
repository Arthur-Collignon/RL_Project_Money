2025-12-17 13:39:44,464 INFO    MainThread:23428 [wandb_setup.py:_flush():80] Current SDK version is 0.23.1
2025-12-17 13:39:44,464 INFO    MainThread:23428 [wandb_setup.py:_flush():80] Configure stats pid to 23428
2025-12-17 13:39:44,464 INFO    MainThread:23428 [wandb_setup.py:_flush():80] Loading settings from C:\Users\arthu\.config\wandb\settings
2025-12-17 13:39:44,464 INFO    MainThread:23428 [wandb_setup.py:_flush():80] Loading settings from C:\Users\arthu\Work\CPE\S9\ReinforcementLearning\Projet\wandb\settings
2025-12-17 13:39:44,464 INFO    MainThread:23428 [wandb_setup.py:_flush():80] Loading settings from environment variables
2025-12-17 13:39:44,464 INFO    MainThread:23428 [wandb_init.py:setup_run_log_directory():714] Logging user logs to C:\Users\arthu\Work\CPE\S9\ReinforcementLearning\Projet\wandb\run-20251217_133944-e00v36fj\logs\debug.log
2025-12-17 13:39:44,467 INFO    MainThread:23428 [wandb_init.py:setup_run_log_directory():715] Logging internal logs to C:\Users\arthu\Work\CPE\S9\ReinforcementLearning\Projet\wandb\run-20251217_133944-e00v36fj\logs\debug-internal.log
2025-12-17 13:39:44,468 INFO    MainThread:23428 [wandb_init.py:monkeypatch_ipython():633] configuring jupyter hooks <wandb.sdk.wandb_init._WandbInit object at 0x0000026A463E9FD0>
2025-12-17 13:39:44,492 INFO    MainThread:23428 [wandb_init.py:init():841] calling init triggers
2025-12-17 13:39:44,492 INFO    MainThread:23428 [wandb_init.py:init():846] wandb.init called with sweep_config: {}
config: {'policy_type': 'MlpLstmPolicy', 'total_timesteps': 100000, 'env_id': 'MultiDatasetTradingEnv', 'learning_rate': 0.0003, 'n_steps': 2048, 'batch_size': 128, 'ent_coef': 0.01, 'portfolio_initial_value': 1000, 'trading_fees': 0.001, 'borrow_interest_rate': 8.333333333333334e-06, 'positions_range': (-1, 1), 'model_name': 'mon_agent_trading', '_wandb': {'code_path': 'code/projet.ipynb'}}
2025-12-17 13:39:44,492 INFO    MainThread:23428 [wandb_init.py:init():889] starting backend
2025-12-17 13:39:47,319 INFO    MainThread:23428 [wandb_init.py:init():892] sending inform_init request
2025-12-17 13:39:47,364 INFO    MainThread:23428 [wandb_init.py:init():900] backend started and connected
2025-12-17 13:39:47,367 INFO    MainThread:23428 [wandb_run.py:_label_probe_notebook():1334] probe notebook
2025-12-17 13:39:47,368 INFO    MainThread:23428 [wandb_run.py:_label_probe_notebook():1344] Unable to probe notebook: 'charmap' codec can't decode byte 0x8f in position 25155: character maps to <undefined>
2025-12-17 13:39:47,368 INFO    MainThread:23428 [wandb_init.py:init():970] updated telemetry
2025-12-17 13:39:47,872 INFO    MainThread:23428 [wandb_init.py:init():994] communicating run to backend with 90.0 second timeout
2025-12-17 13:39:48,208 INFO    MainThread:23428 [wandb_init.py:init():1041] starting run threads in backend
2025-12-17 13:39:48,544 INFO    MainThread:23428 [wandb_run.py:_console_start():2521] atexit reg
2025-12-17 13:39:48,544 INFO    MainThread:23428 [wandb_run.py:_redirect():2369] redirect: wrap_raw
2025-12-17 13:39:48,544 INFO    MainThread:23428 [wandb_run.py:_redirect():2438] Wrapping output streams.
2025-12-17 13:39:48,545 INFO    MainThread:23428 [wandb_run.py:_redirect():2461] Redirects installed.
2025-12-17 13:39:48,573 INFO    MainThread:23428 [wandb_init.py:init():1081] run started, returning control to user process
2025-12-17 13:39:53,431 INFO    MainThread:23428 [wandb_run.py:_config_callback():1396] config_cb None None {'algo': 'RecurrentPPO', 'policy_class': "<class 'sb3_contrib.common.recurrent.policies.RecurrentActorCriticPolicy'>", 'device': 'cpu', 'verbose': 0, 'policy_kwargs': '{}', 'num_timesteps': 0, '_total_timesteps': 100000, '_num_timesteps_at_start': 0, 'seed': 'None', 'action_noise': 'None', 'start_time': 1765975193371797200, 'tensorboard_log': 'None', '_last_obs': '[[ 0.2973548  -0.7190689   0.21888182  0.1353754  99.663826    0.37366688\n   0.37366688]]', '_last_episode_starts': '[ True]', '_last_original_obs': 'None', '_episode_num': 0, 'use_sde': 'False', 'sde_sample_freq': -1, '_current_progress_remaining': 1.0, '_stats_window_size': 100, 'ep_info_buffer': 'deque([], maxlen=100)', 'ep_success_buffer': 'deque([], maxlen=100)', '_n_updates': 0, '_custom_logger': 'False', 'env': '<stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x0000026A47A1D2B0>', '_vec_normalize_env': 'None', 'observation_space': 'Box(-inf, inf, (7,), float32)', 'action_space': 'Box(-1.0, 1.0, (1,), float32)', 'n_envs': 1, 'gamma': 0.99, 'gae_lambda': 0.95, 'vf_coef': 0.5, 'max_grad_norm': 0.5, 'rollout_buffer_class': 'None', 'rollout_buffer_kwargs': '{}', 'n_epochs': 10, 'clip_range': 'FloatSchedule(ConstantSchedule(val=0.2))', 'clip_range_vf': 'None', 'normalize_advantage': 'True', 'target_kl': 'None', '_last_lstm_states': 'RNNStates(pi=(tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0.]]]), tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0.]]])), vf=(tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0.]]]), tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0.]]])))', 'lr_schedule': 'FloatSchedule(ConstantSchedule(val=0.0003))', 'policy': 'RecurrentActorCriticPolicy(\n  (features_extractor): FlattenExtractor(\n    (flatten): Flatten(start_dim=1, end_dim=-1)\n  )\n  (pi_features_extractor): FlattenExtractor(\n    (flatten): Flatten(start_dim=1, end_dim=-1)\n  )\n  (vf_features_extractor): FlattenExtractor(\n    (flatten): Flatten(start_dim=1, end_dim=-1)\n  )\n  (mlp_extractor): MlpExtractor(\n    (policy_net): Sequential(\n      (0): Linear(in_features=256, out_features=64, bias=True)\n      (1): Tanh()\n      (2): Linear(in_features=64, out_features=64, bias=True)\n      (3): Tanh()\n    )\n    (value_net): Sequential(\n      (0): Linear(in_features=256, out_features=64, bias=True)\n      (1): Tanh()\n      (2): Linear(in_features=64, out_features=64, bias=True)\n      (3): Tanh()\n    )\n  )\n  (action_net): Linear(in_features=64, out_features=1, bias=True)\n  (value_net): Linear(in_features=64, out_features=1, bias=True)\n  (lstm_actor): LSTM(7, 256)\n  (lstm_critic): LSTM(7, 256)\n)', 'rollout_buffer': '<sb3_contrib.common.recurrent.buffers.RecurrentRolloutBuffer object at 0x0000026A3C459550>', '_logger': '<stable_baselines3.common.logger.Logger object at 0x0000026A50CC2A50>'}
2025-12-17 13:47:37,197 INFO    MainThread:23428 [wandb_run.py:_finish():2287] finishing run arthur-collignon-cpe-lyon/RL-Trading-Project/e00v36fj
2025-12-17 13:47:37,275 INFO    MainThread:23428 [jupyter.py:save_history():464] saving 13 cells to _session_history.ipynb
2025-12-17 13:47:37,276 INFO    MainThread:23428 [wandb_run.py:_config_callback():1396] config_cb ('_wandb', 'session_history') code\_session_history.ipynb None
2025-12-17 13:47:37,289 INFO    MainThread:23428 [jupyter.py:_save_ipynb():371] looking for notebook: projet.ipynb
2025-12-17 13:47:40,040 INFO    MainThread:23428 [wandb_run.py:_config_callback():1396] config_cb None None {'_wandb': {'code_path': 'source-RL-Trading-Project-projet.ipynb'}}
2025-12-17 13:47:40,040 INFO    MainThread:23428 [wandb_init.py:_jupyter_teardown():617] saved code and history: <Artifact source-RL-Trading-Project-projet.ipynb>
2025-12-17 13:47:40,040 INFO    MainThread:23428 [wandb_init.py:_jupyter_teardown():618] cleaning up jupyter logic
2025-12-17 13:47:40,040 INFO    MainThread:23428 [wandb_run.py:_atexit_cleanup():2486] got exitcode: 0
2025-12-17 13:47:40,041 INFO    MainThread:23428 [wandb_run.py:_restore():2468] restore
2025-12-17 13:47:40,041 INFO    MainThread:23428 [wandb_run.py:_restore():2474] restore done
2025-12-17 13:47:43,819 INFO    MainThread:23428 [wandb_run.py:_footer_sync_info():3862] logging synced files
