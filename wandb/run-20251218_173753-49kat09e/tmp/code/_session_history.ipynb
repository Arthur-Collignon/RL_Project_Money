{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9151c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "from gym_trading_env.wrapper import DiscreteActionsWrapper\n",
    "from stable_baselines3 import PPO\n",
    "from gym_trading_env.renderer import Renderer\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Prétraitement (Inchangé) ---\n",
    "def preprocess(df):\n",
    "    df = df.sort_index().dropna().drop_duplicates()\n",
    "    df['feature_close'] = df['close'].pct_change()\n",
    "    df['feature_rsi'] = calculate_rsi(df['close']) / 100\n",
    "    df['feature_macd'] = calculate_macd(df['close'])\n",
    "    return df.dropna()\n",
    "\n",
    "# --- 2. Reward Function avec \"Malus de Short\" ---\n",
    "def reward_function(history):\n",
    "    current_val = history['portfolio_valuation', -1]\n",
    "    prev_val = history['portfolio_valuation', -2]\n",
    "    reward = np.log(current_val / prev_val)\n",
    "\n",
    "    # --- CORRECTION DU PROBLÈME DE SHORT ---\n",
    "    current_pos = history['position', -1]\n",
    "\n",
    "    # Si l'agent est en position négative (Short), on le punit légèrement\n",
    "    # à chaque step. Cela l'oblige à ne short que s'il est VRAIMENT sûr de gagner gros.\n",
    "    if current_pos < 0:\n",
    "        reward -= 0.0002  # Petite \"taxe\" sur le pessimisme\n",
    "\n",
    "    return reward\n",
    "\n",
    "# --- 3. Création de l'environnement ---\n",
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"./data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    portfolio_initial_value=1000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    "    reward_function=reward_function,\n",
    ")\n",
    "\n",
    "# --- 4. LA SOLUTION AUX DEUX PROBLÈMES ---\n",
    "# On garde le Wrapper (donc l'agent sort des INT), mais on définit\n",
    "# une liste précise qui favorise l'achat (plus de choix positifs).\n",
    "custom_positions = [\n",
    "    -0.5,  # Le seul choix de Short (modéré)\n",
    "    0,     # Cash (Neutre)\n",
    "    0.25,  # Petit investissement\n",
    "    0.5,   # Investissement moyen\n",
    "    0.75,  # Gros investissement\n",
    "    1.0,   # All-in\n",
    "    1.25,  # Petit Levier\n",
    "    1.5    # Gros Levier\n",
    "]\n",
    "\n",
    "env = DiscreteActionsWrapper(env, positions=custom_positions)\n",
    "\n",
    "# --- 5. Agent PPO ---\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=1,\n",
    "    learning_rate=0.0003,\n",
    "    # ent_coef force l'exploration. Si ton agent fait toujours la même chose,\n",
    "    # augmente cette valeur (ex: 0.05)\n",
    "    ent_coef=0.02,\n",
    "    tensorboard_log=\"./ppo_hybrid_tensorboard/\"\n",
    ")\n",
    "\n",
    "print(\"Entraînement avec actions hybrides...\")\n",
    "model.learn(total_timesteps=150_000)\n",
    "model.save(\"ppo_hybrid_solution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c139604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "import gym_trading_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f43b63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       open    high     low   close       volume  \\\n",
      "date_open                                                          \n",
      "2020-08-18 07:00:00  430.00  435.00  410.00  430.30   487.154463   \n",
      "2020-08-18 08:00:00  430.27  431.79  430.27  430.80   454.176153   \n",
      "2020-08-18 09:00:00  430.86  431.13  428.71  429.35  1183.710884   \n",
      "2020-08-18 10:00:00  429.75  432.69  428.59  431.90  1686.183227   \n",
      "2020-08-18 11:00:00  432.09  432.89  426.99  427.45  1980.692724   \n",
      "\n",
      "                             date_close  \n",
      "date_open                                \n",
      "2020-08-18 07:00:00 2020-08-18 08:00:00  \n",
      "2020-08-18 08:00:00 2020-08-18 09:00:00  \n",
      "2020-08-18 09:00:00 2020-08-18 10:00:00  \n",
      "2020-08-18 10:00:00 2020-08-18 11:00:00  \n",
      "2020-08-18 11:00:00 2020-08-18 12:00:00  "
     ]
    }
   ],
   "source": [
    "def preprocess(df):\n",
    "    df = df.sort_index()\n",
    "    df = df.dropna()\n",
    "    df = df.drop_duplicates()\n",
    "    return df\n",
    "\n",
    "df = preprocess(pd.read_pickle('./data/binance-ETHUSD-1h.pkl'))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76c4e5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       open    high     low   close       volume  \\\n",
      "date_open                                                          \n",
      "2020-08-18 07:00:00  430.00  435.00  410.00  430.30   487.154463   \n",
      "2020-08-18 08:00:00  430.27  431.79  430.27  430.80   454.176153   \n",
      "2020-08-18 09:00:00  430.86  431.13  428.71  429.35  1183.710884   \n",
      "2020-08-18 10:00:00  429.75  432.69  428.59  431.90  1686.183227   \n",
      "2020-08-18 11:00:00  432.09  432.89  426.99  427.45  1980.692724   \n",
      "\n",
      "                             date_close  feature_close  \n",
      "date_open                                               \n",
      "2020-08-18 07:00:00 2020-08-18 08:00:00      -1.891634  \n",
      "2020-08-18 08:00:00 2020-08-18 09:00:00      -1.891128  \n",
      "2020-08-18 09:00:00 2020-08-18 10:00:00      -1.892594  \n",
      "2020-08-18 10:00:00 2020-08-18 11:00:00      -1.890016  \n",
      "2020-08-18 11:00:00 2020-08-18 12:00:00      -1.894514  "
     ]
    }
   ],
   "source": [
    "def preprocess(df):\n",
    "    df = df.sort_index()\n",
    "    df = df.dropna()\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    df['feature_close'] = (df['close'] - df['close'].mean()) / df['close'].std()\n",
    "\n",
    "    return df\n",
    "\n",
    "df = preprocess(pd.read_pickle('./data/binance-ETHUSD-1h.pkl'))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dea540fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    portfolio_initial_value=1_000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    ")\n",
    "\n",
    "obs, _ = env.reset()\n",
    "# On veut une position de 88% ETH / 12% USD\n",
    "obs, reward, terminated, truncated, info = env.step(0.88)\n",
    "print(obs)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb2a62d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    position_range=(0, 1),  # ICI : (borne min, borne max)\n",
    "    portfolio_initial_value=1_000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c376773",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_trading_env.wrapper import DiscreteActionsWrapper\n",
    "\n",
    "# Vous pouvez aussi appeler le wrapper `env` pour faire plus simple\n",
    "# Ici, je fais explicitement la distinction entre `wrapper` et `env`\n",
    "wrapper = DiscreteActionsWrapper(env, positions=[-1, 0, 0.25, 0.5, 0.75, 1, 2])\n",
    "obs, _ = wrapper.reset()\n",
    "# On veut une position de 25% ETH / 75% USD ; cela correspond à la position\n",
    "# d'index 2 dans la liste ci-dessus\n",
    "obs, reward, terminated, truncated, info = wrapper.step(2)\n",
    "print(obs)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fccaf84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(history):\n",
    "    return history['portfolio_valuation', -1]\n",
    "\n",
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    portfolio_initial_value=1_000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    "    # On spécifie la fonction de récompense\n",
    "    reward_function=reward_function,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f58bc98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_episodes = 2\n",
    "for episode in range(1, nb_episodes + 1):\n",
    "    obs, _ = env.reset()\n",
    "    print(f'Episode n˚{episode} -- Jeu de donnée {env.name}')\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "    if terminated:\n",
    "        print('Argent perdu')\n",
    "    elif truncated:\n",
    "        print('Épisode terminé')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b82c25e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_portfolio_valuation(history):\n",
    "    return round(history['portfolio_valuation', -1], 2)\n",
    "\n",
    "env.add_metric('Portfolio Valuation', metric_portfolio_valuation)\n",
    "\n",
    "done = False\n",
    "obs, _ = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "255bcb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_valuation = env.historical_info['portfolio_valuation', -1]\n",
    "# Si on avait WandB :\n",
    "# run.summary['portfolio_valuation'] = portfolio_valuation\n",
    "# On simule ça par un simple print...\n",
    "print(portfolio_valuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48f4fb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = env.get_metrics()\n",
    "print(metrics)\n",
    "portfolio_valuation = metrics['Portfolio Valuation']\n",
    "print(portfolio_valuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5587de9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "from gym_trading_env.wrapper import DiscreteActionsWrapper\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "248b6570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rsi(series, window=14):\n",
    "    delta = series.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def calculate_macd(series, slow=26, fast=12, signal=9):\n",
    "    exp1 = series.ewm(span=fast, adjust=False).mean()\n",
    "    exp2 = series.ewm(span=slow, adjust=False).mean()\n",
    "    macd = exp1 - exp2\n",
    "    return macd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5df2262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    # Tri et nettoyage\n",
    "    df = df.sort_index().dropna().drop_duplicates()\n",
    "\n",
    "    # Ajout de features (doivent commencer par \"feature_\")\n",
    "    # 1. RSI normalisé entre 0 et 1\n",
    "    df['feature_RSI'] = calculate_rsi(df['close']) / 100\n",
    "\n",
    "    # 2. MACD\n",
    "    df['feature_MACD'] = calculate_macd(df['close'])\n",
    "\n",
    "    # 3. Rendements logarithmiques (plus stable pour le RL que le prix brut)\n",
    "    df['feature_log_return'] = np.log(df['close'] / df['close'].shift(1))\n",
    "\n",
    "    # 4. Position du prix par rapport à la moyenne mobile\n",
    "    df['feature_sma_dist'] = (df['close'] - df['close'].rolling(20).mean()) / df['close'].rolling(20).std()\n",
    "\n",
    "    return df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f11a65ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(history):\n",
    "    # Récompense basée sur la variation logarithmique de la valeur du portefeuille\n",
    "    # Cela encourage une croissance stable plutôt que des paris risqués\n",
    "    if len(history[\"portfolio_valuation\"]) < 2:\n",
    "        return 0\n",
    "    return np.log(history['portfolio_valuation', -1] / history['portfolio_valuation', -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e0fb8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de l'environnement avec les contraintes du projet\n",
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"./data/*.pkl\", # Assure-toi que le dossier data contient tes fichiers .pkl\n",
    "    preprocess=preprocess,\n",
    "    portfolio_initial_value=1000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    "    reward_function=reward_function,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36b884d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Démarrage de la simulation sur le dataset : {env.unwrapped.name}\")\n",
    "\n",
    "obs, info = env.reset()\n",
    "done = False\n",
    "truncated = False\n",
    "\n",
    "while not (done or truncated):\n",
    "    # Ici, tu remplaceras par : action, _states = model.predict(obs) si tu utilises Stable Baselines\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, truncated, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8552b39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Démarrage de la simulation sur le dataset : {env.unwrapped.name}\")\n",
    "\n",
    "obs, info = env.reset()\n",
    "done = False\n",
    "truncated = False\n",
    "\n",
    "while not (done or truncated):\n",
    "    # Ici, tu remplaceras par : action, _states = model.predict(obs) si tu utilises Stable Baselines\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, truncated, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b0bfac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "from gym_trading_env.wrapper import DiscreteActionsWrapper\n",
    "from stable_baselines3 import PPO\n",
    "from gym_trading_env.renderer import Renderer\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Prétraitement (Inchangé) ---\n",
    "def preprocess(df):\n",
    "    df = df.sort_index().dropna().drop_duplicates()\n",
    "    df['feature_close'] = df['close'].pct_change()\n",
    "    df['feature_rsi'] = calculate_rsi(df['close']) / 100\n",
    "    df['feature_macd'] = calculate_macd(df['close'])\n",
    "    return df.dropna()\n",
    "\n",
    "# --- 2. Reward Function avec \"Malus de Short\" ---\n",
    "def reward_function(history):\n",
    "    current_val = history['portfolio_valuation', -1]\n",
    "    prev_val = history['portfolio_valuation', -2]\n",
    "    reward = np.log(current_val / prev_val)\n",
    "\n",
    "    # --- CORRECTION DU PROBLÈME DE SHORT ---\n",
    "    current_pos = history['position', -1]\n",
    "\n",
    "    # Si l'agent est en position négative (Short), on le punit légèrement\n",
    "    # à chaque step. Cela l'oblige à ne short que s'il est VRAIMENT sûr de gagner gros.\n",
    "    if current_pos < 0:\n",
    "        reward -= 0.0002  # Petite \"taxe\" sur le pessimisme\n",
    "\n",
    "    return reward\n",
    "\n",
    "# --- 3. Création de l'environnement ---\n",
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"./data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    portfolio_initial_value=1000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    "    reward_function=reward_function,\n",
    ")\n",
    "\n",
    "# --- 4. LA SOLUTION AUX DEUX PROBLÈMES ---\n",
    "# On garde le Wrapper (donc l'agent sort des INT), mais on définit\n",
    "# une liste précise qui favorise l'achat (plus de choix positifs).\n",
    "custom_positions = [\n",
    "    -0.5,  # Le seul choix de Short (modéré)\n",
    "    0,     # Cash (Neutre)\n",
    "    0.25,  # Petit investissement\n",
    "    0.5,   # Investissement moyen\n",
    "    0.75,  # Gros investissement\n",
    "    1.0,   # All-in\n",
    "    1.25,  # Petit Levier\n",
    "    1.5    # Gros Levier\n",
    "]\n",
    "\n",
    "env = DiscreteActionsWrapper(env, positions=custom_positions)\n",
    "\n",
    "# --- 5. Agent PPO ---\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=1,\n",
    "    learning_rate=0.0003,\n",
    "    # ent_coef force l'exploration. Si ton agent fait toujours la même chose,\n",
    "    # augmente cette valeur (ex: 0.05)\n",
    "    ent_coef=0.02,\n",
    "    tensorboard_log=\"./ppo_hybrid_tensorboard/\"\n",
    ")\n",
    "\n",
    "print(\"Entraînement avec actions hybrides...\")\n",
    "model.learn(total_timesteps=150_000)\n",
    "model.save(\"ppo_hybrid_solution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8245f2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Simulation ---\n",
    "print(\"Lancement de la simulation...\")\n",
    "obs, info = env.reset()\n",
    "done, truncated = False, False\n",
    "\n",
    "while not (done or truncated):\n",
    "    action, _ = model.predict(obs)\n",
    "\n",
    "    # Ici, 'action' est un INT (l'index dans la liste custom_positions)\n",
    "    # On le convertit en int Python pur pour éviter le bug numpy\n",
    "    action = int(action)\n",
    "\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "# Sauvegarde\n",
    "env.unwrapped.save_for_render(dir=\"render_logs\")\n",
    "renderer = Renderer(render_logs_dir=\"render_logs\")\n",
    "renderer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "189d8b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "from gym_trading_env.wrapper import DiscreteActionsWrapper\n",
    "from stable_baselines3 import PPO\n",
    "from gym_trading_env.renderer import Renderer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "# --- 1. CONFIGURATION ET INDICATEURS ---\n",
    "\n",
    "# On définit les hyperparamètres ici pour que WandB puisse les enregistrer\n",
    "config = {\n",
    "    \"policy_type\": \"MlpPolicy\",\n",
    "    \"total_timesteps\": 200_000,\n",
    "    \"learning_rate\": 0.0003,\n",
    "    \"ent_coef\": 0.02, # Coefficient d'exploration\n",
    "    \"batch_size\": 128,\n",
    "    \"positions\": [-0.5, 0, 0.25, 0.5, 0.75, 1.0, 1.25, 1.5], # Hybride\n",
    "    \"project_name\": \"RL-Trading-Project\"\n",
    "}\n",
    "\n",
    "def calculate_rsi(series, window=14):\n",
    "    delta = series.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def calculate_macd(series, slow=26, fast=12, signal=9):\n",
    "    exp1 = series.ewm(span=fast, adjust=False).mean()\n",
    "    exp2 = series.ewm(span=slow, adjust=False).mean()\n",
    "    macd = exp1 - exp2\n",
    "    return macd\n",
    "\n",
    "def preprocess(df):\n",
    "    df = df.sort_index().dropna().drop_duplicates()\n",
    "    df['feature_close'] = df['close'].pct_change()\n",
    "    df['feature_rsi'] = calculate_rsi(df['close']) / 100\n",
    "    df['feature_macd'] = calculate_macd(df['close'])\n",
    "    return df.dropna()\n",
    "\n",
    "def reward_function(history):\n",
    "    current_val = history['portfolio_valuation', -1]\n",
    "    prev_val = history['portfolio_valuation', -2]\n",
    "    reward = np.log(current_val / prev_val)\n",
    "    \n",
    "    # Malus pour les positions Short (pour éviter le biais négatif)\n",
    "    if history['position', -1] < 0:\n",
    "        reward -= 0.0002\n",
    "    \n",
    "    return reward\n",
    "\n",
    "# --- 2. INITIALISATION DE WANDB ---\n",
    "run = wandb.init(\n",
    "    project=config[\"project_name\"],\n",
    "    config=config,\n",
    "    sync_tensorboard=True, # Synchronise automatiquement les logs SB3\n",
    "    monitor_gym=True,      # Essaie d'enregistrer les vidéos (si disponible)\n",
    "    save_code=True,        # Sauvegarde ce script dans WandB\n",
    ")\n",
    "\n",
    "# --- 3. CRÉATION DE L'ENVIRONNEMENT ---\n",
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"./data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    portfolio_initial_value=1000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    "    reward_function=reward_function,\n",
    ")\n",
    "\n",
    "# Wrapper Hybride (Int -> Float spécifique)\n",
    "env = DiscreteActionsWrapper(env, positions=config[\"positions\"])\n",
    "\n",
    "# --- 4. ENTRAÎNEMENT AVEC CALLBACK WANDB ---\n",
    "model = PPO(\n",
    "    config[\"policy_type\"],\n",
    "    env,\n",
    "    verbose=1,\n",
    "    learning_rate=config[\"learning_rate\"],\n",
    "    ent_coef=config[\"ent_coef\"],\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    tensorboard_log=f\"runs/{run.id}\" # Dossier unique pour Tensorboard\n",
    ")\n",
    "\n",
    "print(f\"Lancement de l'entraînement WandB : {run.name}\")\n",
    "model.learn(\n",
    "    total_timesteps=config[\"total_timesteps\"],\n",
    "    callback=WandbCallback(\n",
    "        gradient_save_freq=100,\n",
    "        model_save_path=f\"models/{run.id}\",\n",
    "        verbose=2,\n",
    "    )\n",
    ")\n",
    "model.save(\"ppo_trading_wandb_final\")\n",
    "\n",
    "# --- 5. ÉVALUATION ET LOGGING FINAL ---\n",
    "print(\"Évaluation finale...\")\n",
    "obs, info = env.reset()\n",
    "done, truncated = False, False\n",
    "\n",
    "while not (done or truncated):\n",
    "    action, _ = model.predict(obs)\n",
    "    action = int(action) # Conversion array -> int pour le wrapper\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "# Récupération des métriques finales de l'environnement\n",
    "final_metrics = env.unwrapped.get_metrics()\n",
    "print(\"Métriques finales :\", final_metrics)\n",
    "\n",
    "# Envoi des métriques clés à WandB (pour le tableau de bord)\n",
    "wandb.log({\n",
    "    \"final_portfolio_valuation\": info['portfolio_valuation'],\n",
    "    \"market_return\": final_metrics.get(\"Market Return\", 0),\n",
    "    \"portfolio_return\": final_metrics.get(\"Portfolio Return\", 0)\n",
    "})\n",
    "\n",
    "# --- 6. VISUALISATION ---\n",
    "env.unwrapped.save_for_render(dir=\"render_logs\")\n",
    "\n",
    "# On ferme le run WandB proprement\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
