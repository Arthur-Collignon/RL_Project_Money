{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99c08c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "import gym_trading_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9523411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       open    high     low   close       volume  \\\n",
      "date_open                                                          \n",
      "2020-08-18 07:00:00  430.00  435.00  410.00  430.30   487.154463   \n",
      "2020-08-18 08:00:00  430.27  431.79  430.27  430.80   454.176153   \n",
      "2020-08-18 09:00:00  430.86  431.13  428.71  429.35  1183.710884   \n",
      "2020-08-18 10:00:00  429.75  432.69  428.59  431.90  1686.183227   \n",
      "2020-08-18 11:00:00  432.09  432.89  426.99  427.45  1980.692724   \n",
      "\n",
      "                             date_close  \n",
      "date_open                                \n",
      "2020-08-18 07:00:00 2020-08-18 08:00:00  \n",
      "2020-08-18 08:00:00 2020-08-18 09:00:00  \n",
      "2020-08-18 09:00:00 2020-08-18 10:00:00  \n",
      "2020-08-18 10:00:00 2020-08-18 11:00:00  \n",
      "2020-08-18 11:00:00 2020-08-18 12:00:00  "
     ]
    }
   ],
   "source": [
    "def preprocess(df):\n",
    "    df = df.sort_index()\n",
    "    df = df.dropna()\n",
    "    df = df.drop_duplicates()\n",
    "    return df\n",
    "\n",
    "df = preprocess(pd.read_pickle('./data/binance-ETHUSD-1h.pkl'))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f647ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       open    high     low   close       volume  \\\n",
      "date_open                                                          \n",
      "2020-08-18 07:00:00  430.00  435.00  410.00  430.30   487.154463   \n",
      "2020-08-18 08:00:00  430.27  431.79  430.27  430.80   454.176153   \n",
      "2020-08-18 09:00:00  430.86  431.13  428.71  429.35  1183.710884   \n",
      "2020-08-18 10:00:00  429.75  432.69  428.59  431.90  1686.183227   \n",
      "2020-08-18 11:00:00  432.09  432.89  426.99  427.45  1980.692724   \n",
      "\n",
      "                             date_close  feature_close  \n",
      "date_open                                               \n",
      "2020-08-18 07:00:00 2020-08-18 08:00:00      -1.891634  \n",
      "2020-08-18 08:00:00 2020-08-18 09:00:00      -1.891128  \n",
      "2020-08-18 09:00:00 2020-08-18 10:00:00      -1.892594  \n",
      "2020-08-18 10:00:00 2020-08-18 11:00:00      -1.890016  \n",
      "2020-08-18 11:00:00 2020-08-18 12:00:00      -1.894514  "
     ]
    }
   ],
   "source": [
    "def preprocess(df):\n",
    "    df = df.sort_index()\n",
    "    df = df.dropna()\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    df['feature_close'] = (df['close'] - df['close'].mean()) / df['close'].std()\n",
    "\n",
    "    return df\n",
    "\n",
    "df = preprocess(pd.read_pickle('./data/binance-ETHUSD-1h.pkl'))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "695c6704",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    portfolio_initial_value=1_000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    ")\n",
    "\n",
    "obs, _ = env.reset()\n",
    "# On veut une position de 88% ETH / 12% USD\n",
    "obs, reward, terminated, truncated, info = env.step(0.88)\n",
    "print(obs)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcde726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    position_range=(0, 1),  # ICI : (borne min, borne max)\n",
    "    portfolio_initial_value=1_000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed29e1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_trading_env.wrapper import DiscreteActionsWrapper\n",
    "\n",
    "# Vous pouvez aussi appeler le wrapper `env` pour faire plus simple\n",
    "# Ici, je fais explicitement la distinction entre `wrapper` et `env`\n",
    "wrapper = DiscreteActionsWrapper(env, positions=[-1, 0, 0.25, 0.5, 0.75, 1, 2])\n",
    "obs, _ = wrapper.reset()\n",
    "# On veut une position de 25% ETH / 75% USD ; cela correspond à la position\n",
    "# d'index 2 dans la liste ci-dessus\n",
    "obs, reward, terminated, truncated, info = wrapper.step(2)\n",
    "print(obs)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ee42aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(history):\n",
    "    return history['portfolio_valuation', -1]\n",
    "\n",
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    portfolio_initial_value=1_000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    "    # On spécifie la fonction de récompense\n",
    "    reward_function=reward_function,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aa15b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_episodes = 2\n",
    "for episode in range(1, nb_episodes + 1):\n",
    "    obs, _ = env.reset()\n",
    "    print(f'Episode n˚{episode} -- Jeu de donnée {env.name}')\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "    if terminated:\n",
    "        print('Argent perdu')\n",
    "    elif truncated:\n",
    "        print('Épisode terminé')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ee28ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_portfolio_valuation(history):\n",
    "    return round(history['portfolio_valuation', -1], 2)\n",
    "\n",
    "env.add_metric('Portfolio Valuation', metric_portfolio_valuation)\n",
    "\n",
    "done = False\n",
    "obs, _ = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e44c6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_valuation = env.historical_info['portfolio_valuation', -1]\n",
    "# Si on avait WandB :\n",
    "# run.summary['portfolio_valuation'] = portfolio_valuation\n",
    "# On simule ça par un simple print...\n",
    "print(portfolio_valuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72b0ad74",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = env.get_metrics()\n",
    "print(metrics)\n",
    "portfolio_valuation = metrics['Portfolio Valuation']\n",
    "print(portfolio_valuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b1e87a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_v2(df):\n",
    "    df = df.sort_index().dropna().drop_duplicates()\n",
    "\n",
    "    # --- 1. Log Returns (Rendements Logarithmiques) ---\n",
    "    df[\"feature_log_returns\"] = np.log(df[\"close\"]).diff()\n",
    "\n",
    "    # --- 2. Indicateurs de Volatilité (ATR simplifié) ---\n",
    "    df['tr1'] = df['high'] - df['low']\n",
    "    df['tr2'] = np.abs(df['high'] - df['close'].shift(1))\n",
    "    df['tr3'] = np.abs(df['low'] - df['close'].shift(1))\n",
    "    df['tr'] = df[['tr1', 'tr2', 'tr3']].max(axis=1)\n",
    "    df['feature_atr'] = df['tr'].rolling(window=14).mean() / df[\"close\"]\n",
    "\n",
    "    # --- 3. Indicateurs de Tendance (MACD) ---\n",
    "    ema_fast = df['close'].ewm(span=12, adjust=False).mean()\n",
    "    ema_slow = df['close'].ewm(span=26, adjust=False).mean()\n",
    "    df['feature_macd'] = ema_fast - ema_slow\n",
    "    df['feature_macd_signal'] = df['feature_macd'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "    # --- 4. Indicateurs de Momentum (RSI) ---\n",
    "    delta = df['close'].diff()\n",
    "    gain = delta.where(delta > 0, 0)\n",
    "    loss = -delta.where(delta < 0, 0)\n",
    "    avg_gain = gain.rolling(window=14).mean()\n",
    "    avg_loss = loss.rolling(window=14).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    df['feature_rsi'] = 100 - (100 / (1 + rs)) / 100\n",
    "\n",
    "    # --- 5. Nettoyage et Normalisation ---\n",
    "    df = df.dropna()\n",
    "    cols_to_normalize = ['feature_log_returns', 'feature_macd', 'feature_macd_signal', 'feature_atr']\n",
    "    for col in cols_to_normalize:\n",
    "        if df[col].std() > 0:\n",
    "            df[col] = (df[col] - df[col].mean()) / df[col].std()\n",
    "        else:\n",
    "             df[col] = 0.0\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "784a44a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function_v2(history):\n",
    "    # Rendement log du portefeuille à l'étape t\n",
    "    prev_val = history['portfolio_valuation', -2]\n",
    "    curr_val = history['portfolio_valuation', -1]\n",
    "\n",
    "    # Gestion du cas initial\n",
    "    if prev_val == 0: return 0\n",
    "\n",
    "    # Calcul du rendement log\n",
    "    reward = np.log(curr_val / prev_val)\n",
    "\n",
    "    # BONUS : Pénalité de risque (Sharpe Ratio simplifié)\n",
    "    # Si vous voulez un agent prudent, vous pouvez soustraire une fraction de la volatilité récente\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c80764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "from sb3_contrib import RecurrentPPO # PPO avec LSTM\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# ... (insérer ici la fonction preprocess améliorée définie plus haut) ...\n",
    "# ... (insérer ici la fonction reward_function définie plus haut) ...\n",
    "\n",
    "# Création de l'environnement\n",
    "# On utilise DummyVecEnv pour la compatibilité avec Stable Baselines 3\n",
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"data/*.pkl\",\n",
    "    preprocess=preprocess_v2,\n",
    "    reward_function=reward_function_v2,\n",
    "    position_range=(-1, 1),\n",
    "    portfolio_initial_value=1_000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    "    # LIGNE SUPPRIMÉE : window_size=1\n",
    ")\n",
    "\n",
    "# Wrapper pour vectoriser (requis par SB3)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# Configuration du modèle\n",
    "model = RecurrentPPO(\n",
    "    \"MlpLstmPolicy\",\n",
    "    env,\n",
    "    verbose=1,\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=2048,\n",
    "    batch_size=128,\n",
    "    ent_coef=0.01,\n",
    "    tensorboard_log=\"./tensorboard_logs/\"\n",
    ")\n",
    "\n",
    "# Entrainement\n",
    "print(\"Début de l'entraînement...\")\n",
    "model.learn(total_timesteps=100_000)\n",
    "print(\"Entraînement terminé.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19e25a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Récupération des fonctions (Doivent être identiques à l'entraînement) ---\n",
    "# Copiez-collez ici votre fonction preprocess() améliorée de l'étape précédente\n",
    "# et votre reward_function (bien que pour le test, la reward ne serve à rien,\n",
    "# l'env en a besoin pour s'initialiser).\n",
    "\n",
    "# --- 2. Chargement de l'environnement de Test ---\n",
    "# Idéalement, pointez vers un fichier .pkl que l'agent n'a JAMAIS vu (ex: 2024.pkl)\n",
    "# Si vous n'avez pas de données séparées, utilisez le même dossier mais gardez en tête\n",
    "# que le résultat sera biaisé (overfitting).\n",
    "env_test = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"data/*.pkl\",\n",
    "    preprocess=preprocess_v2,\n",
    "    reward_function=reward_function_v2,\n",
    "    position_range=(-1, 1),\n",
    "    portfolio_initial_value=1_000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    "    # CETTE LIGNE DOIT ÊTRE SUPPRIMÉE :\n",
    "    # window_size=1\n",
    ")\n",
    "\n",
    "# On wrap l'environnement comme pour l'entraînement\n",
    "env_test = DummyVecEnv([lambda: env_test])\n",
    "\n",
    "# --- 3. Chargement de l'Agent ---\n",
    "# On charge le modèle sauvegardé\n",
    "model = RecurrentPPO.load(\"mon_agent_trading\")\n",
    "\n",
    "print(\"Modèle chargé avec succès. Début du backtest...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4173f44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# --- NOUVELLES IMPORTATIONS ---\n",
    "import wandb\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "# --- HYPERPARAMÈTRES (pour le suivi WandB) ---\n",
    "config = {\n",
    "    \"policy_type\": \"MlpLstmPolicy\",\n",
    "    \"total_timesteps\": 100_000,\n",
    "    \"env_id\": \"MultiDatasetTradingEnv\",\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"n_steps\": 2048,\n",
    "    \"batch_size\": 128,\n",
    "    \"ent_coef\": 0.01,\n",
    "    # Ajoutez ici tous les paramètres de l'environnement (frais, capital, etc.)\n",
    "}\n",
    "\n",
    "# --- 1. INITIALISATION DE WANDB ---\n",
    "run = wandb.init(\n",
    "    project=\"RL-Trading-Project\", # Nom de votre projet\n",
    "    entity=\"arthur-collignon-cpe-lyon\", # Remplacez par votre nom d'utilisateur WandB\n",
    "    config=config,\n",
    "    sync_tensorboard=True, # Synchroniser TensorBoard (si vous l'utilisez encore)\n",
    "    monitor_gym=True,\n",
    "    save_code=True,\n",
    ")\n",
    "\n",
    "# --- 2. CRÉATION DE L'ENVIRONNEMENT ET DU MODÈLE (Comme avant) ---\n",
    "\n",
    "# ... (Vos fonctions preprocess/reward_function ici) ...\n",
    "\n",
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    reward_function=reward_function,\n",
    "    position_range=(-1, 1),\n",
    "    portfolio_initial_value=1_000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    ")\n",
    "\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "model = RecurrentPPO(\n",
    "    config[\"policy_type\"], # Utiliser le dictionnaire de config\n",
    "    env,\n",
    "    verbose=0, # Mettre à 0 pour éviter les logs console en faveur de WandB\n",
    "    learning_rate=config[\"learning_rate\"],\n",
    "    n_steps=config[\"n_steps\"],\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    ent_coef=config[\"ent_coef\"],\n",
    "    tensorboard_log=f\"runs/{run.id}\", # Pointer le log TensorBoard vers le dossier de WandB\n",
    ")\n",
    "\n",
    "# --- 3. DÉFINITION DU CALLBACK WANDB ---\n",
    "wandb_callback = WandbCallback(\n",
    "    model_save_path=f\"models/{run.id}\",\n",
    "    verbose=1,\n",
    "    model_save_freq=10000, # Sauvegarder le modèle tous les 10,000 pas\n",
    ")\n",
    "\n",
    "\n",
    "# --- 4. ENTRAÎNEMENT AVEC LE CALLBACK ---\n",
    "try:\n",
    "    print(\"Début de l'entraînement avec WandB...\")\n",
    "    model.learn(\n",
    "        total_timesteps=config[\"total_timesteps\"],\n",
    "        callback=wandb_callback, # Passage du Callback ici\n",
    "    )\n",
    "finally:\n",
    "    # --- 5. FIN DU RUN WANDB ---\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94e5cc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Récupération des fonctions (Doivent être identiques à l'entraînement) ---\n",
    "# Copiez-collez ici votre fonction preprocess() améliorée de l'étape précédente\n",
    "# et votre reward_function (bien que pour le test, la reward ne serve à rien,\n",
    "# l'env en a besoin pour s'initialiser).\n",
    "\n",
    "# --- 2. Chargement de l'environnement de Test ---\n",
    "# Idéalement, pointez vers un fichier .pkl que l'agent n'a JAMAIS vu (ex: 2024.pkl)\n",
    "# Si vous n'avez pas de données séparées, utilisez le même dossier mais gardez en tête\n",
    "# que le résultat sera biaisé (overfitting).\n",
    "env_test = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"data/*.pkl\",\n",
    "    preprocess=preprocess_v2,\n",
    "    reward_function=reward_function_v2,\n",
    "    position_range=(-1, 1),\n",
    "    portfolio_initial_value=1_000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    "    # CETTE LIGNE DOIT ÊTRE SUPPRIMÉE :\n",
    "    # window_size=1\n",
    ")\n",
    "\n",
    "# On wrap l'environnement comme pour l'entraînement\n",
    "env_test = DummyVecEnv([lambda: env_test])\n",
    "\n",
    "# --- 3. Chargement de l'Agent ---\n",
    "# On charge le modèle sauvegardé\n",
    "model = RecurrentPPO.load(\"mon_agent_trading\")\n",
    "\n",
    "print(\"Modèle chargé avec succès. Début du backtest...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4dd2b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# --- NOUVELLES IMPORTATIONS ---\n",
    "import wandb\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "# --- HYPERPARAMÈTRES (pour le suivi WandB) ---\n",
    "config = {\n",
    "    \"policy_type\": \"MlpLstmPolicy\",\n",
    "    \"total_timesteps\": 100_000,\n",
    "    \"env_id\": \"MultiDatasetTradingEnv\",\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"n_steps\": 2048,\n",
    "    \"batch_size\": 128,\n",
    "    \"ent_coef\": 0.01,\n",
    "    # Ajoutez ici tous les paramètres de l'environnement (frais, capital, etc.)\n",
    "}\n",
    "\n",
    "# --- 1. INITIALISATION DE WANDB ---\n",
    "run = wandb.init(\n",
    "    project=\"RL-Trading-Project\", # Nom de votre projet\n",
    "    entity=\"arthur-collignon-cpe-lyon\", # Remplacez par votre nom d'utilisateur WandB\n",
    "    config=config,\n",
    "    sync_tensorboard=True, # Synchroniser TensorBoard (si vous l'utilisez encore)\n",
    "    monitor_gym=True,\n",
    "    save_code=True,\n",
    ")\n",
    "\n",
    "# --- 2. CRÉATION DE L'ENVIRONNEMENT ET DU MODÈLE (Comme avant) ---\n",
    "\n",
    "# ... (Vos fonctions preprocess/reward_function ici) ...\n",
    "\n",
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    reward_function=reward_function,\n",
    "    position_range=(-1, 1),\n",
    "    portfolio_initial_value=1_000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    ")\n",
    "\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "model = RecurrentPPO(\n",
    "    config[\"policy_type\"], # Utiliser le dictionnaire de config\n",
    "    env,\n",
    "    verbose=0, # Mettre à 0 pour éviter les logs console en faveur de WandB\n",
    "    learning_rate=config[\"learning_rate\"],\n",
    "    n_steps=config[\"n_steps\"],\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    ent_coef=config[\"ent_coef\"],\n",
    "    tensorboard_log=f\"runs/{run.id}\", # Pointer le log TensorBoard vers le dossier de WandB\n",
    ")\n",
    "\n",
    "# --- DÉFINITION DE LA LISTE DE CALLBACKS ---\n",
    "callback = CallbackList([\n",
    "    # Callback SB3 WandB: gère les logs d'entraînement (reward, loss, entropie)\n",
    "    WandbCallback(\n",
    "        model_save_path=f\"models/{run.id}\",\n",
    "        verbose=0,\n",
    "        model_save_freq=10000,\n",
    "    ),\n",
    "    # Callback Personnalisé: gère les logs financiers (portfolio_valuation, returns)\n",
    "    CustomTradingCallback(verbose=0), \n",
    "])\n",
    "\n",
    "\n",
    "# --- ENTRAÎNEMENT AVEC LA LISTE DE CALLBACKS ---\n",
    "try:\n",
    "    print(\"Début de l'entraînement avec WandB...\")\n",
    "    model.learn(\n",
    "        total_timesteps=config[\"total_timesteps\"],\n",
    "        callback=callback, # Passe la liste des deux Callbacks\n",
    "    )\n",
    "finally:\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6b4f1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback, CallbackList\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class CustomTradingCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback pour enregistrer les métriques financières finales dans WandB\n",
    "    à la fin de chaque épisode.\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose: int = 0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_num = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Vérifie si l'épisode est terminé\n",
    "        if self.locals['dones'][0]: # L'indice 0 est pour notre DummyVecEnv simple\n",
    "            self.episode_num += 1\n",
    "\n",
    "            # 1. Accéder à l'environnement non-vectorisé\n",
    "            # Le VecEnv est un wrapper, on doit aller chercher l'env \"nu\"\n",
    "            raw_env = self.training_env.envs[0].unwrapped\n",
    "\n",
    "            # 2. Calculer les métriques\n",
    "            # Note: gym-trading-env calcule déjà les métriques finales\n",
    "            metrics = raw_env.get_metrics()\n",
    "\n",
    "            # --- Journalisation dans WandB ---\n",
    "\n",
    "            # A. Le critère d'évaluation final (Portfolio Valuation)\n",
    "            final_val = metrics.get('Portfolio Valuation')\n",
    "\n",
    "            # B. Performance vs. Marché (pour le contexte)\n",
    "            market_return_str = metrics.get('Market Return', '0.00%').strip()\n",
    "            market_return = float(market_return_str.strip('%')) / 100\n",
    "\n",
    "            portfolio_return_str = metrics.get('Portfolio Return', '0.00%').strip()\n",
    "            portfolio_return = float(portfolio_return_str.strip('%')) / 100\n",
    "\n",
    "\n",
    "            if self.logger is not None:\n",
    "                # Enregistrer les métriques spécifiques\n",
    "                self.logger.record(\"episode/final_portfolio_valuation\", final_val)\n",
    "                self.logger.record(\"episode/return_vs_market_pct\", (portfolio_return - market_return) * 100)\n",
    "                self.logger.record(\"episode/total_portfolio_return_pct\", portfolio_return * 100)\n",
    "                self.logger.record(\"episode/market_return_pct\", market_return * 100)\n",
    "                self.logger.record(\"episode/steps\", raw_env.steps)\n",
    "\n",
    "                # S'assurer que le log est écrit immédiatement\n",
    "                self.logger.dump(step=self.num_timesteps)\n",
    "\n",
    "            # Optionnel: Réinitialiser l'environnement si nécessaire (déjà fait par SB3)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa77e511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# --- NOUVELLES IMPORTATIONS ---\n",
    "import wandb\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "# --- HYPERPARAMÈTRES (pour le suivi WandB) ---\n",
    "config = {\n",
    "    \"policy_type\": \"MlpLstmPolicy\",\n",
    "    \"total_timesteps\": 100_000,\n",
    "    \"env_id\": \"MultiDatasetTradingEnv\",\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"n_steps\": 2048,\n",
    "    \"batch_size\": 128,\n",
    "    \"ent_coef\": 0.01,\n",
    "    # Ajoutez ici tous les paramètres de l'environnement (frais, capital, etc.)\n",
    "}\n",
    "\n",
    "# --- 1. INITIALISATION DE WANDB ---\n",
    "run = wandb.init(\n",
    "    project=\"RL-Trading-Project\", # Nom de votre projet\n",
    "    entity=\"arthur-collignon-cpe-lyon\", # Remplacez par votre nom d'utilisateur WandB\n",
    "    config=config,\n",
    "    sync_tensorboard=True, # Synchroniser TensorBoard (si vous l'utilisez encore)\n",
    "    monitor_gym=True,\n",
    "    save_code=True,\n",
    ")\n",
    "\n",
    "# --- 2. CRÉATION DE L'ENVIRONNEMENT ET DU MODÈLE (Comme avant) ---\n",
    "\n",
    "# ... (Vos fonctions preprocess/reward_function ici) ...\n",
    "\n",
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    reward_function=reward_function,\n",
    "    position_range=(-1, 1),\n",
    "    portfolio_initial_value=1_000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    ")\n",
    "\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "model = RecurrentPPO(\n",
    "    config[\"policy_type\"], # Utiliser le dictionnaire de config\n",
    "    env,\n",
    "    verbose=0, # Mettre à 0 pour éviter les logs console en faveur de WandB\n",
    "    learning_rate=config[\"learning_rate\"],\n",
    "    n_steps=config[\"n_steps\"],\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    ent_coef=config[\"ent_coef\"],\n",
    "    tensorboard_log=f\"runs/{run.id}\", # Pointer le log TensorBoard vers le dossier de WandB\n",
    ")\n",
    "\n",
    "# --- DÉFINITION DE LA LISTE DE CALLBACKS ---\n",
    "callback = CallbackList([\n",
    "    # Callback SB3 WandB: gère les logs d'entraînement (reward, loss, entropie)\n",
    "    WandbCallback(\n",
    "        model_save_path=f\"models/{run.id}\",\n",
    "        verbose=0,\n",
    "        model_save_freq=10000,\n",
    "    ),\n",
    "    # Callback Personnalisé: gère les logs financiers (portfolio_valuation, returns)\n",
    "    CustomTradingCallback(verbose=0), \n",
    "])\n",
    "\n",
    "\n",
    "# --- ENTRAÎNEMENT AVEC LA LISTE DE CALLBACKS ---\n",
    "try:\n",
    "    print(\"Début de l'entraînement avec WandB...\")\n",
    "    model.learn(\n",
    "        total_timesteps=config[\"total_timesteps\"],\n",
    "        callback=callback, # Passe la liste des deux Callbacks\n",
    "    )\n",
    "finally:\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ff2cd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback, CallbackList\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class CustomTradingCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback pour enregistrer les métriques financières finales dans WandB\n",
    "    à la fin de chaque épisode.\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose: int = 0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_num = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Vérifie si l'épisode est terminé\n",
    "        if self.locals['dones'][0]:\n",
    "            self.episode_num += 1\n",
    "\n",
    "            raw_env = self.training_env.envs[0].unwrapped\n",
    "            metrics = raw_env.get_metrics()\n",
    "\n",
    "            # 1. Accéder à l'environnement non-vectorisé\n",
    "            # Le VecEnv est un wrapper, on doit aller chercher l'env \"nu\"\n",
    "            raw_env = self.training_env.envs[0].unwrapped\n",
    "\n",
    "            # 2. Calculer les métriques\n",
    "            # Note: gym-trading-env calcule déjà les métriques finales\n",
    "            metrics = raw_env.get_metrics()\n",
    "\n",
    "            # --- Journalisation dans WandB ---\n",
    "\n",
    "            # A. Le critère d'évaluation final (Portfolio Valuation)\n",
    "            final_val = metrics.get('Portfolio Valuation')\n",
    "\n",
    "            # B. Performance vs. Marché (pour le contexte)\n",
    "            market_return_str = metrics.get('Market Return', '0.00%').strip()\n",
    "            market_return = float(market_return_str.strip('%')) / 100\n",
    "\n",
    "            portfolio_return_str = metrics.get('Portfolio Return', '0.00%').strip()\n",
    "            portfolio_return = float(portfolio_return_str.strip('%')) / 100\n",
    "\n",
    "\n",
    "            if self.logger is not None:\n",
    "                # Enregistrer les métriques spécifiques\n",
    "                self.logger.record(\"episode/final_portfolio_valuation\", final_val)\n",
    "                self.logger.record(\"episode/return_vs_market_pct\", (portfolio_return - market_return) * 100)\n",
    "                self.logger.record(\"episode/total_portfolio_return_pct\", portfolio_return * 100)\n",
    "                self.logger.record(\"episode/market_return_pct\", market_return * 100)\n",
    "                # CORRECTION ICI : Utiliser raw_env.step au lieu de raw_env.steps\n",
    "                self.logger.record(\"episode/steps\", raw_env.step)\n",
    "                \n",
    "                # S'assurer que le log est écrit immédiatement\n",
    "                self.logger.dump(step=self.num_timesteps)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cce29615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# --- NOUVELLES IMPORTATIONS ---\n",
    "import wandb\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "# --- HYPERPARAMÈTRES (pour le suivi WandB) ---\n",
    "config = {\n",
    "    \"policy_type\": \"MlpLstmPolicy\",\n",
    "    \"total_timesteps\": 100_000,\n",
    "    \"env_id\": \"MultiDatasetTradingEnv\",\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"n_steps\": 2048,\n",
    "    \"batch_size\": 128,\n",
    "    \"ent_coef\": 0.01,\n",
    "    # Ajoutez ici tous les paramètres de l'environnement (frais, capital, etc.)\n",
    "}\n",
    "\n",
    "# --- 1. INITIALISATION DE WANDB ---\n",
    "run = wandb.init(\n",
    "    project=\"RL-Trading-Project\", # Nom de votre projet\n",
    "    entity=\"arthur-collignon-cpe-lyon\", # Remplacez par votre nom d'utilisateur WandB\n",
    "    config=config,\n",
    "    sync_tensorboard=True, # Synchroniser TensorBoard (si vous l'utilisez encore)\n",
    "    monitor_gym=True,\n",
    "    save_code=True,\n",
    ")\n",
    "\n",
    "# --- 2. CRÉATION DE L'ENVIRONNEMENT ET DU MODÈLE (Comme avant) ---\n",
    "\n",
    "# ... (Vos fonctions preprocess/reward_function ici) ...\n",
    "\n",
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    reward_function=reward_function,\n",
    "    position_range=(-1, 1),\n",
    "    portfolio_initial_value=1_000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    ")\n",
    "\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "model = RecurrentPPO(\n",
    "    config[\"policy_type\"], # Utiliser le dictionnaire de config\n",
    "    env,\n",
    "    verbose=0, # Mettre à 0 pour éviter les logs console en faveur de WandB\n",
    "    learning_rate=config[\"learning_rate\"],\n",
    "    n_steps=config[\"n_steps\"],\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    ent_coef=config[\"ent_coef\"],\n",
    "    tensorboard_log=f\"runs/{run.id}\", # Pointer le log TensorBoard vers le dossier de WandB\n",
    ")\n",
    "\n",
    "# --- DÉFINITION DE LA LISTE DE CALLBACKS ---\n",
    "callback = CallbackList([\n",
    "    # Callback SB3 WandB: gère les logs d'entraînement (reward, loss, entropie)\n",
    "    WandbCallback(\n",
    "        model_save_path=f\"models/{run.id}\",\n",
    "        verbose=0,\n",
    "        model_save_freq=10000,\n",
    "    ),\n",
    "    # Callback Personnalisé: gère les logs financiers (portfolio_valuation, returns)\n",
    "    CustomTradingCallback(verbose=0),\n",
    "])\n",
    "\n",
    "\n",
    "# --- ENTRAÎNEMENT AVEC LA LISTE DE CALLBACKS ---\n",
    "try:\n",
    "    print(\"Début de l'entraînement avec WandB...\")\n",
    "    model.learn(\n",
    "        total_timesteps=config[\"total_timesteps\"],\n",
    "        callback=callback, # Passe la liste des deux Callbacks\n",
    "    )\n",
    "finally:\n",
    "    run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
