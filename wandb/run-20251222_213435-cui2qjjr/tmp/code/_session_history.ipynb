{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41c6531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "from gym_trading_env.wrapper import DiscreteActionsWrapper\n",
    "from stable_baselines3 import PPO\n",
    "from gym_trading_env.renderer import Renderer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "# --- 1. CONFIGURATION ET INDICATEURS ---\n",
    "\n",
    "# On définit les hyperparamètres ici pour que WandB puisse les enregistrer\n",
    "config = {\n",
    "    \"policy_type\": \"MlpPolicy\",\n",
    "    \"total_timesteps\": 200_000,\n",
    "    \"learning_rate\": 0.0003,\n",
    "    \"ent_coef\": 0.02, # Coefficient d'exploration\n",
    "    \"batch_size\": 128,\n",
    "    \"positions\": [-0.5, 0, 0.25, 0.5, 0.75, 1.0, 1.25, 1.5], # Hybride\n",
    "    \"project_name\": \"RL-Trading-Project\"\n",
    "}\n",
    "\n",
    "def calculate_rsi(series, window=14):\n",
    "    delta = series.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def calculate_macd(series, slow=26, fast=12, signal=9):\n",
    "    exp1 = series.ewm(span=fast, adjust=False).mean()\n",
    "    exp2 = series.ewm(span=slow, adjust=False).mean()\n",
    "    macd = exp1 - exp2\n",
    "    return macd\n",
    "\n",
    "def preprocess(df):\n",
    "    df = df.sort_index().dropna().drop_duplicates()\n",
    "    df['feature_close'] = df['close'].pct_change()\n",
    "    df['feature_rsi'] = calculate_rsi(df['close']) / 100\n",
    "    df['feature_macd'] = calculate_macd(df['close'])\n",
    "    return df.dropna()\n",
    "\n",
    "def reward_function(history):\n",
    "    current_val = history['portfolio_valuation', -1]\n",
    "    prev_val = history['portfolio_valuation', -2]\n",
    "    reward = np.log(current_val / prev_val)\n",
    "\n",
    "    # Malus pour les positions Short (pour éviter le biais négatif)\n",
    "    if history['position', -1] < 0:\n",
    "        reward -= 0.0002\n",
    "\n",
    "    return reward\n",
    "\n",
    "# --- 2. INITIALISATION DE WANDB ---\n",
    "run = wandb.init(\n",
    "    project=config[\"project_name\"],\n",
    "    config=config,\n",
    "    sync_tensorboard=True, # Synchronise automatiquement les logs SB3\n",
    "    monitor_gym=True,      # Essaie d'enregistrer les vidéos (si disponible)\n",
    "    save_code=True,        # Sauvegarde ce script dans WandB\n",
    ")\n",
    "\n",
    "# --- 3. CRÉATION DE L'ENVIRONNEMENT ---\n",
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"./data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    portfolio_initial_value=1000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    "    reward_function=reward_function,\n",
    ")\n",
    "\n",
    "# Wrapper Hybride (Int -> Float spécifique)\n",
    "env = DiscreteActionsWrapper(env, positions=config[\"positions\"])\n",
    "\n",
    "# --- 4. ENTRAÎNEMENT AVEC CALLBACK WANDB ---\n",
    "model = PPO(\n",
    "    config[\"policy_type\"],\n",
    "    env,\n",
    "    verbose=1,\n",
    "    learning_rate=config[\"learning_rate\"],\n",
    "    ent_coef=config[\"ent_coef\"],\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    tensorboard_log=f\"runs/{run.id}\" # Dossier unique pour Tensorboard\n",
    ")\n",
    "\n",
    "print(f\"Lancement de l'entraînement WandB : {run.name}\")\n",
    "model.learn(\n",
    "    total_timesteps=config[\"total_timesteps\"],\n",
    "    callback=WandbCallback(\n",
    "        gradient_save_freq=100,\n",
    "        model_save_path=f\"models/{run.id}\",\n",
    "        verbose=2,\n",
    "    )\n",
    ")\n",
    "model.save(\"ppo_trading_wandb_final\")\n",
    "\n",
    "# --- 5. ÉVALUATION ET LOGGING FINAL ---\n",
    "print(\"Évaluation finale...\")\n",
    "obs, info = env.reset()\n",
    "done, truncated = False, False\n",
    "\n",
    "while not (done or truncated):\n",
    "    action, _ = model.predict(obs)\n",
    "    action = int(action) # Conversion array -> int pour le wrapper\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "# Récupération des métriques finales de l'environnement\n",
    "final_metrics = env.unwrapped.get_metrics()\n",
    "print(\"Métriques finales :\", final_metrics)\n",
    "\n",
    "# Envoi des métriques clés à WandB (pour le tableau de bord)\n",
    "wandb.log({\n",
    "    \"final_portfolio_valuation\": info['portfolio_valuation'],\n",
    "    \"market_return\": final_metrics.get(\"Market Return\", 0),\n",
    "    \"portfolio_return\": final_metrics.get(\"Portfolio Return\", 0)\n",
    "})\n",
    "\n",
    "# --- 6. VISUALISATION ---\n",
    "env.unwrapped.save_for_render(dir=\"render_logs\")\n",
    "\n",
    "# On ferme le run WandB proprement\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb205047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lancement du renderer local\n",
    "renderer = Renderer(render_logs_dir=\"render_logs\")\n",
    "renderer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4a8d748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "from gym_trading_env.wrapper import DiscreteActionsWrapper\n",
    "from gym_trading_env.renderer import Renderer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "# Vous aurez besoin de cette librairie pour le LSTM\n",
    "# !pip install sb3-contrib\n",
    "\n",
    "from sb3_contrib import RecurrentPPO\n",
    "\n",
    "# --- 1. CONFIGURATION (WandB) ---\n",
    "config = {\n",
    "    \"policy_type\": \"MlpLstmPolicy\",  # Changement majeur : LSTM\n",
    "    \"total_timesteps\": 500_000,      # Un peu plus long pour le LSTM\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"batch_size\": 128,\n",
    "    \"n_steps\": 2048,\n",
    "    \"window_size\": 20,               # Fenêtre d'observation pour le LSTM\n",
    "    \"positions\": [0, 0.5, 1.0],      # Simplifié au début : Cash, Moitié, Full (Pas de levier/short risqué)\n",
    "    \"project_name\": \"RL-Trading-Project\",\n",
    "    \"run_name\": \"RecurrentPPO_Optimized\"\n",
    "}\n",
    "\n",
    "# --- 2. FONCTIONS DE TRAITEMENT ---\n",
    "\n",
    "def calculate_indicators(df):\n",
    "    # RSI\n",
    "    delta = df['close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    df['feature_rsi'] = 100 - (100 / (1 + rs))\n",
    "    df['feature_rsi'] = df['feature_rsi'] / 100.0 # Normalisé\n",
    "\n",
    "    # MACD\n",
    "    exp1 = df['close'].ewm(span=12, adjust=False).mean()\n",
    "    exp2 = df['close'].ewm(span=26, adjust=False).mean()\n",
    "    df['feature_macd'] = (exp1 - exp2) / df['close'] # Normalisé par le prix\n",
    "\n",
    "    # ATR (Volatilité) - Important pour la survie\n",
    "    high_low = df['high'] - df['low']\n",
    "    high_close = np.abs(df['high'] - df['close'].shift())\n",
    "    low_close = np.abs(df['low'] - df['close'].shift())\n",
    "    true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    df['feature_atr'] = true_range.rolling(14).mean() / df['close']\n",
    "    \n",
    "    # Returns\n",
    "    df['feature_return'] = df['close'].pct_change()\n",
    "    \n",
    "    return df.dropna()\n",
    "\n",
    "def preprocess(df):\n",
    "    df = df.sort_index().dropna().drop_duplicates()\n",
    "    return calculate_indicators(df)\n",
    "\n",
    "def reward_function(history):\n",
    "    # Rendement logarithmique\n",
    "    current_val = history['portfolio_valuation', -1]\n",
    "    prev_val = history['portfolio_valuation', -2]\n",
    "    ret = np.log(current_val / prev_val)\n",
    "    \n",
    "    # Pénalité de volatilité (Sharpe Ratio implicite)\n",
    "    # Cela calme l'agent pour éviter les -50%\n",
    "    risk_penalty = 0.1 * (ret ** 2)\n",
    "    \n",
    "    return ret - risk_penalty\n",
    "\n",
    "# --- 3. INITIALISATION WANDB ---\n",
    "run = wandb.init(\n",
    "    project=config[\"project_name\"],\n",
    "    name=config[\"run_name\"],\n",
    "    config=config,\n",
    "    sync_tensorboard=True, # Synchronise les logs SB3 avec WandB\n",
    "    monitor_gym=True,\n",
    "    save_code=True,\n",
    ")\n",
    "\n",
    "# --- 4. CRÉATION DE L'ENVIRONNEMENT ---\n",
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"./data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    portfolio_initial_value=1000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    "    reward_function=reward_function,\n",
    "    window_size=config[\"window_size\"] # Important pour le LSTM\n",
    ")\n",
    "\n",
    "env = DiscreteActionsWrapper(env, positions=config[\"positions\"])\n",
    "\n",
    "# --- 5. CRÉATION DU MODÈLE ET ENTRAÎNEMENT ---\n",
    "model = RecurrentPPO(\n",
    "    config[\"policy_type\"],\n",
    "    env,\n",
    "    verbose=1,\n",
    "    learning_rate=config[\"learning_rate\"],\n",
    "    ent_coef=config[\"ent_coef\"],\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    n_steps=config[\"n_steps\"],\n",
    "    # Log dans le dossier spécifique pour que WandB le trouve\n",
    "    tensorboard_log=f\"runs/{run.id}\" \n",
    ")\n",
    "\n",
    "print(f\"Lancement du run WandB : {run.name}\")\n",
    "model.learn(\n",
    "    total_timesteps=config[\"total_timesteps\"],\n",
    "    callback=WandbCallback(\n",
    "        gradient_save_freq=100,\n",
    "        model_save_path=f\"models/{run.id}\",\n",
    "        verbose=2,\n",
    "    )\n",
    ")\n",
    "\n",
    "model.save(\"recurrent_ppo_final\")\n",
    "\n",
    "# --- 6. ÉVALUATION ET LOGGING FINAL ---\n",
    "print(\"Évaluation...\")\n",
    "obs, info = env.reset()\n",
    "done, truncated = False, False\n",
    "\n",
    "while not (done or truncated):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    action = int(action)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "# Envoi des métriques finales manuelles\n",
    "final_metrics = env.unwrapped.get_metrics()\n",
    "wandb.log({\n",
    "    \"final_portfolio_valuation\": info['portfolio_valuation'],\n",
    "    \"market_return\": final_metrics.get(\"Market Return\", 0),\n",
    "    \"portfolio_return\": final_metrics.get(\"Portfolio Return\", 0)\n",
    "})\n",
    "\n",
    "# Fin du run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99c86a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "import gym_trading_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b745fae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       open    high     low   close       volume  \\\n",
      "date_open                                                          \n",
      "2020-08-18 07:00:00  430.00  435.00  410.00  430.30   487.154463   \n",
      "2020-08-18 08:00:00  430.27  431.79  430.27  430.80   454.176153   \n",
      "2020-08-18 09:00:00  430.86  431.13  428.71  429.35  1183.710884   \n",
      "2020-08-18 10:00:00  429.75  432.69  428.59  431.90  1686.183227   \n",
      "2020-08-18 11:00:00  432.09  432.89  426.99  427.45  1980.692724   \n",
      "\n",
      "                             date_close  \n",
      "date_open                                \n",
      "2020-08-18 07:00:00 2020-08-18 08:00:00  \n",
      "2020-08-18 08:00:00 2020-08-18 09:00:00  \n",
      "2020-08-18 09:00:00 2020-08-18 10:00:00  \n",
      "2020-08-18 10:00:00 2020-08-18 11:00:00  \n",
      "2020-08-18 11:00:00 2020-08-18 12:00:00  "
     ]
    }
   ],
   "source": [
    "def preprocess(df):\n",
    "    df = df.sort_index()\n",
    "    df = df.dropna()\n",
    "    df = df.drop_duplicates()\n",
    "    return df\n",
    "\n",
    "df = preprocess(pd.read_pickle('./data/binance-ETHUSD-1h.pkl'))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "619e5964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       open    high     low   close       volume  \\\n",
      "date_open                                                          \n",
      "2020-08-18 07:00:00  430.00  435.00  410.00  430.30   487.154463   \n",
      "2020-08-18 08:00:00  430.27  431.79  430.27  430.80   454.176153   \n",
      "2020-08-18 09:00:00  430.86  431.13  428.71  429.35  1183.710884   \n",
      "2020-08-18 10:00:00  429.75  432.69  428.59  431.90  1686.183227   \n",
      "2020-08-18 11:00:00  432.09  432.89  426.99  427.45  1980.692724   \n",
      "\n",
      "                             date_close  feature_close  \n",
      "date_open                                               \n",
      "2020-08-18 07:00:00 2020-08-18 08:00:00      -1.891634  \n",
      "2020-08-18 08:00:00 2020-08-18 09:00:00      -1.891128  \n",
      "2020-08-18 09:00:00 2020-08-18 10:00:00      -1.892594  \n",
      "2020-08-18 10:00:00 2020-08-18 11:00:00      -1.890016  \n",
      "2020-08-18 11:00:00 2020-08-18 12:00:00      -1.894514  "
     ]
    }
   ],
   "source": [
    "def preprocess(df):\n",
    "    df = df.sort_index()\n",
    "    df = df.dropna()\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    df['feature_close'] = (df['close'] - df['close'].mean()) / df['close'].std()\n",
    "\n",
    "    return df\n",
    "\n",
    "df = preprocess(pd.read_pickle('./data/binance-ETHUSD-1h.pkl'))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8302c02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    portfolio_initial_value=1_000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    ")\n",
    "\n",
    "obs, _ = env.reset()\n",
    "# On veut une position de 88% ETH / 12% USD\n",
    "obs, reward, terminated, truncated, info = env.step(0.88)\n",
    "print(obs)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a28bd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    position_range=(0, 1),  # ICI : (borne min, borne max)\n",
    "    portfolio_initial_value=1_000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60093001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_trading_env.wrapper import DiscreteActionsWrapper\n",
    "\n",
    "# Vous pouvez aussi appeler le wrapper `env` pour faire plus simple\n",
    "# Ici, je fais explicitement la distinction entre `wrapper` et `env`\n",
    "wrapper = DiscreteActionsWrapper(env, positions=[-1, 0, 0.25, 0.5, 0.75, 1, 2])\n",
    "obs, _ = wrapper.reset()\n",
    "# On veut une position de 25% ETH / 75% USD ; cela correspond à la position\n",
    "# d'index 2 dans la liste ci-dessus\n",
    "obs, reward, terminated, truncated, info = wrapper.step(2)\n",
    "print(obs)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8df06e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(history):\n",
    "    return history['portfolio_valuation', -1]\n",
    "\n",
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    portfolio_initial_value=1_000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    "    # On spécifie la fonction de récompense\n",
    "    reward_function=reward_function,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c2c7642",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_episodes = 2\n",
    "for episode in range(1, nb_episodes + 1):\n",
    "    obs, _ = env.reset()\n",
    "    print(f'Episode n˚{episode} -- Jeu de donnée {env.name}')\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "    if terminated:\n",
    "        print('Argent perdu')\n",
    "    elif truncated:\n",
    "        print('Épisode terminé')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c14a2b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_portfolio_valuation(history):\n",
    "    return round(history['portfolio_valuation', -1], 2)\n",
    "\n",
    "env.add_metric('Portfolio Valuation', metric_portfolio_valuation)\n",
    "\n",
    "done = False\n",
    "obs, _ = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6e10e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_valuation = env.historical_info['portfolio_valuation', -1]\n",
    "# Si on avait WandB :\n",
    "# run.summary['portfolio_valuation'] = portfolio_valuation\n",
    "# On simule ça par un simple print...\n",
    "print(portfolio_valuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "081a9026",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = env.get_metrics()\n",
    "print(metrics)\n",
    "portfolio_valuation = metrics['Portfolio Valuation']\n",
    "print(portfolio_valuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7e0e3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "from gym_trading_env.wrapper import DiscreteActionsWrapper\n",
    "from gym_trading_env.renderer import Renderer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "# Vous aurez besoin de cette librairie pour le LSTM\n",
    "# !pip install sb3-contrib\n",
    "\n",
    "from sb3_contrib import RecurrentPPO\n",
    "\n",
    "# --- 1. CONFIGURATION (WandB) ---\n",
    "config = {\n",
    "    \"policy_type\": \"MlpLstmPolicy\",  # Changement majeur : LSTM\n",
    "    \"total_timesteps\": 500_000,      # Un peu plus long pour le LSTM\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"batch_size\": 128,\n",
    "    \"n_steps\": 2048,\n",
    "    \"window_size\": 20,               # Fenêtre d'observation pour le LSTM\n",
    "    \"positions\": [0, 0.5, 1.0],      # Simplifié au début : Cash, Moitié, Full (Pas de levier/short risqué)\n",
    "    \"project_name\": \"RL-Trading-Project\",\n",
    "    \"run_name\": \"RecurrentPPO_Optimized\"\n",
    "}\n",
    "\n",
    "# --- 2. FONCTIONS DE TRAITEMENT ---\n",
    "\n",
    "def calculate_indicators(df):\n",
    "    # RSI\n",
    "    delta = df['close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    df['feature_rsi'] = 100 - (100 / (1 + rs))\n",
    "    df['feature_rsi'] = df['feature_rsi'] / 100.0 # Normalisé\n",
    "\n",
    "    # MACD\n",
    "    exp1 = df['close'].ewm(span=12, adjust=False).mean()\n",
    "    exp2 = df['close'].ewm(span=26, adjust=False).mean()\n",
    "    df['feature_macd'] = (exp1 - exp2) / df['close'] # Normalisé par le prix\n",
    "\n",
    "    # ATR (Volatilité) - Important pour la survie\n",
    "    high_low = df['high'] - df['low']\n",
    "    high_close = np.abs(df['high'] - df['close'].shift())\n",
    "    low_close = np.abs(df['low'] - df['close'].shift())\n",
    "    true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    df['feature_atr'] = true_range.rolling(14).mean() / df['close']\n",
    "\n",
    "    # Returns\n",
    "    df['feature_return'] = df['close'].pct_change()\n",
    "\n",
    "    return df.dropna()\n",
    "\n",
    "def preprocess(df):\n",
    "    df = df.sort_index().dropna().drop_duplicates()\n",
    "    return calculate_indicators(df)\n",
    "\n",
    "def reward_function(history):\n",
    "    # Rendement logarithmique\n",
    "    current_val = history['portfolio_valuation', -1]\n",
    "    prev_val = history['portfolio_valuation', -2]\n",
    "    ret = np.log(current_val / prev_val)\n",
    "\n",
    "    # Pénalité de volatilité (Sharpe Ratio implicite)\n",
    "    # Cela calme l'agent pour éviter les -50%\n",
    "    risk_penalty = 0.1 * (ret ** 2)\n",
    "\n",
    "    return ret - risk_penalty\n",
    "\n",
    "# --- 3. INITIALISATION WANDB ---\n",
    "run = wandb.init(\n",
    "    project=config[\"project_name\"],\n",
    "    name=config[\"run_name\"],\n",
    "    config=config,\n",
    "    sync_tensorboard=True, # Synchronise les logs SB3 avec WandB\n",
    "    monitor_gym=True,\n",
    "    save_code=True,\n",
    ")\n",
    "\n",
    "# --- 4. CRÉATION DE L'ENVIRONNEMENT ---\n",
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"./data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    portfolio_initial_value=1000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    "    reward_function=reward_function,\n",
    "    window_size=config[\"window_size\"] # Important pour le LSTM\n",
    ")\n",
    "\n",
    "env = DiscreteActionsWrapper(env, positions=config[\"positions\"])\n",
    "\n",
    "# --- 5. CRÉATION DU MODÈLE ET ENTRAÎNEMENT ---\n",
    "model = RecurrentPPO(\n",
    "    config[\"policy_type\"],\n",
    "    env,\n",
    "    verbose=1,\n",
    "    learning_rate=config[\"learning_rate\"],\n",
    "    ent_coef=config[\"ent_coef\"],\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    n_steps=config[\"n_steps\"],\n",
    "    # Log dans le dossier spécifique pour que WandB le trouve\n",
    "    tensorboard_log=f\"runs/{run.id}\"\n",
    ")\n",
    "\n",
    "print(f\"Lancement du run WandB : {run.name}\")\n",
    "model.learn(\n",
    "    total_timesteps=config[\"total_timesteps\"],\n",
    "    callback=WandbCallback(\n",
    "        gradient_save_freq=100,\n",
    "        model_save_path=f\"models/{run.id}\",\n",
    "        verbose=2,\n",
    "    )\n",
    ")\n",
    "\n",
    "model.save(\"recurrent_ppo_final\")\n",
    "\n",
    "# --- 6. ÉVALUATION ET LOGGING FINAL ---\n",
    "print(\"Évaluation...\")\n",
    "obs, info = env.reset()\n",
    "done, truncated = False, False\n",
    "\n",
    "while not (done or truncated):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    action = int(action)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "# Envoi des métriques finales manuelles\n",
    "final_metrics = env.unwrapped.get_metrics()\n",
    "wandb.log({\n",
    "    \"final_portfolio_valuation\": info['portfolio_valuation'],\n",
    "    \"market_return\": final_metrics.get(\"Market Return\", 0),\n",
    "    \"portfolio_return\": final_metrics.get(\"Portfolio Return\", 0)\n",
    "})\n",
    "\n",
    "# Fin du run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5619039e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "from gym_trading_env.wrapper import DiscreteActionsWrapper\n",
    "from gym_trading_env.renderer import Renderer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "# Vous aurez besoin de cette librairie pour le LSTM\n",
    "# !pip install sb3-contrib\n",
    "\n",
    "from sb3_contrib import RecurrentPPO\n",
    "\n",
    "# --- 1. CONFIGURATION (WandB) ---\n",
    "config = {\n",
    "    \"policy_type\": \"MlpLstmPolicy\",  # Changement majeur : LSTM\n",
    "    \"total_timesteps\": 500_000,      # Un peu plus long pour le LSTM\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"batch_size\": 128,\n",
    "    \"n_steps\": 2048,\n",
    "    \"positions\": [0, 0.5, 1.0],      # Simplifié au début : Cash, Moitié, Full (Pas de levier/short risqué)\n",
    "    \"project_name\": \"RL-Trading-Project\",\n",
    "    \"run_name\": \"RecurrentPPO_Optimized\"\n",
    "}\n",
    "\n",
    "# --- 2. FONCTIONS DE TRAITEMENT ---\n",
    "\n",
    "def calculate_indicators(df):\n",
    "    # RSI\n",
    "    delta = df['close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    df['feature_rsi'] = 100 - (100 / (1 + rs))\n",
    "    df['feature_rsi'] = df['feature_rsi'] / 100.0 # Normalisé\n",
    "\n",
    "    # MACD\n",
    "    exp1 = df['close'].ewm(span=12, adjust=False).mean()\n",
    "    exp2 = df['close'].ewm(span=26, adjust=False).mean()\n",
    "    df['feature_macd'] = (exp1 - exp2) / df['close'] # Normalisé par le prix\n",
    "\n",
    "    # ATR (Volatilité) - Important pour la survie\n",
    "    high_low = df['high'] - df['low']\n",
    "    high_close = np.abs(df['high'] - df['close'].shift())\n",
    "    low_close = np.abs(df['low'] - df['close'].shift())\n",
    "    true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    df['feature_atr'] = true_range.rolling(14).mean() / df['close']\n",
    "    \n",
    "    # Returns\n",
    "    df['feature_return'] = df['close'].pct_change()\n",
    "    \n",
    "    return df.dropna()\n",
    "\n",
    "def preprocess(df):\n",
    "    df = df.sort_index().dropna().drop_duplicates()\n",
    "    return calculate_indicators(df)\n",
    "\n",
    "def reward_function(history):\n",
    "    # Rendement logarithmique\n",
    "    current_val = history['portfolio_valuation', -1]\n",
    "    prev_val = history['portfolio_valuation', -2]\n",
    "    ret = np.log(current_val / prev_val)\n",
    "    \n",
    "    # Pénalité de volatilité (Sharpe Ratio implicite)\n",
    "    # Cela calme l'agent pour éviter les -50%\n",
    "    risk_penalty = 0.1 * (ret ** 2)\n",
    "    \n",
    "    return ret - risk_penalty\n",
    "\n",
    "# --- 3. INITIALISATION WANDB ---\n",
    "run = wandb.init(\n",
    "    project=config[\"project_name\"],\n",
    "    name=config[\"run_name\"],\n",
    "    config=config,\n",
    "    sync_tensorboard=True, # Synchronise les logs SB3 avec WandB\n",
    "    monitor_gym=True,\n",
    "    save_code=True,\n",
    ")\n",
    "\n",
    "# --- 4. CRÉATION DE L'ENVIRONNEMENT ---\n",
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"./data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    portfolio_initial_value=1000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    "    reward_function=reward_function,\n",
    "    window_size=config[\"window_size\"] # Important pour le LSTM\n",
    ")\n",
    "\n",
    "env = DiscreteActionsWrapper(env, positions=config[\"positions\"])\n",
    "\n",
    "# --- 5. CRÉATION DU MODÈLE ET ENTRAÎNEMENT ---\n",
    "model = RecurrentPPO(\n",
    "    config[\"policy_type\"],\n",
    "    env,\n",
    "    verbose=1,\n",
    "    learning_rate=config[\"learning_rate\"],\n",
    "    ent_coef=config[\"ent_coef\"],\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    n_steps=config[\"n_steps\"],\n",
    "    # Log dans le dossier spécifique pour que WandB le trouve\n",
    "    tensorboard_log=f\"runs/{run.id}\" \n",
    ")\n",
    "\n",
    "print(f\"Lancement du run WandB : {run.name}\")\n",
    "model.learn(\n",
    "    total_timesteps=config[\"total_timesteps\"],\n",
    "    callback=WandbCallback(\n",
    "        gradient_save_freq=100,\n",
    "        model_save_path=f\"models/{run.id}\",\n",
    "        verbose=2,\n",
    "    )\n",
    ")\n",
    "\n",
    "model.save(\"recurrent_ppo_final\")\n",
    "\n",
    "# --- 6. ÉVALUATION ET LOGGING FINAL ---\n",
    "print(\"Évaluation...\")\n",
    "obs, info = env.reset()\n",
    "done, truncated = False, False\n",
    "\n",
    "while not (done or truncated):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    action = int(action)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "# Envoi des métriques finales manuelles\n",
    "final_metrics = env.unwrapped.get_metrics()\n",
    "wandb.log({\n",
    "    \"final_portfolio_valuation\": info['portfolio_valuation'],\n",
    "    \"market_return\": final_metrics.get(\"Market Return\", 0),\n",
    "    \"portfolio_return\": final_metrics.get(\"Portfolio Return\", 0)\n",
    "})\n",
    "\n",
    "# Fin du run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77cd83d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "from gym_trading_env.wrapper import DiscreteActionsWrapper\n",
    "from gym_trading_env.renderer import Renderer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "from sb3_contrib import RecurrentPPO\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "config = {\n",
    "    \"policy_type\": \"MlpLstmPolicy\",\n",
    "    \"total_timesteps\": 500_000,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"batch_size\": 128,\n",
    "    \"n_steps\": 2048,\n",
    "    # \"window_size\": 20,  <-- SUPPRIMÉ car géré par le LSTM interne\n",
    "    \"positions\": [0, 0.5, 1.0],\n",
    "    \"project_name\": \"RL-Trading-Project\",\n",
    "    \"run_name\": \"RecurrentPPO_Fix\"\n",
    "}\n",
    "\n",
    "# --- 2. FONCTIONS DE TRAITEMENT ---\n",
    "def calculate_indicators(df):\n",
    "    # RSI\n",
    "    delta = df['close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    df['feature_rsi'] = 100 - (100 / (1 + rs))\n",
    "    df['feature_rsi'] = df['feature_rsi'] / 100.0 \n",
    "\n",
    "    # MACD\n",
    "    exp1 = df['close'].ewm(span=12, adjust=False).mean()\n",
    "    exp2 = df['close'].ewm(span=26, adjust=False).mean()\n",
    "    df['feature_macd'] = (exp1 - exp2) / df['close']\n",
    "\n",
    "    # ATR (Volatilité)\n",
    "    high_low = df['high'] - df['low']\n",
    "    high_close = np.abs(df['high'] - df['close'].shift())\n",
    "    low_close = np.abs(df['low'] - df['close'].shift())\n",
    "    true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    df['feature_atr'] = true_range.rolling(14).mean() / df['close']\n",
    "    \n",
    "    # Returns\n",
    "    df['feature_return'] = df['close'].pct_change()\n",
    "    \n",
    "    return df.dropna()\n",
    "\n",
    "def preprocess(df):\n",
    "    df = df.sort_index().dropna().drop_duplicates()\n",
    "    return calculate_indicators(df)\n",
    "\n",
    "def reward_function(history):\n",
    "    current_val = history['portfolio_valuation', -1]\n",
    "    prev_val = history['portfolio_valuation', -2]\n",
    "    ret = np.log(current_val / prev_val)\n",
    "    # Pénalité de volatilité\n",
    "    risk_penalty = 0.1 * (ret ** 2)\n",
    "    return ret - risk_penalty\n",
    "\n",
    "# --- 3. INITIALISATION WANDB ---\n",
    "run = wandb.init(\n",
    "    project=config[\"project_name\"],\n",
    "    name=config[\"run_name\"],\n",
    "    config=config,\n",
    "    sync_tensorboard=True,\n",
    "    monitor_gym=True,\n",
    "    save_code=True,\n",
    ")\n",
    "\n",
    "# --- 4. CRÉATION DE L'ENVIRONNEMENT ---\n",
    "# CORRECTION ICI : Suppression de window_size\n",
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"./data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    portfolio_initial_value=1000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    "    reward_function=reward_function,\n",
    ")\n",
    "\n",
    "env = DiscreteActionsWrapper(env, positions=config[\"positions\"])\n",
    "\n",
    "# --- 5. CRÉATION DU MODÈLE ET ENTRAÎNEMENT ---\n",
    "model = RecurrentPPO(\n",
    "    config[\"policy_type\"],\n",
    "    env,\n",
    "    verbose=1,\n",
    "    learning_rate=config[\"learning_rate\"],\n",
    "    ent_coef=config[\"ent_coef\"],\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    n_steps=config[\"n_steps\"],\n",
    "    tensorboard_log=f\"runs/{run.id}\" \n",
    ")\n",
    "\n",
    "print(f\"Lancement du run WandB : {run.name}\")\n",
    "model.learn(\n",
    "    total_timesteps=config[\"total_timesteps\"],\n",
    "    callback=WandbCallback(\n",
    "        gradient_save_freq=100,\n",
    "        model_save_path=f\"models/{run.id}\",\n",
    "        verbose=2,\n",
    "    )\n",
    ")\n",
    "\n",
    "model.save(\"recurrent_ppo_final\")\n",
    "\n",
    "# --- 6. ÉVALUATION ---\n",
    "print(\"Évaluation...\")\n",
    "obs, info = env.reset()\n",
    "done, truncated = False, False\n",
    "\n",
    "while not (done or truncated):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    action = int(action)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "final_metrics = env.unwrapped.get_metrics()\n",
    "wandb.log({\n",
    "    \"final_portfolio_valuation\": info['portfolio_valuation'],\n",
    "    \"market_return\": final_metrics.get(\"Market Return\", 0),\n",
    "    \"portfolio_return\": final_metrics.get(\"Portfolio Return\", 0)\n",
    "})\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3403cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation locale\n",
    "env.unwrapped.save_for_render(dir=\"render_logs\")\n",
    "renderer = Renderer(render_logs_dir=\"render_logs\")\n",
    "renderer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc110815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "from gym_trading_env.wrapper import DiscreteActionsWrapper\n",
    "from gym_trading_env.renderer import Renderer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "from sb3_contrib import RecurrentPPO\n",
    "\n",
    "# --- 1. CONFIGURATION \"PHASE 2\" ---\n",
    "config = {\n",
    "    \"policy_type\": \"MlpLstmPolicy\",\n",
    "    \"total_timesteps\": 1_000_000,    # DOUBLÉ : Le LSTM a besoin de temps\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"batch_size\": 128,\n",
    "    \"n_steps\": 2048,\n",
    "    \n",
    "    # CHANGEMENT MAJEUR : On active le SHORT (-1)\n",
    "    # Positions : [-1 = Short, 0 = Cash, 1 = Long]\n",
    "    # Toujours pas de levier (1.5) pour l'instant, on veut d'abord qu'il maîtrise le sens.\n",
    "    \"positions\": [-1, 0, 1], \n",
    "    \n",
    "    \"project_name\": \"RL-Trading-Project\",\n",
    "    \"run_name\": \"RecurrentPPO_Phase2_ShortEnabled\"\n",
    "}\n",
    "\n",
    "# --- 2. INDICATEURS (Inchangés car robustes) ---\n",
    "def calculate_indicators(df):\n",
    "    # RSI\n",
    "    delta = df['close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    df['feature_rsi'] = 100 - (100 / (1 + rs))\n",
    "    df['feature_rsi'] = df['feature_rsi'] / 100.0 \n",
    "\n",
    "    # MACD\n",
    "    exp1 = df['close'].ewm(span=12, adjust=False).mean()\n",
    "    exp2 = df['close'].ewm(span=26, adjust=False).mean()\n",
    "    df['feature_macd'] = (exp1 - exp2) / df['close']\n",
    "\n",
    "    # ATR (Volatilité)\n",
    "    high_low = df['high'] - df['low']\n",
    "    high_close = np.abs(df['high'] - df['close'].shift())\n",
    "    low_close = np.abs(df['low'] - df['close'].shift())\n",
    "    true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    df['feature_atr'] = true_range.rolling(14).mean() / df['close']\n",
    "    \n",
    "    # Returns\n",
    "    df['feature_return'] = df['close'].pct_change()\n",
    "    \n",
    "    return df.dropna()\n",
    "\n",
    "def preprocess(df):\n",
    "    df = df.sort_index().dropna().drop_duplicates()\n",
    "    return calculate_indicators(df)\n",
    "\n",
    "# --- 3. RÉCOMPENSE AJUSTÉE ---\n",
    "def reward_function(history):\n",
    "    current_val = history['portfolio_valuation', -1]\n",
    "    prev_val = history['portfolio_valuation', -2]\n",
    "    ret = np.log(current_val / prev_val)\n",
    "    \n",
    "    # AJUSTEMENT : Pénalité réduite (0.05 au lieu de 0.1)\n",
    "    # On laisse l'agent prendre un peu plus de risques pour chercher du profit.\n",
    "    risk_penalty = 0.05 * (ret ** 2)\n",
    "    \n",
    "    return ret - risk_penalty\n",
    "\n",
    "# --- 4. INIT WANDB ---\n",
    "run = wandb.init(\n",
    "    project=config[\"project_name\"],\n",
    "    name=config[\"run_name\"],\n",
    "    config=config,\n",
    "    sync_tensorboard=True,\n",
    "    monitor_gym=True,\n",
    "    save_code=True,\n",
    ")\n",
    "\n",
    "# --- 5. ENVIRONNEMENT ---\n",
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"./data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    portfolio_initial_value=1000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    "    reward_function=reward_function,\n",
    "    # Rappel : Pas de window_size ici, le LSTM gère sa mémoire\n",
    ")\n",
    "\n",
    "env = DiscreteActionsWrapper(env, positions=config[\"positions\"])\n",
    "\n",
    "# --- 6. MODÈLE & ENTRAÎNEMENT ---\n",
    "model = RecurrentPPO(\n",
    "    config[\"policy_type\"],\n",
    "    env,\n",
    "    verbose=1,\n",
    "    learning_rate=config[\"learning_rate\"],\n",
    "    ent_coef=config[\"ent_coef\"],\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    n_steps=config[\"n_steps\"],\n",
    "    tensorboard_log=f\"runs/{run.id}\" \n",
    ")\n",
    "\n",
    "print(f\"Lancement du run WandB : {run.name} (Short activé)\")\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=config[\"total_timesteps\"],\n",
    "    callback=WandbCallback(\n",
    "        gradient_save_freq=100,\n",
    "        model_save_path=f\"models/{run.id}\",\n",
    "        verbose=2,\n",
    "    )\n",
    ")\n",
    "\n",
    "model.save(\"recurrent_ppo_short_enabled\")\n",
    "\n",
    "# --- 7. ÉVALUATION FINALE ---\n",
    "print(\"Évaluation finale...\")\n",
    "obs, info = env.reset()\n",
    "done, truncated = False, False\n",
    "\n",
    "while not (done or truncated):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    action = int(action)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "final_metrics = env.unwrapped.get_metrics()\n",
    "wandb.log({\n",
    "    \"final_portfolio_valuation\": info['portfolio_valuation'],\n",
    "    \"market_return\": final_metrics.get(\"Market Return\", 0),\n",
    "    \"portfolio_return\": final_metrics.get(\"Portfolio Return\", 0)\n",
    "})\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ff2f027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rendu visuel\n",
    "env.unwrapped.save_for_render(dir=\"render_logs\")\n",
    "renderer = Renderer(render_logs_dir=\"render_logs\")\n",
    "renderer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f6505bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "from gym_trading_env.wrapper import DiscreteActionsWrapper\n",
    "from gym_trading_env.renderer import Renderer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "from sb3_contrib import RecurrentPPO\n",
    "\n",
    "# --- 1. CONFIGURATION \"ALPHA HUNTER\" ---\n",
    "config = {\n",
    "    \"policy_type\": \"MlpLstmPolicy\",\n",
    "    \"total_timesteps\": 1_500_000,    # On allonge encore, l'Alpha est dur à trouver\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \n",
    "    # CHANGEMENT CRUCIAL : Entropie x5\n",
    "    # Cela force l'agent à essayer des actions \"bizarres\" (comme shorter en bull run)\n",
    "    # au lieu de s'endormir sur une position Long.\n",
    "    \"ent_coef\": 0.05,  \n",
    "    \n",
    "    \"batch_size\": 256, # Batch plus gros pour lisser le bruit des returns\n",
    "    \"n_steps\": 2048,\n",
    "    \"positions\": [-1, 0, 1], \n",
    "    \"project_name\": \"RL-Trading-Project\",\n",
    "    \"run_name\": \"RecurrentPPO_AlphaHunter\"\n",
    "}\n",
    "\n",
    "# --- 2. TRAITEMENT (Inchangé) ---\n",
    "def calculate_indicators(df):\n",
    "    delta = df['close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    df['feature_rsi'] = 100 - (100 / (1 + rs))\n",
    "    df['feature_rsi'] = df['feature_rsi'] / 100.0 \n",
    "\n",
    "    exp1 = df['close'].ewm(span=12, adjust=False).mean()\n",
    "    exp2 = df['close'].ewm(span=26, adjust=False).mean()\n",
    "    df['feature_macd'] = (exp1 - exp2) / df['close']\n",
    "\n",
    "    high_low = df['high'] - df['low']\n",
    "    high_close = np.abs(df['high'] - df['close'].shift())\n",
    "    low_close = np.abs(df['low'] - df['close'].shift())\n",
    "    true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    df['feature_atr'] = true_range.rolling(14).mean() / df['close']\n",
    "    \n",
    "    df['feature_return'] = df['close'].pct_change()\n",
    "    \n",
    "    return df.dropna()\n",
    "\n",
    "def preprocess(df):\n",
    "    df = df.sort_index().dropna().drop_duplicates()\n",
    "    return calculate_indicators(df)\n",
    "\n",
    "# --- 3. RÉCOMPENSE DIFFERENCIELLE (ALPHA) ---\n",
    "def reward_function(history):\n",
    "    # Performance de l'agent\n",
    "    current_val = history['portfolio_valuation', -1]\n",
    "    prev_val = history['portfolio_valuation', -2]\n",
    "    portfolio_ret = np.log(current_val / prev_val)\n",
    "    \n",
    "    # Performance du marché (Data \"close\" est souvent la colonne 0 ou accessible via history)\n",
    "    # Gym-trading-env stocke les données brutes dans history['data_close', t]\n",
    "    current_price = history['data_close', -1]\n",
    "    prev_price = history['data_close', -2]\n",
    "    market_ret = np.log(current_price / prev_price)\n",
    "    \n",
    "    # RECOMPENSE = ALPHA (Surperformance)\n",
    "    # Si l'agent fait pareil que le marché, Reward = 0.\n",
    "    # S'il fait mieux (ex: cash quand ça baisse, ou short), Reward > 0.\n",
    "    reward = portfolio_ret - market_ret\n",
    "    \n",
    "    # Petit bonus pour l'action (éviter la léthargie)\n",
    "    # reward += 0.00001 \n",
    "    \n",
    "    return reward \n",
    "\n",
    "# --- 4. RUN WANDB ---\n",
    "run = wandb.init(\n",
    "    project=config[\"project_name\"],\n",
    "    name=config[\"run_name\"],\n",
    "    config=config,\n",
    "    sync_tensorboard=True,\n",
    "    monitor_gym=True,\n",
    "    save_code=True,\n",
    ")\n",
    "\n",
    "# --- 5. ENVIRONNEMENT ---\n",
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"./data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    portfolio_initial_value=1000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    "    reward_function=reward_function,\n",
    ")\n",
    "\n",
    "env = DiscreteActionsWrapper(env, positions=config[\"positions\"])\n",
    "\n",
    "# --- 6. MODÈLE ---\n",
    "model = RecurrentPPO(\n",
    "    config[\"policy_type\"],\n",
    "    env,\n",
    "    verbose=1,\n",
    "    learning_rate=config[\"learning_rate\"],\n",
    "    ent_coef=config[\"ent_coef\"], # C'est ici que ça se joue\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    n_steps=config[\"n_steps\"],\n",
    "    tensorboard_log=f\"runs/{run.id}\" \n",
    ")\n",
    "\n",
    "print(f\"--- Démarrage Alpha Hunter ---\")\n",
    "print(f\"Objectif : Battre le Buy & Hold (Reward = Return - Market)\")\n",
    "print(f\"Exploration forcée (Ent_coef={config['ent_coef']})\")\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=config[\"total_timesteps\"],\n",
    "    callback=WandbCallback(\n",
    "        gradient_save_freq=100,\n",
    "        model_save_path=f\"models/{run.id}\",\n",
    "        verbose=2,\n",
    "    )\n",
    ")\n",
    "\n",
    "model.save(\"recurrent_ppo_alpha_hunter\")\n",
    "\n",
    "# --- 7. EVALUATION ---\n",
    "obs, info = env.reset()\n",
    "done, truncated = False, False\n",
    "\n",
    "while not (done or truncated):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    action = int(action)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "metrics = env.unwrapped.get_metrics()\n",
    "print(\"Métriques finales :\", metrics)\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
